{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì¶ 1. Setup Environment for LoRA/PEFT\n",
    "\n",
    "This notebook covers the first step in our LoRA/PEFT checklist: setting up the environment.\n",
    "\n",
    "## Checklist Items:\n",
    "- ‚úÖ Install Python >= 3.8\n",
    "- ‚úÖ Create virtual environment\n",
    "- ‚úÖ Install PyTorch (GPU support)\n",
    "- ‚úÖ Install required libraries\n",
    "- ‚úÖ Verify installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check Python Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check if Python >= 3.8\n",
    "if sys.version_info >= (3, 8):\n",
    "    print(\"‚úÖ Python version is compatible\")\n",
    "else:\n",
    "    print(\"‚ùå Python version is too old. Please upgrade to Python 3.8+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Required Packages\n",
    "\n",
    "Run the following commands in your terminal to install the required packages:\n",
    "\n",
    "```bash\n",
    "# Create virtual environment\n",
    "python -m venv lora_env\n",
    "source lora_env/bin/activate  # On Windows: lora_env\\Scripts\\activate\n",
    "\n",
    "# Install packages\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PyTorch installation\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "    print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‚úÖ CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"‚úÖ GPU count: {torch.cuda.device_count()}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå PyTorch not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Transformers\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"‚úÖ Transformers version: {transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Transformers not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PEFT\n",
    "try:\n",
    "    import peft\n",
    "    print(f\"‚úÖ PEFT version: {peft.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå PEFT not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check other important libraries\n",
    "libraries = [\n",
    "    ('datasets', 'datasets'),\n",
    "    ('accelerate', 'accelerate'),\n",
    "    ('bitsandbytes', 'bitsandbytes'),\n",
    "    ('numpy', 'numpy'),\n",
    "    ('scipy', 'scipy'),\n",
    "    ('evaluate', 'evaluate'),\n",
    "    ('tqdm', 'tqdm'),\n",
    "]\n",
    "\n",
    "for name, module in libraries:\n",
    "    try:\n",
    "        lib = __import__(module)\n",
    "        version = getattr(lib, '__version__', 'unknown')\n",
    "        print(f\"‚úÖ {name}: {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {name}: not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Basic Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic model loading\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "try:\n",
    "    # Load a small model for testing\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully loaded {model_name}\")\n",
    "    print(f\"‚úÖ Model parameters: {model.num_parameters():,}\")\n",
    "    \n",
    "    # Test tokenization\n",
    "    text = \"Hello, this is a test!\"\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "    print(f\"‚úÖ Tokenization works: {tokens['input_ids'].shape}\")\n",
    "    \n",
    "    # Test model forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    print(f\"‚úÖ Model forward pass works: {outputs.last_hidden_state.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error testing basic functionality: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test PEFT functionality\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "try:\n",
    "    # Create LoRA config\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.FEATURE_EXTRACTION,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"query\", \"value\"]\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA to model\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    print(f\"‚úÖ PEFT model created successfully\")\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in peft_model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    \n",
    "    print(f\"‚úÖ Trainable params: {trainable_params:,} ({100 * trainable_params / all_param:.2f}%)\")\n",
    "    print(f\"‚úÖ All params: {all_param:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error testing PEFT functionality: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Our Custom Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our custom implementation\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "try:\n",
    "    from config import ModelConfig, PEFTConfig\n",
    "    from models import PEFTModelWrapper\n",
    "    \n",
    "    # Create configurations\n",
    "    model_config = ModelConfig(\n",
    "        model_name_or_path=\"distilbert-base-uncased\",\n",
    "        num_labels=2\n",
    "    )\n",
    "    \n",
    "    peft_config = PEFTConfig(\n",
    "        peft_type=\"LORA\",\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        r=8,\n",
    "        lora_alpha=16\n",
    "    )\n",
    "    \n",
    "    # Create model wrapper\n",
    "    model_wrapper = PEFTModelWrapper(model_config, peft_config)\n",
    "    peft_model = model_wrapper.load_model()\n",
    "    \n",
    "    print(\"‚úÖ Custom implementation works!\")\n",
    "    print(f\"‚úÖ Model info: {model_wrapper.get_model_info()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error testing custom implementation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Environment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import psutil\n",
    "\n",
    "print(\"üñ•Ô∏è System Information:\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"CPU cores: {psutil.cpu_count()}\")\n",
    "print(f\"RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüöÄ GPU Information:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "        print(f\"GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\nelse:\n",
    "    print(\"\\n‚ö†Ô∏è No GPU available - training will be slower\")\n",
    "\n",
    "print(\"\\n‚úÖ Environment setup complete!\")\n",
    "print(\"\\nüìù Next steps:\")\n",
    "print(\"1. Open notebook 02_data_preparation.ipynb\")\n",
    "print(\"2. Learn about data loading and preprocessing\")\n",
    "print(\"3. Prepare your dataset for training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
