# üöÄ H∆∞·ªõng D·∫´n Implement LoRA/PEFT T·ª´ S·ªë 0

H∆∞·ªõng d·∫´n n√†y s·∫Ω gi√∫p b·∫°n hi·ªÉu v√† x√¢y d·ª±ng l·∫°i to√†n b·ªô h·ªá th·ªëng LoRA/PEFT t·ª´ ƒë·∫ßu, t·ª´ng b∆∞·ªõc m·ªôt.

## üìö Ki·∫øn Th·ª©c C·∫ßn C√≥ Tr∆∞·ªõc

### 1. Python C∆° B·∫£n
- Classes v√† Objects
- Dataclasses
- Type hints
- Import/Export modules

### 2. Machine Learning C∆° B·∫£n
- Tokenization (chuy·ªÉn text th√†nh s·ªë)
- Neural Networks c∆° b·∫£n
- Training/Validation/Test sets

### 3. Th∆∞ Vi·ªán C·∫ßn Bi·∫øt
- `transformers`: Load models t·ª´ Hugging Face
- `datasets`: Load v√† x·ª≠ l√Ω data
- `torch`: PyTorch framework
- `peft`: Parameter Efficient Fine-Tuning

---

## üèóÔ∏è B∆∞·ªõc 1: Hi·ªÉu Ki·∫øn Tr√∫c T·ªïng Th·ªÉ

### T·∫°i Sao C·∫ßn Nhi·ªÅu Files?
```
Nguy√™n t·∫Øc: "Separation of Concerns" - M·ªói file l√†m 1 vi·ªác c·ª• th·ªÉ

config/     ‚Üí C·∫•u h√¨nh (nh∆∞ setting trong game)
data/       ‚Üí X·ª≠ l√Ω d·ªØ li·ªáu (load v√† clean data)
models/     ‚Üí Qu·∫£n l√Ω AI models
training/   ‚Üí Logic hu·∫•n luy·ªán model
evaluation/ ‚Üí ƒê√°nh gi√° model
inference/  ‚Üí S·ª≠ d·ª•ng model ƒë√£ train
examples/   ‚Üí V√≠ d·ª• c√°ch s·ª≠ d·ª•ng
```

### Lu·ªìng Ho·∫°t ƒê·ªông
```
1. Config ‚Üí Thi·∫øt l·∫≠p tham s·ªë
2. Data ‚Üí Load v√† x·ª≠ l√Ω d·ªØ li·ªáu
3. Model ‚Üí T·∫°o model v·ªõi PEFT
4. Training ‚Üí Hu·∫•n luy·ªán model
5. Evaluation ‚Üí ƒê√°nh gi√° k·∫øt qu·∫£
6. Inference ‚Üí S·ª≠ d·ª•ng model
```

---

## üîß B∆∞·ªõc 2: Implement Config System

### T·∫°i Sao C·∫ßn Config?
```python
# Thay v√¨ hard-code nh∆∞ n√†y:
model = AutoModel.from_pretrained("bert-base-uncased")
batch_size = 16
learning_rate = 0.001

# Ta d√πng config ƒë·ªÉ d·ªÖ thay ƒë·ªïi:
config = ModelConfig(
    model_name="bert-base-uncased",
    batch_size=16,
    learning_rate=0.001
)
```

### 2.1 T·∫°o `config/__init__.py`
```python
"""
File n√†y cho Python bi·∫øt config l√† m·ªôt package
V√† export nh·ªØng class ch√≠nh ƒë·ªÉ d·ªÖ import
"""
from .model_config import ModelConfig, PEFTConfig
from .training_config import TrainingConfig

__all__ = ["ModelConfig", "PEFTConfig", "TrainingConfig"]
```

### 2.2 T·∫°o `config/model_config.py`
```python
"""
C·∫•u h√¨nh cho model v√† PEFT
"""
from dataclasses import dataclass
from typing import List, Optional
from peft import TaskType

@dataclass
class ModelConfig:
    """C·∫•u h√¨nh cho base model"""
    
    # T√™n model t·ª´ Hugging Face Hub
    model_name_or_path: str = "bert-base-uncased"
    
    # S·ªë l∆∞·ª£ng labels (cho classification)
    num_labels: int = 2
    
    # ƒê·ªô d√†i t·ªëi ƒëa c·ªßa sequence
    max_length: int = 512
    
    # C√≥ d√πng quantization kh√¥ng (ti·∫øt ki·ªám memory)
    use_quantization: bool = False
    quantization_bits: int = 4  # 4-bit ho·∫∑c 8-bit

@dataclass  
class PEFTConfig:
    """C·∫•u h√¨nh cho PEFT methods"""
    
    # Lo·∫°i PEFT: LORA, PREFIX_TUNING, etc.
    peft_type: str = "LORA"
    
    # Lo·∫°i task: classification, generation, etc.
    task_type: TaskType = TaskType.SEQ_CLS
    
    # LoRA parameters
    r: int = 16              # Rank - c√†ng nh·ªè c√†ng √≠t parameters
    lora_alpha: int = 32     # Scaling factor
    lora_dropout: float = 0.1 # Dropout ƒë·ªÉ tr√°nh overfitting
    
    # Modules n√†o s·∫Ω apply LoRA
    target_modules: Optional[List[str]] = None
```

**Gi·∫£i th√≠ch chi ti·∫øt:**
- `@dataclass`: T·ª± ƒë·ªông t·∫°o `__init__`, `__repr__` methods
- `Optional[List[str]]`: C√≥ th·ªÉ l√† None ho·∫∑c list of strings
- `TaskType.SEQ_CLS`: Enum t·ª´ th∆∞ vi·ªán PEFT

### 2.3 T·∫°o `config/training_config.py`
```python
"""
C·∫•u h√¨nh cho qu√° tr√¨nh training
"""
from dataclasses import dataclass
from typing import Optional, Dict, Any

@dataclass
class TrainingConfig:
    """C·∫•u h√¨nh training"""
    
    # Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£
    output_dir: str = "./results"
    
    # Training parameters
    num_train_epochs: int = 3           # S·ªë epochs
    per_device_train_batch_size: int = 8 # Batch size
    learning_rate: float = 5e-4         # Learning rate
    
    # Evaluation
    evaluation_strategy: str = "steps"   # Khi n√†o evaluate
    eval_steps: int = 500               # Evaluate m·ªói 500 steps
    
    # Logging
    logging_steps: int = 100            # Log m·ªói 100 steps
    
    def to_training_arguments(self) -> Dict[str, Any]:
        """Convert sang format c·ªßa Hugging Face Trainer"""
        return {
            "output_dir": self.output_dir,
            "num_train_epochs": self.num_train_epochs,
            "per_device_train_batch_size": self.per_device_train_batch_size,
            "learning_rate": self.learning_rate,
            "evaluation_strategy": self.evaluation_strategy,
            "eval_steps": self.eval_steps,
            "logging_steps": self.logging_steps,
        }
```

---

## üìä B∆∞·ªõc 3: Implement Data Processing

### T·∫°i Sao C·∫ßn Data Processing?
```
Raw text: "This movie is great!"
‚Üì Tokenization
Token IDs: [101, 2023, 3185, 2003, 2307, 999, 102]
‚Üì Padding/Truncation
Fixed length: [101, 2023, 3185, 2003, 2307, 999, 102, 0, 0, 0]
```

### 3.1 T·∫°o `data/__init__.py`
```python
"""
Export main data classes
"""
from .data_loader import DataLoader, load_dataset_from_hub
from .preprocessing import TextClassificationPreprocessor

__all__ = [
    "DataLoader", 
    "load_dataset_from_hub",
    "TextClassificationPreprocessor"
]
```

### 3.2 T·∫°o `data/data_loader.py`
```python
"""
Load datasets t·ª´ nhi·ªÅu ngu·ªìn kh√°c nhau
"""
import os
import json
from typing import Union, Optional
from datasets import Dataset, DatasetDict, load_dataset

class DataLoader:
    """Class ƒë·ªÉ load datasets"""
    
    def __init__(self, cache_dir: Optional[str] = None):
        self.cache_dir = cache_dir
    
    def load_from_hub(self, dataset_name: str) -> DatasetDict:
        """Load dataset t·ª´ Hugging Face Hub"""
        try:
            dataset = load_dataset(dataset_name, cache_dir=self.cache_dir)
            print(f"‚úÖ Loaded dataset: {dataset_name}")
            return dataset
        except Exception as e:
            print(f"‚ùå Error loading {dataset_name}: {e}")
            raise
    
    def load_from_json(self, file_path: str) -> Dataset:
        """Load dataset t·ª´ JSON file"""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        return Dataset.from_list(data)

def load_dataset_from_hub(dataset_name: str) -> DatasetDict:
    """Convenience function"""
    loader = DataLoader()
    return loader.load_from_hub(dataset_name)
```

**Gi·∫£i th√≠ch:**
- `DatasetDict`: Dictionary ch·ª©a train/validation/test splits
- `Dataset`: M·ªôt split c·ª• th·ªÉ
- `cache_dir`: Th∆∞ m·ª•c cache ƒë·ªÉ kh√¥ng download l·∫°i

### 3.3 T·∫°o `data/preprocessing.py`
```python
"""
Preprocessing data cho c√°c tasks kh√°c nhau
"""
from abc import ABC, abstractmethod
from typing import Dict, List
from datasets import Dataset
from transformers import PreTrainedTokenizer

class DataPreprocessor(ABC):
    """Abstract base class cho preprocessing"""
    
    def __init__(self, tokenizer: PreTrainedTokenizer, max_length: int = 512):
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    @abstractmethod
    def preprocess_function(self, examples: Dict[str, List]) -> Dict[str, List]:
        """Method n√†y ph·∫£i ƒë∆∞·ª£c implement b·ªüi subclasses"""
        pass
    
    def preprocess_dataset(self, dataset: Dataset) -> Dataset:
        """Apply preprocessing l√™n to√†n b·ªô dataset"""
        return dataset.map(
            self.preprocess_function,
            batched=True,  # X·ª≠ l√Ω theo batch ƒë·ªÉ nhanh h∆°n
            desc="Preprocessing dataset"
        )

class TextClassificationPreprocessor(DataPreprocessor):
    """Preprocessor cho text classification"""
    
    def __init__(
        self, 
        tokenizer: PreTrainedTokenizer,
        text_column: str = "text",
        label_column: str = "label",
        max_length: int = 512
    ):
        super().__init__(tokenizer, max_length)
        self.text_column = text_column
        self.label_column = label_column
    
    def preprocess_function(self, examples: Dict[str, List]) -> Dict[str, List]:
        """Tokenize texts v√† prepare labels"""
        
        # Tokenize texts
        result = self.tokenizer(
            examples[self.text_column],
            padding=True,           # Pad v·ªÅ c√πng ƒë·ªô d√†i
            truncation=True,        # C·∫Øt n·∫øu qu√° d√†i
            max_length=self.max_length,
            return_tensors=None     # Return Python lists, not tensors
        )
        
        # Add labels
        if self.label_column in examples:
            result["labels"] = examples[self.label_column]
        
        return result
```

**Gi·∫£i th√≠ch:**
- `ABC`: Abstract Base Class - class cha kh√¥ng th·ªÉ instantiate tr·ª±c ti·∫øp
- `@abstractmethod`: Method b·∫Øt bu·ªôc ph·∫£i implement
- `batched=True`: X·ª≠ l√Ω nhi·ªÅu examples c√πng l√∫c
- `padding=True`: Th√™m padding tokens ƒë·ªÉ c√πng ƒë·ªô d√†i

---

## ü§ñ B∆∞·ªõc 4: Implement Model Wrappers

### T·∫°i Sao C·∫ßn Wrappers?
```python
# Thay v√¨ code ph·ª©c t·∫°p:
model = AutoModelForSequenceClassification.from_pretrained(...)
if use_quantization:
    model = quantize_model(model)
peft_config = LoraConfig(...)
model = get_peft_model(model, peft_config)

# Ta c√≥ wrapper ƒë∆°n gi·∫£n:
wrapper = PEFTModelWrapper(model_config, peft_config)
model = wrapper.load_model()
```

### 4.1 T·∫°o `models/__init__.py`
```python
"""
Export model classes
"""
from .base_model import BaseModelWrapper
from .peft_model import PEFTModelWrapper

__all__ = ["BaseModelWrapper", "PEFTModelWrapper"]
```

### 4.2 T·∫°o `models/base_model.py`
```python
"""
Wrapper cho base models
"""
import torch
from typing import Optional
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    BitsAndBytesConfig
)
from peft import TaskType

from ..config.model_config import ModelConfig

class BaseModelWrapper:
    """Wrapper cho base model"""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
    
    def load_tokenizer(self):
        """Load tokenizer"""
        if self.tokenizer is None:
            print(f"Loading tokenizer: {self.config.model_name_or_path}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.model_name_or_path
            )
            
            # Add pad token n·∫øu ch∆∞a c√≥
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print(f"‚úÖ Tokenizer loaded")
        
        return self.tokenizer
    
    def load_model(self, task_type: TaskType):
        """Load model based on task type"""
        if self.model is None:
            print(f"Loading model: {self.config.model_name_or_path}")
            
            # Prepare arguments
            model_kwargs = {}
            
            # Add quantization n·∫øu c·∫ßn
            if self.config.use_quantization:
                model_kwargs["quantization_config"] = self._get_quantization_config()
                model_kwargs["torch_dtype"] = torch.float16
            
            # Load model based on task
            if task_type == TaskType.SEQ_CLS:
                self.model = AutoModelForSequenceClassification.from_pretrained(
                    self.config.model_name_or_path,
                    num_labels=self.config.num_labels,
                    **model_kwargs
                )
            
            print(f"‚úÖ Model loaded")
        
        return self.model
    
    def _get_quantization_config(self):
        """T·∫°o quantization config"""
        if self.config.quantization_bits == 4:
            return BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
            )
        elif self.config.quantization_bits == 8:
            return BitsAndBytesConfig(load_in_8bit=True)
```

**Gi·∫£i th√≠ch:**
- `TaskType.SEQ_CLS`: Sequence Classification
- `quantization_config`: C·∫•u h√¨nh ƒë·ªÉ gi·∫£m memory usage
- `torch.float16`: Half precision ƒë·ªÉ ti·∫øt ki·ªám memory

---

## ‚è∞ T·∫°m D·ª´ng - Checkpoint 1

ƒê·∫øn ƒë√¢y b·∫°n ƒë√£ hi·ªÉu:
1. ‚úÖ T·∫°i sao c·∫ßn chia code th√†nh nhi·ªÅu files
2. ‚úÖ C√°ch t·∫°o config system
3. ‚úÖ C√°ch load v√† preprocess data
4. ‚úÖ C√°ch wrap models

**Ti·∫øp theo**: Ch√∫ng ta s·∫Ω implement PEFT wrapper, training system, v√† evaluation.

---

## üß© B∆∞·ªõc 5: Implement PEFT Model Wrapper

### T·∫°i Sao C·∫ßn PEFT?
```
Full Fine-tuning: Train to√†n b·ªô 110M parameters c·ªßa BERT
LoRA: Ch·ªâ train 0.3M parameters m·ªõi (99.7% √≠t h∆°n!)

K·∫øt qu·∫£: G·∫ßn nh∆∞ same accuracy, nh∆∞ng:
- √çt memory h∆°n
- Train nhanh h∆°n
- D·ªÖ share model h∆°n (ch·ªâ c·∫ßn share LoRA weights)
```

### 5.1 T·∫°o `models/peft_model.py`
```python
"""
PEFT Model Wrapper
"""
import torch
from typing import Optional
from peft import get_peft_model, LoraConfig, TaskType

from .base_model import BaseModelWrapper
from ..config.model_config import ModelConfig, PEFTConfig

class PEFTModelWrapper:
    """Wrapper cho PEFT models"""

    def __init__(self, model_config: ModelConfig, peft_config: PEFTConfig):
        self.model_config = model_config
        self.peft_config = peft_config
        self.base_wrapper = BaseModelWrapper(model_config)
        self.peft_model = None

    def load_model(self):
        """Load v√† setup PEFT model"""
        if self.peft_model is None:
            print("Setting up PEFT model...")

            # Load base model
            base_model = self.base_wrapper.load_model(self.peft_config.task_type)

            # Create PEFT config
            peft_config = self._create_lora_config()

            # Apply PEFT
            self.peft_model = get_peft_model(base_model, peft_config)

            # Print trainable parameters
            self._print_trainable_parameters()

            print("‚úÖ PEFT model ready")

        return self.peft_model

    def _create_lora_config(self):
        """T·∫°o LoRA configuration"""

        # T·ª± ƒë·ªông detect target modules based on model type
        target_modules = self._get_target_modules()

        return LoraConfig(
            task_type=self.peft_config.task_type,
            r=self.peft_config.r,                    # Rank
            lora_alpha=self.peft_config.lora_alpha,  # Scaling
            lora_dropout=self.peft_config.lora_dropout,
            target_modules=target_modules,
            bias="none",  # Kh√¥ng train bias
        )

    def _get_target_modules(self):
        """T·ª± ƒë·ªông ch·ªçn target modules based on model type"""
        model_name = self.model_config.model_name_or_path.lower()

        if "bert" in model_name:
            return ["query", "value"]  # BERT attention layers
        elif "llama" in model_name:
            return ["q_proj", "v_proj"]  # LLaMA attention layers
        elif "gpt" in model_name:
            return ["c_attn"]  # GPT attention layers
        else:
            return ["query", "value"]  # Default

    def _print_trainable_parameters(self):
        """In s·ªë l∆∞·ª£ng parameters trainable"""
        trainable_params = 0
        all_params = 0

        for _, param in self.peft_model.named_parameters():
            all_params += param.numel()
            if param.requires_grad:
                trainable_params += param.numel()

        percentage = 100 * trainable_params / all_params

        print(f"Trainable params: {trainable_params:,}")
        print(f"All params: {all_params:,}")
        print(f"Trainable%: {percentage:.2f}%")

    def get_tokenizer(self):
        """Get tokenizer"""
        return self.base_wrapper.load_tokenizer()

    def save_peft_model(self, save_directory: str):
        """Save ch·ªâ PEFT weights (r·∫•t nh·ªè!)"""
        if self.peft_model is not None:
            self.peft_model.save_pretrained(save_directory)
            print(f"‚úÖ PEFT model saved to {save_directory}")
```

**Gi·∫£i th√≠ch:**
- `get_peft_model()`: Function t·ª´ th∆∞ vi·ªán PEFT ƒë·ªÉ wrap model
- `target_modules`: Layers n√†o s·∫Ω ƒë∆∞·ª£c apply LoRA
- `r` (rank): C√†ng nh·ªè c√†ng √≠t parameters, nh∆∞ng c√≥ th·ªÉ gi·∫£m performance
- `lora_alpha`: Scaling factor, th∆∞·ªùng = 2 * r

---

## üèãÔ∏è B∆∞·ªõc 6: Implement Training System

### Training Flow
```
1. Setup model + data
2. Create Trainer v·ªõi callbacks
3. Train v·ªõi monitoring
4. Save best model
```

### 6.1 T·∫°o `training/__init__.py`
```python
"""
Training module exports
"""
from .trainer import PEFTTrainer
from .utils import setup_logging, compute_metrics

__all__ = ["PEFTTrainer", "setup_logging", "compute_metrics"]
```

### 6.2 T·∫°o `training/utils.py`
```python
"""
Training utilities
"""
import logging
import numpy as np
from sklearn.metrics import accuracy_score, f1_score

def setup_logging():
    """Setup logging cho training"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

def compute_metrics(eval_pred):
    """Compute metrics cho evaluation"""
    predictions, labels = eval_pred

    # Convert logits to predictions
    predictions = np.argmax(predictions, axis=1)

    # Calculate metrics
    accuracy = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='weighted')

    return {
        "accuracy": accuracy,
        "f1": f1,
    }
```

### 6.3 T·∫°o `training/trainer.py`
```python
"""
Main PEFT Trainer
"""
import os
from typing import Optional
from datasets import Dataset
from transformers import Trainer, TrainingArguments, DataCollatorWithPadding

from ..models.peft_model import PEFTModelWrapper
from ..data.preprocessing import DataPreprocessor
from ..config.model_config import ModelConfig, PEFTConfig
from ..config.training_config import TrainingConfig
from .utils import setup_logging, compute_metrics

class PEFTTrainer:
    """Main trainer class"""

    def __init__(
        self,
        model_config: ModelConfig,
        peft_config: PEFTConfig,
        training_config: TrainingConfig,
        task_type: str = "classification"
    ):
        self.model_config = model_config
        self.peft_config = peft_config
        self.training_config = training_config
        self.task_type = task_type

        # Setup logging
        setup_logging()

        # Initialize components
        self.model_wrapper = None
        self.trainer = None

    def setup_model(self):
        """Setup PEFT model"""
        if self.model_wrapper is None:
            self.model_wrapper = PEFTModelWrapper(
                self.model_config,
                self.peft_config
            )
            self.model_wrapper.load_model()

        return self.model_wrapper

    def train(
        self,
        train_dataset: Dataset,
        eval_dataset: Optional[Dataset] = None,
        preprocessor: Optional[DataPreprocessor] = None
    ):
        """Main training function"""

        print("üöÄ Starting PEFT training...")

        # Setup model
        self.setup_model()
        model = self.model_wrapper.peft_model
        tokenizer = self.model_wrapper.get_tokenizer()

        # Preprocess datasets
        if preprocessor is not None:
            print("üìä Preprocessing datasets...")
            train_dataset = preprocessor.preprocess_dataset(train_dataset)
            if eval_dataset is not None:
                eval_dataset = preprocessor.preprocess_dataset(eval_dataset)

        # Create training arguments
        training_args = TrainingArguments(
            **self.training_config.to_training_arguments()
        )

        # Create data collator
        data_collator = DataCollatorWithPadding(
            tokenizer=tokenizer,
            padding=True
        )

        # Create trainer
        self.trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=tokenizer,
            data_collator=data_collator,
            compute_metrics=compute_metrics,
        )

        # Start training
        print("üèãÔ∏è Training started...")
        train_result = self.trainer.train()

        # Save model
        self.save_model()

        print("‚úÖ Training completed!")
        return train_result

    def save_model(self):
        """Save trained model"""
        save_dir = self.training_config.output_dir

        # Save PEFT model
        peft_save_dir = os.path.join(save_dir, "peft_model")
        self.model_wrapper.save_peft_model(peft_save_dir)

        # Save tokenizer
        tokenizer = self.model_wrapper.get_tokenizer()
        tokenizer.save_pretrained(save_dir)

        print(f"üíæ Model saved to {save_dir}")
```

**Gi·∫£i th√≠ch:**
- `TrainingArguments`: C·∫•u h√¨nh cho Hugging Face Trainer
- `DataCollatorWithPadding`: T·ª± ƒë·ªông pad sequences trong batch
- `compute_metrics`: Function ƒë·ªÉ t√≠nh accuracy, F1 score
- `self.trainer.train()`: B·∫Øt ƒë·∫ßu training loop

---

## üìä B∆∞·ªõc 7: Implement Evaluation System

### 7.1 T·∫°o `evaluation/__init__.py`
```python
"""
Evaluation module
"""
from .evaluator import ModelEvaluator

__all__ = ["ModelEvaluator"]
```

### 7.2 T·∫°o `evaluation/evaluator.py`
```python
"""
Model evaluation
"""
import torch
import numpy as np
from typing import Dict, Any
from datasets import Dataset

from ..models.peft_model import PEFTModelWrapper

class ModelEvaluator:
    """Model evaluator"""

    def __init__(self, model_wrapper: PEFTModelWrapper):
        self.model_wrapper = model_wrapper
        self.model = model_wrapper.peft_model
        self.tokenizer = model_wrapper.get_tokenizer()

    def evaluate_dataset(self, dataset: Dataset) -> Dict[str, Any]:
        """Evaluate model tr√™n dataset"""

        print("üìä Evaluating model...")

        # Set model to eval mode
        self.model.eval()

        all_predictions = []
        all_labels = []

        # Process dataset
        for example in dataset:
            # Tokenize input
            inputs = self.tokenizer(
                example["text"],
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors="pt"
            )

            # Get prediction
            with torch.no_grad():
                outputs = self.model(**inputs)
                prediction = torch.argmax(outputs.logits, dim=-1)

            all_predictions.append(prediction.item())
            all_labels.append(example["label"])

        # Calculate metrics
        accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))

        results = {
            "accuracy": accuracy,
            "num_examples": len(dataset),
        }

        print(f"‚úÖ Evaluation completed: {results}")
        return results
```

---

## üöÄ B∆∞·ªõc 8: Implement Inference Pipeline

### 8.1 T·∫°o `inference/__init__.py`
```python
"""
Inference module
"""
from .pipeline import InferencePipeline

__all__ = ["InferencePipeline"]
```

### 8.2 T·∫°o `inference/pipeline.py`
```python
"""
Production inference pipeline
"""
import torch
from typing import List, Union
from transformers import pipeline

from ..models.peft_model import PEFTModelWrapper

class InferencePipeline:
    """Easy-to-use inference pipeline"""

    def __init__(self, model_wrapper: PEFTModelWrapper):
        self.model_wrapper = model_wrapper
        self.model = model_wrapper.peft_model
        self.tokenizer = model_wrapper.get_tokenizer()

        # Create HF pipeline
        self.classifier = pipeline(
            "text-classification",
            model=self.model,
            tokenizer=self.tokenizer,
            device=0 if torch.cuda.is_available() else -1
        )

    def classify_text(self, texts: Union[str, List[str]]):
        """Classify single text or list of texts"""

        if isinstance(texts, str):
            # Single text
            result = self.classifier(texts)
            return result[0]
        else:
            # Multiple texts
            results = self.classifier(texts)
            return results

    @classmethod
    def from_pretrained(cls, model_path: str):
        """Load t·ª´ saved model"""
        # Implementation ƒë·ªÉ load saved model
        pass
```

---

## üéØ B∆∞·ªõc 9: T·∫°o Example ƒê∆°n Gi·∫£n

### 9.1 T·∫°o `examples/simple_example.py`
```python
"""
V√≠ d·ª• ƒë∆°n gi·∫£n nh·∫•t
"""
from datasets import load_dataset

# Import our modules
from config import ModelConfig, PEFTConfig, TrainingConfig
from data import TextClassificationPreprocessor
from training import PEFTTrainer
from peft import TaskType

def main():
    """V√≠ d·ª• train model classification ƒë∆°n gi·∫£n"""

    print("üöÄ Starting simple LoRA example...")

    # 1. Setup configs
    model_config = ModelConfig(
        model_name_or_path="distilbert-base-uncased",  # Model nh·ªè ƒë·ªÉ test
        num_labels=2,
        max_length=128  # Ng·∫Øn ƒë·ªÉ train nhanh
    )

    peft_config = PEFTConfig(
        peft_type="LORA",
        task_type=TaskType.SEQ_CLS,
        r=8,  # Rank nh·ªè
        lora_alpha=16
    )

    training_config = TrainingConfig(
        output_dir="./simple_results",
        num_train_epochs=1,  # 1 epoch ƒë·ªÉ test nhanh
        per_device_train_batch_size=16,
        learning_rate=1e-3,
        eval_steps=100,
        logging_steps=50
    )

    # 2. Load dataset
    print("üìä Loading dataset...")
    dataset = load_dataset("imdb")

    # Take small subset ƒë·ªÉ test nhanh
    train_dataset = dataset["train"].select(range(100))
    eval_dataset = dataset["test"].select(range(50))

    # 3. Setup trainer
    trainer = PEFTTrainer(
        model_config=model_config,
        peft_config=peft_config,
        training_config=training_config
    )

    # 4. Setup preprocessor
    model_wrapper = trainer.setup_model()
    tokenizer = model_wrapper.get_tokenizer()

    preprocessor = TextClassificationPreprocessor(
        tokenizer=tokenizer,
        text_column="text",
        label_column="label",
        max_length=128
    )

    # 5. Train
    print("üèãÔ∏è Starting training...")
    trainer.train(
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        preprocessor=preprocessor
    )

    print("‚úÖ Example completed!")

if __name__ == "__main__":
    main()
```

---

## üéâ Ho√†n Th√†nh - B·∫°n ƒê√£ C√≥ H·ªá Th·ªëng Ho√†n Ch·ªânh!

### T√≥m T·∫Øt Nh·ªØng G√¨ ƒê√£ Implement:

1. ‚úÖ **Config System**: Qu·∫£n l√Ω tham s·ªë d·ªÖ d√†ng
2. ‚úÖ **Data Processing**: Load v√† preprocess data
3. ‚úÖ **Model Wrappers**: Wrap models v·ªõi PEFT
4. ‚úÖ **Training System**: Train models v·ªõi monitoring
5. ‚úÖ **Evaluation**: ƒê√°nh gi√° model performance
6. ‚úÖ **Inference**: S·ª≠ d·ª•ng model ƒë√£ train
7. ‚úÖ **Example**: V√≠ d·ª• ho√†n ch·ªânh

### C√°ch Ch·∫°y:
```bash
cd lora-peft
python examples/simple_example.py
```

### Hi·ªÉu ƒê∆∞·ª£c G√¨:
- T·∫°i sao chia code th√†nh modules
- C√°ch PEFT/LoRA ho·∫°t ƒë·ªông
- C√°ch training pipeline ho·∫°t ƒë·ªông
- C√°ch integrate c√°c components

### B∆∞·ªõc Ti·∫øp Theo:
1. Ch·∫°y example ƒë·ªÉ th·∫•y k·∫øt qu·∫£
2. Th·ª≠ thay ƒë·ªïi parameters
3. Th·ª≠ datasets kh√°c
4. Implement th√™m features

**Ch√∫c m·ª´ng! B·∫°n ƒë√£ hi·ªÉu v√† implement ƒë∆∞·ª£c m·ªôt h·ªá th·ªëng LoRA/PEFT ho√†n ch·ªânh t·ª´ s·ªë 0! üéâ**
