# ğŸš€ HÆ°á»›ng Dáº«n Implement LoRA/PEFT Tá»« Sá»‘ 0

HÆ°á»›ng dáº«n nÃ y sáº½ giÃºp báº¡n hiá»ƒu vÃ  xÃ¢y dá»±ng láº¡i toÃ n bá»™ há»‡ thá»‘ng LoRA/PEFT tá»« Ä‘áº§u, tá»«ng bÆ°á»›c má»™t.

## ğŸ“š Kiáº¿n Thá»©c Cáº§n CÃ³ TrÆ°á»›c

### 1. Python CÆ¡ Báº£n
- Classes vÃ  Objects
- Dataclasses
- Type hints
- Import/Export modules

### 2. Machine Learning CÆ¡ Báº£n
- Tokenization (chuyá»ƒn text thÃ nh sá»‘)
- Neural Networks cÆ¡ báº£n
- Training/Validation/Test sets

### 3. ThÆ° Viá»‡n Cáº§n Biáº¿t
- `transformers`: Load models tá»« Hugging Face
- `datasets`: Load vÃ  xá»­ lÃ½ data
- `torch`: PyTorch framework
- `peft`: Parameter Efficient Fine-Tuning

---

## ğŸ—ï¸ BÆ°á»›c 1: Hiá»ƒu Kiáº¿n TrÃºc Tá»•ng Thá»ƒ

### Táº¡i Sao Cáº§n Nhiá»u Files?
```
NguyÃªn táº¯c: "Separation of Concerns" - Má»—i file lÃ m 1 viá»‡c cá»¥ thá»ƒ

config/     â†’ Cáº¥u hÃ¬nh (nhÆ° setting trong game)
data/       â†’ Xá»­ lÃ½ dá»¯ liá»‡u (load vÃ  clean data)
models/     â†’ Quáº£n lÃ½ AI models
training/   â†’ Logic huáº¥n luyá»‡n model
evaluation/ â†’ ÄÃ¡nh giÃ¡ model
inference/  â†’ Sá»­ dá»¥ng model Ä‘Ã£ train
examples/   â†’ VÃ­ dá»¥ cÃ¡ch sá»­ dá»¥ng
```

### Luá»“ng Hoáº¡t Äá»™ng
```
1. Config â†’ Thiáº¿t láº­p tham sá»‘
2. Data â†’ Load vÃ  xá»­ lÃ½ dá»¯ liá»‡u
3. Model â†’ Táº¡o model vá»›i PEFT
4. Training â†’ Huáº¥n luyá»‡n model
5. Evaluation â†’ ÄÃ¡nh giÃ¡ káº¿t quáº£
6. Inference â†’ Sá»­ dá»¥ng model
```

---

## ğŸ”§ BÆ°á»›c 2: Implement Config System

### Táº¡i Sao Cáº§n Config?
```python
# Thay vÃ¬ hard-code nhÆ° nÃ y:
model = AutoModel.from_pretrained("bert-base-uncased")
batch_size = 16
learning_rate = 0.001

# Ta dÃ¹ng config Ä‘á»ƒ dá»… thay Ä‘á»•i:
config = ModelConfig(
    model_name="bert-base-uncased",
    batch_size=16,
    learning_rate=0.001
)
```

### 2.1 Táº¡o `config/__init__.py`
```python
"""
File nÃ y cho Python biáº¿t config lÃ  má»™t package
VÃ  export nhá»¯ng class chÃ­nh Ä‘á»ƒ dá»… import
"""
from .model_config import ModelConfig, PEFTConfig
from .training_config import TrainingConfig

__all__ = ["ModelConfig", "PEFTConfig", "TrainingConfig"]
```

### 2.2 Táº¡o `config/model_config.py`
```python
"""
Cáº¥u hÃ¬nh cho model vÃ  PEFT
"""
from dataclasses import dataclass
from typing import List, Optional
from peft import TaskType

@dataclass
class ModelConfig:
    """Cáº¥u hÃ¬nh cho base model"""
    
    # TÃªn model tá»« Hugging Face Hub
    model_name_or_path: str = "bert-base-uncased"
    
    # Sá»‘ lÆ°á»£ng labels (cho classification)
    num_labels: int = 2
    
    # Äá»™ dÃ i tá»‘i Ä‘a cá»§a sequence
    max_length: int = 512
    
    # CÃ³ dÃ¹ng quantization khÃ´ng (tiáº¿t kiá»‡m memory)
    use_quantization: bool = False
    quantization_bits: int = 4  # 4-bit hoáº·c 8-bit

@dataclass  
class PEFTConfig:
    """Cáº¥u hÃ¬nh cho PEFT methods"""
    
    # Loáº¡i PEFT: LORA, PREFIX_TUNING, etc.
    peft_type: str = "LORA"
    
    # Loáº¡i task: classification, generation, etc.
    task_type: TaskType = TaskType.SEQ_CLS
    
    # LoRA parameters
    r: int = 16              # Rank - cÃ ng nhá» cÃ ng Ã­t parameters
    lora_alpha: int = 32     # Scaling factor
    lora_dropout: float = 0.1 # Dropout Ä‘á»ƒ trÃ¡nh overfitting
    
    # Modules nÃ o sáº½ apply LoRA
    target_modules: Optional[List[str]] = None
```

**Giáº£i thÃ­ch chi tiáº¿t:**
- `@dataclass`: Tá»± Ä‘á»™ng táº¡o `__init__`, `__repr__` methods
- `Optional[List[str]]`: CÃ³ thá»ƒ lÃ  None hoáº·c list of strings
- `TaskType.SEQ_CLS`: Enum tá»« thÆ° viá»‡n PEFT

### 2.3 Táº¡o `config/training_config.py`
```python
"""
Cáº¥u hÃ¬nh cho quÃ¡ trÃ¬nh training
"""
from dataclasses import dataclass
from typing import Optional, Dict, Any

@dataclass
class TrainingConfig:
    """Cáº¥u hÃ¬nh training"""
    
    # ThÆ° má»¥c lÆ°u káº¿t quáº£
    output_dir: str = "./results"
    
    # Training parameters
    num_train_epochs: int = 3           # Sá»‘ epochs
    per_device_train_batch_size: int = 8 # Batch size
    learning_rate: float = 5e-4         # Learning rate
    
    # Evaluation
    evaluation_strategy: str = "steps"   # Khi nÃ o evaluate
    eval_steps: int = 500               # Evaluate má»—i 500 steps
    
    # Logging
    logging_steps: int = 100            # Log má»—i 100 steps
    
    def to_training_arguments(self) -> Dict[str, Any]:
        """Convert sang format cá»§a Hugging Face Trainer"""
        return {
            "output_dir": self.output_dir,
            "num_train_epochs": self.num_train_epochs,
            "per_device_train_batch_size": self.per_device_train_batch_size,
            "learning_rate": self.learning_rate,
            "evaluation_strategy": self.evaluation_strategy,
            "eval_steps": self.eval_steps,
            "logging_steps": self.logging_steps,
        }
```

---

## ğŸ“Š BÆ°á»›c 3: Implement Data Processing

### Táº¡i Sao Cáº§n Data Processing?
```
Raw text: "This movie is great!"
â†“ Tokenization
Token IDs: [101, 2023, 3185, 2003, 2307, 999, 102]
â†“ Padding/Truncation
Fixed length: [101, 2023, 3185, 2003, 2307, 999, 102, 0, 0, 0]
```

### 3.1 Táº¡o `data/__init__.py`
```python
"""
Export main data classes
"""
from .data_loader import DataLoader, load_dataset_from_hub
from .preprocessing import TextClassificationPreprocessor

__all__ = [
    "DataLoader", 
    "load_dataset_from_hub",
    "TextClassificationPreprocessor"
]
```

### 3.2 Táº¡o `data/data_loader.py`
```python
"""
Load datasets tá»« nhiá»u nguá»“n khÃ¡c nhau
"""
import os
import json
from typing import Union, Optional
from datasets import Dataset, DatasetDict, load_dataset

class DataLoader:
    """Class Ä‘á»ƒ load datasets"""
    
    def __init__(self, cache_dir: Optional[str] = None):
        self.cache_dir = cache_dir
    
    def load_from_hub(self, dataset_name: str) -> DatasetDict:
        """Load dataset tá»« Hugging Face Hub"""
        try:
            dataset = load_dataset(dataset_name, cache_dir=self.cache_dir)
            print(f"âœ… Loaded dataset: {dataset_name}")
            return dataset
        except Exception as e:
            print(f"âŒ Error loading {dataset_name}: {e}")
            raise
    
    def load_from_json(self, file_path: str) -> Dataset:
        """Load dataset tá»« JSON file"""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        return Dataset.from_list(data)

def load_dataset_from_hub(dataset_name: str) -> DatasetDict:
    """Convenience function"""
    loader = DataLoader()
    return loader.load_from_hub(dataset_name)
```

**Giáº£i thÃ­ch:**
- `DatasetDict`: Dictionary chá»©a train/validation/test splits
- `Dataset`: Má»™t split cá»¥ thá»ƒ
- `cache_dir`: ThÆ° má»¥c cache Ä‘á»ƒ khÃ´ng download láº¡i

### 3.3 Táº¡o `data/preprocessing.py`
```python
"""
Preprocessing data cho cÃ¡c tasks khÃ¡c nhau
"""
from abc import ABC, abstractmethod
from typing import Dict, List
from datasets import Dataset
from transformers import PreTrainedTokenizer

class DataPreprocessor(ABC):
    """Abstract base class cho preprocessing"""
    
    def __init__(self, tokenizer: PreTrainedTokenizer, max_length: int = 512):
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    @abstractmethod
    def preprocess_function(self, examples: Dict[str, List]) -> Dict[str, List]:
        """Method nÃ y pháº£i Ä‘Æ°á»£c implement bá»Ÿi subclasses"""
        pass
    
    def preprocess_dataset(self, dataset: Dataset) -> Dataset:
        """Apply preprocessing lÃªn toÃ n bá»™ dataset"""
        return dataset.map(
            self.preprocess_function,
            batched=True,  # Xá»­ lÃ½ theo batch Ä‘á»ƒ nhanh hÆ¡n
            desc="Preprocessing dataset"
        )

class TextClassificationPreprocessor(DataPreprocessor):
    """Preprocessor cho text classification"""
    
    def __init__(
        self, 
        tokenizer: PreTrainedTokenizer,
        text_column: str = "text",
        label_column: str = "label",
        max_length: int = 512
    ):
        super().__init__(tokenizer, max_length)
        self.text_column = text_column
        self.label_column = label_column
    
    def preprocess_function(self, examples: Dict[str, List]) -> Dict[str, List]:
        """Tokenize texts vÃ  prepare labels"""
        
        # Tokenize texts
        result = self.tokenizer(
            examples[self.text_column],
            padding=True,           # Pad vá» cÃ¹ng Ä‘á»™ dÃ i
            truncation=True,        # Cáº¯t náº¿u quÃ¡ dÃ i
            max_length=self.max_length,
            return_tensors=None     # Return Python lists, not tensors
        )
        
        # Add labels
        if self.label_column in examples:
            result["labels"] = examples[self.label_column]
        
        return result
```

**Giáº£i thÃ­ch:**
- `ABC`: Abstract Base Class - class cha khÃ´ng thá»ƒ instantiate trá»±c tiáº¿p
- `@abstractmethod`: Method báº¯t buá»™c pháº£i implement
- `batched=True`: Xá»­ lÃ½ nhiá»u examples cÃ¹ng lÃºc
- `padding=True`: ThÃªm padding tokens Ä‘á»ƒ cÃ¹ng Ä‘á»™ dÃ i

---

## ğŸ¤– BÆ°á»›c 4: Implement Model Wrappers

### Táº¡i Sao Cáº§n Wrappers?
```python
# Thay vÃ¬ code phá»©c táº¡p:
model = AutoModelForSequenceClassification.from_pretrained(...)
if use_quantization:
    model = quantize_model(model)
peft_config = LoraConfig(...)
model = get_peft_model(model, peft_config)

# Ta cÃ³ wrapper Ä‘Æ¡n giáº£n:
wrapper = PEFTModelWrapper(model_config, peft_config)
model = wrapper.load_model()
```

### 4.1 Táº¡o `models/__init__.py`
```python
"""
Export model classes
"""
from .base_model import BaseModelWrapper
from .peft_model import PEFTModelWrapper

__all__ = ["BaseModelWrapper", "PEFTModelWrapper"]
```

### 4.2 Táº¡o `models/base_model.py`
```python
"""
Wrapper cho base models
"""
import torch
from typing import Optional
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    BitsAndBytesConfig
)
from peft import TaskType

from ..config.model_config import ModelConfig

class BaseModelWrapper:
    """Wrapper cho base model"""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
    
    def load_tokenizer(self):
        """Load tokenizer"""
        if self.tokenizer is None:
            print(f"Loading tokenizer: {self.config.model_name_or_path}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.model_name_or_path
            )
            
            # Add pad token náº¿u chÆ°a cÃ³
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print(f"âœ… Tokenizer loaded")
        
        return self.tokenizer
    
    def load_model(self, task_type: TaskType):
        """Load model based on task type"""
        if self.model is None:
            print(f"Loading model: {self.config.model_name_or_path}")
            
            # Prepare arguments
            model_kwargs = {}
            
            # Add quantization náº¿u cáº§n
            if self.config.use_quantization:
                model_kwargs["quantization_config"] = self._get_quantization_config()
                model_kwargs["torch_dtype"] = torch.float16
            
            # Load model based on task
            if task_type == TaskType.SEQ_CLS:
                self.model = AutoModelForSequenceClassification.from_pretrained(
                    self.config.model_name_or_path,
                    num_labels=self.config.num_labels,
                    **model_kwargs
                )
            
            print(f"âœ… Model loaded")
        
        return self.model
    
    def _get_quantization_config(self):
        """Táº¡o quantization config"""
        if self.config.quantization_bits == 4:
            return BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
            )
        elif self.config.quantization_bits == 8:
            return BitsAndBytesConfig(load_in_8bit=True)
```

**Giáº£i thÃ­ch:**
- `TaskType.SEQ_CLS`: Sequence Classification
- `quantization_config`: Cáº¥u hÃ¬nh Ä‘á»ƒ giáº£m memory usage
- `torch.float16`: Half precision Ä‘á»ƒ tiáº¿t kiá»‡m memory

---

## â° Táº¡m Dá»«ng - Checkpoint 1

Äáº¿n Ä‘Ã¢y báº¡n Ä‘Ã£ hiá»ƒu:
1. âœ… Táº¡i sao cáº§n chia code thÃ nh nhiá»u files
2. âœ… CÃ¡ch táº¡o config system
3. âœ… CÃ¡ch load vÃ  preprocess data
4. âœ… CÃ¡ch wrap models

**Tiáº¿p theo**: ChÃºng ta sáº½ implement PEFT wrapper, training system, vÃ  evaluation.

---

## ğŸ§© BÆ°á»›c 5: Implement PEFT Model Wrapper

### Táº¡i Sao Cáº§n PEFT?
```
Full Fine-tuning: Train toÃ n bá»™ 110M parameters cá»§a BERT
LoRA: Chá»‰ train 0.3M parameters má»›i (99.7% Ã­t hÆ¡n!)

Káº¿t quáº£: Gáº§n nhÆ° same accuracy, nhÆ°ng:
- Ãt memory hÆ¡n
- Train nhanh hÆ¡n
- Dá»… share model hÆ¡n (chá»‰ cáº§n share LoRA weights)
```

### 5.1 Táº¡o `models/peft_model.py`
```python
"""
PEFT Model Wrapper
"""
import torch
from typing import Optional
from peft import get_peft_model, LoraConfig, TaskType

from .base_model import BaseModelWrapper
from ..config.model_config import ModelConfig, PEFTConfig

class PEFTModelWrapper:
    """Wrapper cho PEFT models"""

    def __init__(self, model_config: ModelConfig, peft_config: PEFTConfig):
        self.model_config = model_config
        self.peft_config = peft_config
        self.base_wrapper = BaseModelWrapper(model_config)
        self.peft_model = None

    def load_model(self):
        """Load vÃ  setup PEFT model"""
        if self.peft_model is None:
            print("Setting up PEFT model...")

            # Load base model
            base_model = self.base_wrapper.load_model(self.peft_config.task_type)

            # Create PEFT config
            peft_config = self._create_lora_config()

            # Apply PEFT
            self.peft_model = get_peft_model(base_model, peft_config)

            # Print trainable parameters
            self._print_trainable_parameters()

            print("âœ… PEFT model ready")

        return self.peft_model

    def _create_lora_config(self):
        """Táº¡o LoRA configuration"""

        # Tá»± Ä‘á»™ng detect target modules based on model type
        target_modules = self._get_target_modules()

        return LoraConfig(
            task_type=self.peft_config.task_type,
            r=self.peft_config.r,                    # Rank
            lora_alpha=self.peft_config.lora_alpha,  # Scaling
            lora_dropout=self.peft_config.lora_dropout,
            target_modules=target_modules,
            bias="none",  # KhÃ´ng train bias
        )

    def _get_target_modules(self):
        """Tá»± Ä‘á»™ng chá»n target modules based on model type"""
        model_name = self.model_config.model_name_or_path.lower()

        if "bert" in model_name:
            return ["query", "value"]  # BERT attention layers
        elif "llama" in model_name:
            return ["q_proj", "v_proj"]  # LLaMA attention layers
        elif "gpt" in model_name:
            return ["c_attn"]  # GPT attention layers
        else:
            return ["query", "value"]  # Default

    def _print_trainable_parameters(self):
        """In sá»‘ lÆ°á»£ng parameters trainable"""
        trainable_params = 0
        all_params = 0

        for _, param in self.peft_model.named_parameters():
            all_params += param.numel()
            if param.requires_grad:
                trainable_params += param.numel()

        percentage = 100 * trainable_params / all_params

        print(f"Trainable params: {trainable_params:,}")
        print(f"All params: {all_params:,}")
        print(f"Trainable%: {percentage:.2f}%")

    def get_tokenizer(self):
        """Get tokenizer"""
        return self.base_wrapper.load_tokenizer()

    def save_peft_model(self, save_directory: str):
        """Save chá»‰ PEFT weights (ráº¥t nhá»!)"""
        if self.peft_model is not None:
            self.peft_model.save_pretrained(save_directory)
            print(f"âœ… PEFT model saved to {save_directory}")
```

**Giáº£i thÃ­ch:**
- `get_peft_model()`: Function tá»« thÆ° viá»‡n PEFT Ä‘á»ƒ wrap model
- `target_modules`: Layers nÃ o sáº½ Ä‘Æ°á»£c apply LoRA
- `r` (rank): CÃ ng nhá» cÃ ng Ã­t parameters, nhÆ°ng cÃ³ thá»ƒ giáº£m performance
- `lora_alpha`: Scaling factor, thÆ°á»ng = 2 * r

---

## ğŸ‹ï¸ BÆ°á»›c 6: Implement Training System

### Training Flow
```
1. Setup model + data
2. Create Trainer vá»›i callbacks
3. Train vá»›i monitoring
4. Save best model
```

### 6.1 Táº¡o `training/__init__.py`
```python
"""
Training module exports
"""
from .trainer import PEFTTrainer
from .utils import setup_logging, compute_metrics

__all__ = ["PEFTTrainer", "setup_logging", "compute_metrics"]
```

### 6.2 Táº¡o `training/utils.py`
```python
"""
Training utilities
"""
import logging
import numpy as np
from sklearn.metrics import accuracy_score, f1_score

def setup_logging():
    """Setup logging cho training"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

def compute_metrics(eval_pred):
    """Compute metrics cho evaluation"""
    predictions, labels = eval_pred

    # Convert logits to predictions
    predictions = np.argmax(predictions, axis=1)

    # Calculate metrics
    accuracy = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='weighted')

    return {
        "accuracy": accuracy,
        "f1": f1,
    }
```

### 6.3 Táº¡o `training/trainer.py`
```python
"""
Main PEFT Trainer
"""
import os
from typing import Optional
from datasets import Dataset
from transformers import Trainer, TrainingArguments, DataCollatorWithPadding

from ..models.peft_model import PEFTModelWrapper
from ..data.preprocessing import DataPreprocessor
from ..config.model_config import ModelConfig, PEFTConfig
from ..config.training_config import TrainingConfig
from .utils import setup_logging, compute_metrics

class PEFTTrainer:
    """Main trainer class"""

    def __init__(
        self,
        model_config: ModelConfig,
        peft_config: PEFTConfig,
        training_config: TrainingConfig,
        task_type: str = "classification"
    ):
        self.model_config = model_config
        self.peft_config = peft_config
        self.training_config = training_config
        self.task_type = task_type

        # Setup logging
        setup_logging()

        # Initialize components
        self.model_wrapper = None
        self.trainer = None

    def setup_model(self):
        """Setup PEFT model"""
        if self.model_wrapper is None:
            self.model_wrapper = PEFTModelWrapper(
                self.model_config,
                self.peft_config
            )
            self.model_wrapper.load_model()

        return self.model_wrapper

    def train(
        self,
        train_dataset: Dataset,
        eval_dataset: Optional[Dataset] = None,
        preprocessor: Optional[DataPreprocessor] = None
    ):
        """Main training function"""

        print("ğŸš€ Starting PEFT training...")

        # Setup model
        self.setup_model()
        model = self.model_wrapper.peft_model
        tokenizer = self.model_wrapper.get_tokenizer()

        # Preprocess datasets
        if preprocessor is not None:
            print("ğŸ“Š Preprocessing datasets...")
            train_dataset = preprocessor.preprocess_dataset(train_dataset)
            if eval_dataset is not None:
                eval_dataset = preprocessor.preprocess_dataset(eval_dataset)

        # Create training arguments
        training_args = TrainingArguments(
            **self.training_config.to_training_arguments()
        )

        # Create data collator
        data_collator = DataCollatorWithPadding(
            tokenizer=tokenizer,
            padding=True
        )

        # Create trainer
        self.trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=tokenizer,
            data_collator=data_collator,
            compute_metrics=compute_metrics,
        )

        # Start training
        print("ğŸ‹ï¸ Training started...")
        train_result = self.trainer.train()

        # Save model
        self.save_model()

        print("âœ… Training completed!")
        return train_result

    def save_model(self):
        """Save trained model"""
        save_dir = self.training_config.output_dir

        # Save PEFT model
        peft_save_dir = os.path.join(save_dir, "peft_model")
        self.model_wrapper.save_peft_model(peft_save_dir)

        # Save tokenizer
        tokenizer = self.model_wrapper.get_tokenizer()
        tokenizer.save_pretrained(save_dir)

        print(f"ğŸ’¾ Model saved to {save_dir}")
```

**Giáº£i thÃ­ch:**
- `TrainingArguments`: Cáº¥u hÃ¬nh cho Hugging Face Trainer
- `DataCollatorWithPadding`: Tá»± Ä‘á»™ng pad sequences trong batch
- `compute_metrics`: Function Ä‘á»ƒ tÃ­nh accuracy, F1 score
- `self.trainer.train()`: Báº¯t Ä‘áº§u training loop

---

## ğŸ“Š BÆ°á»›c 7: Implement Evaluation System

### 7.1 Táº¡o `evaluation/__init__.py`
```python
"""
Evaluation module
"""
from .evaluator import ModelEvaluator

__all__ = ["ModelEvaluator"]
```

### 7.2 Táº¡o `evaluation/evaluator.py`
```python
"""
Model evaluation
"""
import torch
import numpy as np
from typing import Dict, Any
from datasets import Dataset

from ..models.peft_model import PEFTModelWrapper

class ModelEvaluator:
    """Model evaluator"""

    def __init__(self, model_wrapper: PEFTModelWrapper):
        self.model_wrapper = model_wrapper
        self.model = model_wrapper.peft_model
        self.tokenizer = model_wrapper.get_tokenizer()

    def evaluate_dataset(self, dataset: Dataset) -> Dict[str, Any]:
        """Evaluate model trÃªn dataset"""

        print("ğŸ“Š Evaluating model...")

        # Set model to eval mode
        self.model.eval()

        all_predictions = []
        all_labels = []

        # Process dataset
        for example in dataset:
            # Tokenize input
            inputs = self.tokenizer(
                example["text"],
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors="pt"
            )

            # Get prediction
            with torch.no_grad():
                outputs = self.model(**inputs)
                prediction = torch.argmax(outputs.logits, dim=-1)

            all_predictions.append(prediction.item())
            all_labels.append(example["label"])

        # Calculate metrics
        accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))

        results = {
            "accuracy": accuracy,
            "num_examples": len(dataset),
        }

        print(f"âœ… Evaluation completed: {results}")
        return results
```

---

## ğŸš€ BÆ°á»›c 8: Implement Inference Pipeline

### 8.1 Táº¡o `inference/__init__.py`
```python
"""
Inference module
"""
from .pipeline import InferencePipeline

__all__ = ["InferencePipeline"]
```

### 8.2 Táº¡o `inference/pipeline.py`
```python
"""
Production inference pipeline
"""
import torch
from typing import List, Union
from transformers import pipeline

from ..models.peft_model import PEFTModelWrapper

class InferencePipeline:
    """Easy-to-use inference pipeline"""

    def __init__(self, model_wrapper: PEFTModelWrapper):
        self.model_wrapper = model_wrapper
        self.model = model_wrapper.peft_model
        self.tokenizer = model_wrapper.get_tokenizer()

        # Create HF pipeline
        self.classifier = pipeline(
            "text-classification",
            model=self.model,
            tokenizer=self.tokenizer,
            device=0 if torch.cuda.is_available() else -1
        )

    def classify_text(self, texts: Union[str, List[str]]):
        """Classify single text or list of texts"""

        if isinstance(texts, str):
            # Single text
            result = self.classifier(texts)
            return result[0]
        else:
            # Multiple texts
            results = self.classifier(texts)
            return results

    @classmethod
    def from_pretrained(cls, model_path: str):
        """Load tá»« saved model"""
        # Implementation Ä‘á»ƒ load saved model
        pass
```

---

## ğŸ¯ BÆ°á»›c 9: Táº¡o Example ÄÆ¡n Giáº£n

### 9.1 Táº¡o `examples/simple_example.py`
```python
"""
VÃ­ dá»¥ Ä‘Æ¡n giáº£n nháº¥t
"""
from datasets import load_dataset

# Import our modules
from config import ModelConfig, PEFTConfig, TrainingConfig
from data import TextClassificationPreprocessor
from training import PEFTTrainer
from peft import TaskType

def main():
    """VÃ­ dá»¥ train model classification Ä‘Æ¡n giáº£n"""

    print("ğŸš€ Starting simple LoRA example...")

    # 1. Setup configs
    model_config = ModelConfig(
        model_name_or_path="distilbert-base-uncased",  # Model nhá» Ä‘á»ƒ test
        num_labels=2,
        max_length=128  # Ngáº¯n Ä‘á»ƒ train nhanh
    )

    peft_config = PEFTConfig(
        peft_type="LORA",
        task_type=TaskType.SEQ_CLS,
        r=8,  # Rank nhá»
        lora_alpha=16
    )

    training_config = TrainingConfig(
        output_dir="./simple_results",
        num_train_epochs=1,  # 1 epoch Ä‘á»ƒ test nhanh
        per_device_train_batch_size=16,
        learning_rate=1e-3,
        eval_steps=100,
        logging_steps=50
    )

    # 2. Load dataset
    print("ğŸ“Š Loading dataset...")
    dataset = load_dataset("imdb")

    # Take small subset Ä‘á»ƒ test nhanh
    train_dataset = dataset["train"].select(range(100))
    eval_dataset = dataset["test"].select(range(50))

    # 3. Setup trainer
    trainer = PEFTTrainer(
        model_config=model_config,
        peft_config=peft_config,
        training_config=training_config
    )

    # 4. Setup preprocessor
    model_wrapper = trainer.setup_model()
    tokenizer = model_wrapper.get_tokenizer()

    preprocessor = TextClassificationPreprocessor(
        tokenizer=tokenizer,
        text_column="text",
        label_column="label",
        max_length=128
    )

    # 5. Train
    print("ğŸ‹ï¸ Starting training...")
    trainer.train(
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        preprocessor=preprocessor
    )

    print("âœ… Example completed!")

if __name__ == "__main__":
    main()
```

---

## ğŸ‰ HoÃ n ThÃ nh - Báº¡n ÄÃ£ CÃ³ Há»‡ Thá»‘ng HoÃ n Chá»‰nh!

### TÃ³m Táº¯t Nhá»¯ng GÃ¬ ÄÃ£ Implement:

1. âœ… **Config System**: Quáº£n lÃ½ tham sá»‘ dá»… dÃ ng
2. âœ… **Data Processing**: Load vÃ  preprocess data
3. âœ… **Model Wrappers**: Wrap models vá»›i PEFT
4. âœ… **Training System**: Train models vá»›i monitoring
5. âœ… **Evaluation**: ÄÃ¡nh giÃ¡ model performance
6. âœ… **Inference**: Sá»­ dá»¥ng model Ä‘Ã£ train
7. âœ… **Example**: VÃ­ dá»¥ hoÃ n chá»‰nh

### CÃ¡ch Cháº¡y:
```bash
cd lora-peft
python examples/simple_example.py
```

### Hiá»ƒu ÄÆ°á»£c GÃ¬:
- Táº¡i sao chia code thÃ nh modules
- CÃ¡ch PEFT/LoRA hoáº¡t Ä‘á»™ng
- CÃ¡ch training pipeline hoáº¡t Ä‘á»™ng
- CÃ¡ch integrate cÃ¡c components

### BÆ°á»›c Tiáº¿p Theo:
1. Cháº¡y example Ä‘á»ƒ tháº¥y káº¿t quáº£
2. Thá»­ thay Ä‘á»•i parameters
3. Thá»­ datasets khÃ¡c
4. Implement thÃªm features

**ChÃºc má»«ng! Báº¡n Ä‘Ã£ hiá»ƒu vÃ  implement Ä‘Æ°á»£c má»™t há»‡ thá»‘ng LoRA/PEFT hoÃ n chá»‰nh tá»« sá»‘ 0! ğŸ‰**
