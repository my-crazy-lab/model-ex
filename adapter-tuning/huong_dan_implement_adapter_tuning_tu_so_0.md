# ğŸ”§ HÆ°á»›ng Dáº«n Implement Adapter Tuning Tá»« Sá»‘ 0

HÆ°á»›ng dáº«n nÃ y sáº½ giÃºp báº¡n hiá»ƒu vÃ  xÃ¢y dá»±ng láº¡i toÃ n bá»™ há»‡ thá»‘ng Adapter Tuning tá»« Ä‘áº§u, tá»«ng bÆ°á»›c má»™t.

## ğŸ“š Kiáº¿n Thá»©c Cáº§n CÃ³ TrÆ°á»›c

### 1. Python CÆ¡ Báº£n
- Classes vÃ  Objects
- Inheritance vÃ  Abstract classes
- Type hints
- Context managers

### 2. Deep Learning CÆ¡ Báº£n
- Neural Networks
- Transformer architecture
- Fine-tuning vs Transfer learning
- Parameter-efficient methods

### 3. ThÆ° Viá»‡n Cáº§n Biáº¿t
- `torch`: PyTorch framework
- `transformers`: Hugging Face transformers
- `datasets`: Data loading vÃ  processing
- `sklearn`: Metrics calculation

---

## ğŸ¯ Adapter Tuning LÃ  GÃ¬?

### Váº¥n Äá» Vá»›i Full Fine-tuning
```
BERT-base: 110M parameters
Fine-tune cho 1 task â†’ Train toÃ n bá»™ 110M parameters
Fine-tune cho 10 tasks â†’ 10 Ã— 110M = 1.1B parameters storage!
```

### Giáº£i PhÃ¡p: Adapter Tuning
```
Base Model: 110M parameters (freeze - khÃ´ng train)
Adapter: 0.5M parameters (train)
â†’ Chá»‰ cáº§n 0.45% parameters Ä‘á»ƒ Ä‘áº¡t performance tÆ°Æ¡ng tá»±!
```

### Kiáº¿n TrÃºc Adapter
```
Input (768 dim) 
    â†“
Down-projection (768 â†’ 64)  # Bottleneck
    â†“
Activation (ReLU/GELU)
    â†“
Up-projection (64 â†’ 768)
    â†“
Residual Connection (+)
    â†“
Output (768 dim)
```

---

## ğŸ—ï¸ BÆ°á»›c 1: Hiá»ƒu Kiáº¿n TrÃºc Tá»•ng Thá»ƒ

### Táº¡i Sao Cáº§n Nhiá»u Files?
```
NguyÃªn táº¯c: "Single Responsibility Principle"

config/         â†’ Quáº£n lÃ½ cáº¥u hÃ¬nh
adapters/       â†’ Core adapter implementation
data/           â†’ Data loading vÃ  preprocessing
training/       â†’ Training logic
evaluation/     â†’ Model evaluation
inference/      â†’ Production inference
examples/       â†’ VÃ­ dá»¥ sá»­ dá»¥ng
```

### Luá»“ng Hoáº¡t Äá»™ng
```
1. Config â†’ Thiáº¿t láº­p parameters
2. Adapters â†’ Táº¡o adapter modules
3. Data â†’ Load vÃ  preprocess data
4. Training â†’ Train adapters (freeze base model)
5. Evaluation â†’ ÄÃ¡nh giÃ¡ performance
6. Inference â†’ Sá»­ dá»¥ng model Ä‘Ã£ train
```

---

## ğŸ”§ BÆ°á»›c 2: Implement Core Adapter Layer

### 2.1 Hiá»ƒu Bottleneck Architecture

**Táº¡i sao dÃ¹ng Bottleneck?**
```python
# Thay vÃ¬ train full linear layer:
# 768 Ã— 768 = 589,824 parameters

# Ta dÃ¹ng bottleneck:
# Down: 768 Ã— 64 = 49,152 parameters
# Up: 64 Ã— 768 = 49,152 parameters
# Total: 98,304 parameters (6x Ã­t hÆ¡n!)
```

### 2.2 Táº¡o `adapters/adapter_layer.py`

```python
"""
Core adapter implementation - TrÃ¡i tim cá»§a há»‡ thá»‘ng
"""
import torch
import torch.nn as nn
import torch.nn.functional as F

class BottleneckAdapter(nn.Module):
    """
    Adapter vá»›i kiáº¿n trÃºc bottleneck
    
    Luá»“ng: Input â†’ Down â†’ Activation â†’ Up â†’ Residual â†’ Output
    """
    
    def __init__(
        self,
        input_size: int,      # KÃ­ch thÆ°á»›c input (768 cho BERT)
        adapter_size: int,    # KÃ­ch thÆ°á»›c bottleneck (64, 128, etc.)
        dropout: float = 0.1, # Dropout Ä‘á»ƒ trÃ¡nh overfitting
        activation: str = "relu"  # Activation function
    ):
        super().__init__()
        
        # LÆ°u parameters
        self.input_size = input_size
        self.adapter_size = adapter_size
        
        # Down-projection: giáº£m dimension
        self.down_project = nn.Linear(input_size, adapter_size)
        
        # Up-projection: tÄƒng dimension vá» ban Ä‘áº§u
        self.up_project = nn.Linear(adapter_size, input_size)
        
        # Activation function
        if activation == "relu":
            self.activation = F.relu
        elif activation == "gelu":
            self.activation = F.gelu
        else:
            raise ValueError(f"Unsupported activation: {activation}")
        
        # Dropout layer
        self.dropout = nn.Dropout(dropout)
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        """Initialize adapter weights"""
        # Initialize down projection normally
        nn.init.normal_(self.down_project.weight, std=0.02)
        nn.init.zeros_(self.down_project.bias)
        
        # Initialize up projection to ZERO - quan trá»ng!
        # Äiá»u nÃ y Ä‘áº£m báº£o adapter ban Ä‘áº§u khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n model
        nn.init.zeros_(self.up_project.weight)
        nn.init.zeros_(self.up_project.bias)
    
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """
        Forward pass qua adapter
        
        Args:
            hidden_states: [batch_size, seq_len, input_size]
        Returns:
            output: [batch_size, seq_len, input_size]
        """
        # LÆ°u input cho residual connection
        residual = hidden_states
        
        # Down-projection
        hidden_states = self.down_project(hidden_states)
        
        # Activation
        hidden_states = self.activation(hidden_states)
        
        # Dropout
        hidden_states = self.dropout(hidden_states)
        
        # Up-projection
        hidden_states = self.up_project(hidden_states)
        
        # Residual connection - QUAN TRá»ŒNG!
        # Äáº£m báº£o gradient flow vÃ  stable training
        output = residual + hidden_states
        
        return output
```

**Giáº£i thÃ­ch chi tiáº¿t:**
- `down_project`: Giáº£m dimension tá»« 768 â†’ 64 (compression)
- `activation`: Non-linearity Ä‘á»ƒ há»c complex patterns
- `up_project`: TÄƒng dimension tá»« 64 â†’ 768 (decompression)
- `residual`: Skip connection Ä‘á»ƒ stable training
- `_init_weights()`: Initialize up_project = 0 Ä‘á»ƒ adapter ban Ä‘áº§u khÃ´ng áº£nh hÆ°á»Ÿng

---

## ğŸ¤– BÆ°á»›c 3: Implement Model Wrapper

### 3.1 Táº¡i Sao Cáº§n Model Wrapper?

```python
# Thay vÃ¬ modify trá»±c tiáº¿p transformer:
model = AutoModel.from_pretrained("bert-base-uncased")
# Pháº£i manually add adapters vÃ o tá»«ng layer...

# Ta dÃ¹ng wrapper Ä‘á»ƒ tá»± Ä‘á»™ng:
adapter_model = AdapterModel(model_config, adapter_config)
# Tá»± Ä‘á»™ng add adapters vÃ o Ä‘Ãºng vá»‹ trÃ­!
```

### 3.2 Táº¡o `adapters/adapter_model.py`

```python
"""
Model wrapper Ä‘á»ƒ add adapters vÃ o pre-trained models
"""
import torch.nn as nn
from transformers import AutoModelForSequenceClassification

class AdapterModel(nn.Module):
    """Wrapper Ä‘á»ƒ add adapters vÃ o transformer model"""
    
    def __init__(self, model_config, adapter_config):
        super().__init__()
        
        # Load base model
        self.base_model = AutoModelForSequenceClassification.from_pretrained(
            model_config.model_name_or_path,
            num_labels=model_config.num_labels
        )
        
        # Store configs
        self.model_config = model_config
        self.adapter_config = adapter_config
        
        # Add adapters to model
        self._add_adapters()
        
        # Freeze base model parameters
        if adapter_config.freeze_base_model:
            self._freeze_base_model()
    
    def _add_adapters(self):
        """Add adapters to transformer layers"""
        
        # Get encoder layers (BERT example)
        if hasattr(self.base_model, 'bert'):
            encoder_layers = self.base_model.bert.encoder.layer
        else:
            raise ValueError("Unsupported model architecture")
        
        # Add adapter to each layer
        for layer_idx, layer in enumerate(encoder_layers):
            
            # Get hidden size from layer
            hidden_size = layer.output.dense.out_features
            
            # Create adapter
            adapter = BottleneckAdapter(
                input_size=hidden_size,
                adapter_size=self.adapter_config.adapter_size,
                dropout=self.adapter_config.adapter_dropout,
                activation=self.adapter_config.adapter_activation
            )
            
            # Add adapter to layer
            layer.adapter = adapter
            
            # Modify layer forward method
            self._modify_layer_forward(layer)
    
    def _modify_layer_forward(self, layer):
        """Modify layer forward Ä‘á»ƒ include adapter"""
        
        # LÆ°u original forward method
        original_forward = layer.forward
        
        def forward_with_adapter(hidden_states, attention_mask=None, **kwargs):
            # Gá»i original forward
            outputs = original_forward(hidden_states, attention_mask, **kwargs)
            
            # Extract hidden states
            if isinstance(outputs, tuple):
                hidden_states = outputs[0]
            else:
                hidden_states = outputs
            
            # Apply adapter
            if hasattr(layer, 'adapter'):
                hidden_states = layer.adapter(hidden_states)
            
            # Return in original format
            if isinstance(outputs, tuple):
                return (hidden_states,) + outputs[1:]
            else:
                return hidden_states
        
        # Replace forward method
        layer.forward = forward_with_adapter
    
    def _freeze_base_model(self):
        """Freeze base model parameters"""
        for name, param in self.base_model.named_parameters():
            if "adapter" not in name:
                param.requires_grad = False
    
    def forward(self, **kwargs):
        """Forward pass through model"""
        return self.base_model(**kwargs)
    
    def get_adapter_parameters(self):
        """Get only adapter parameters for optimization"""
        return [p for n, p in self.named_parameters() if "adapter" in n]
    
    def print_adapter_info(self):
        """Print adapter information"""
        total_params = sum(p.numel() for p in self.parameters())
        adapter_params = sum(p.numel() for p in self.get_adapter_parameters())
        
        print(f"Total parameters: {total_params:,}")
        print(f"Adapter parameters: {adapter_params:,}")
        print(f"Adapter percentage: {adapter_params/total_params*100:.2f}%")
```

**Giáº£i thÃ­ch chi tiáº¿t:**
- `_add_adapters()`: Tá»± Ä‘á»™ng detect vÃ  add adapters vÃ o má»—i transformer layer
- `_modify_layer_forward()`: Modify forward method Ä‘á»ƒ call adapter sau original computation
- `_freeze_base_model()`: Freeze táº¥t cáº£ parameters trá»« adapters
- `get_adapter_parameters()`: Láº¥y chá»‰ adapter parameters Ä‘á»ƒ optimize

---

## â° Táº¡m Dá»«ng - Checkpoint 1

Äáº¿n Ä‘Ã¢y báº¡n Ä‘Ã£ hiá»ƒu:
1. âœ… Adapter architecture vÃ  táº¡i sao nÃ³ hiá»‡u quáº£
2. âœ… CÃ¡ch implement bottleneck adapter
3. âœ… CÃ¡ch integrate adapter vÃ o pre-trained model
4. âœ… Parameter freezing vÃ  selective training

**Tiáº¿p theo**: ChÃºng ta sáº½ implement data processing, training system, vÃ  evaluation.

---

## ğŸ“Š BÆ°á»›c 4: Implement Configuration System

### 4.1 Táº¡i Sao Cáº§n Config System?

```python
# Thay vÃ¬ hard-code:
adapter = BottleneckAdapter(768, 64, 0.1, "relu")
model = AutoModel.from_pretrained("bert-base-uncased")

# Ta dÃ¹ng config Ä‘á»ƒ dá»… thay Ä‘á»•i:
adapter_config = AdapterConfig(adapter_size=64, dropout=0.1)
model_config = ModelConfig(model_name="bert-base-uncased")
```

### 4.2 Táº¡o `config/adapter_config.py`

```python
"""
Configuration cho adapters
"""
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class AdapterConfig:
    """Configuration cho adapter modules"""

    # Basic adapter parameters
    adapter_size: int = 64              # Bottleneck dimension
    adapter_dropout: float = 0.1        # Dropout rate
    adapter_activation: str = "relu"    # Activation function

    # Adapter placement
    adapter_location: str = "both"      # "attention", "feedforward", "both"
    adapter_layers: Optional[List[int]] = None  # Which layers (None = all)

    # Training settings
    freeze_base_model: bool = True      # Freeze base model
    train_adapter_only: bool = True     # Train only adapters

    # Advanced options
    use_residual: bool = True           # Use residual connections
    adapter_init_range: float = 1e-3   # Weight initialization range

    def __post_init__(self):
        """Validate configuration"""
        valid_activations = ["relu", "gelu", "swish", "tanh"]
        if self.adapter_activation not in valid_activations:
            raise ValueError(f"activation must be one of {valid_activations}")

        valid_locations = ["attention", "feedforward", "both"]
        if self.adapter_location not in valid_locations:
            raise ValueError(f"location must be one of {valid_locations}")

    def get_adapter_size(self, hidden_size: int) -> int:
        """Calculate adapter size based on hidden size"""
        return self.adapter_size

    def should_add_adapter(self, layer_idx: int) -> bool:
        """Check if should add adapter to specific layer"""
        if self.adapter_layers is None:
            return True
        return layer_idx in self.adapter_layers
```

### 4.3 Táº¡o `config/model_config.py`

```python
"""
Configuration cho base models
"""
from dataclasses import dataclass
from typing import Optional

@dataclass
class ModelConfig:
    """Configuration cho base model"""

    # Model identification
    model_name_or_path: str = "bert-base-uncased"
    tokenizer_name_or_path: Optional[str] = None

    # Model parameters
    num_labels: int = 2                 # Number of output labels
    max_length: int = 512               # Max sequence length

    # Task configuration
    task_type: str = "classification"   # Task type

    def __post_init__(self):
        if self.tokenizer_name_or_path is None:
            self.tokenizer_name_or_path = self.model_name_or_path
```

**Giáº£i thÃ­ch:**
- `@dataclass`: Tá»± Ä‘á»™ng táº¡o `__init__`, `__repr__` methods
- `__post_init__`: Validation sau khi táº¡o object
- Type hints: GiÃºp IDE hiá»ƒu vÃ  check types

---

## ğŸ“Š BÆ°á»›c 5: Implement Data Processing

### 5.1 Táº¡i Sao Cáº§n Data Processing?

```
Raw text: "This movie is great!"
â†“ Tokenization
Token IDs: [101, 2023, 3185, 2003, 2307, 999, 102]
â†“ Padding/Truncation
Fixed length: [101, 2023, 3185, 2003, 2307, 999, 102, 0, 0, 0]
â†“ Attention mask
Mask: [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]
```

### 5.2 Táº¡o `data/preprocessing.py`

```python
"""
Data preprocessing cho adapter tuning
"""
from abc import ABC, abstractmethod
from typing import Dict, List
from datasets import Dataset
from transformers import PreTrainedTokenizer

class DataPreprocessor(ABC):
    """Abstract base class cho preprocessing"""

    def __init__(self, tokenizer: PreTrainedTokenizer, max_length: int = 512):
        self.tokenizer = tokenizer
        self.max_length = max_length

    @abstractmethod
    def preprocess_function(self, examples: Dict[str, List]) -> Dict[str, List]:
        """Method nÃ y pháº£i Ä‘Æ°á»£c implement bá»Ÿi subclasses"""
        pass

    def preprocess_dataset(self, dataset: Dataset) -> Dataset:
        """Apply preprocessing lÃªn toÃ n bá»™ dataset"""
        return dataset.map(
            self.preprocess_function,
            batched=True,
            desc="Preprocessing dataset"
        )

class TextClassificationPreprocessor(DataPreprocessor):
    """Preprocessor cho text classification"""

    def __init__(
        self,
        tokenizer: PreTrainedTokenizer,
        text_column: str = "text",
        label_column: str = "label",
        max_length: int = 512
    ):
        super().__init__(tokenizer, max_length)
        self.text_column = text_column
        self.label_column = label_column

    def preprocess_function(self, examples: Dict[str, List]) -> Dict[str, List]:
        """Tokenize texts vÃ  prepare labels"""

        # Tokenize texts
        result = self.tokenizer(
            examples[self.text_column],
            padding=True,           # Pad vá» cÃ¹ng Ä‘á»™ dÃ i
            truncation=True,        # Cáº¯t náº¿u quÃ¡ dÃ i
            max_length=self.max_length,
            return_tensors=None     # Return Python lists
        )

        # Add labels
        if self.label_column in examples:
            result["labels"] = examples[self.label_column]

        return result
```

**Giáº£i thÃ­ch:**
- `ABC`: Abstract Base Class - class cha khÃ´ng thá»ƒ instantiate
- `@abstractmethod`: Method báº¯t buá»™c pháº£i implement
- `batched=True`: Xá»­ lÃ½ nhiá»u examples cÃ¹ng lÃºc Ä‘á»ƒ nhanh hÆ¡n
- `padding=True`: ThÃªm padding tokens Ä‘á»ƒ cÃ¹ng Ä‘á»™ dÃ i

---

## ğŸ‹ï¸ BÆ°á»›c 6: Implement Training System

### 6.1 Training Flow

```
1. Setup model + data
2. Create optimizer (chá»‰ cho adapter parameters)
3. Training loop vá»›i monitoring
4. Save adapters (khÃ´ng save base model)
```

### 6.2 Táº¡o `training/trainer.py`

```python
"""
Adapter trainer implementation
"""
from typing import Optional
from datasets import Dataset
from transformers import Trainer, TrainingArguments, DataCollatorWithPadding
import torch.optim as optim

class AdapterTrainer:
    """Main trainer cho adapter tuning"""

    def __init__(
        self,
        model_config,
        adapter_config,
        training_config,
        task_type: str = "classification"
    ):
        self.model_config = model_config
        self.adapter_config = adapter_config
        self.training_config = training_config
        self.task_type = task_type

        # Initialize components
        self.adapter_model = None
        self.tokenizer = None
        self.trainer = None

    def setup_model(self):
        """Setup adapter model vÃ  tokenizer"""
        if self.adapter_model is None:
            # Load tokenizer
            from transformers import AutoTokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_config.tokenizer_name_or_path
            )

            # Add pad token náº¿u chÆ°a cÃ³
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            # Create adapter model
            self.adapter_model = AdapterModel(
                self.model_config,
                self.adapter_config
            )

            print("âœ… Adapter model setup completed")
            self.adapter_model.print_adapter_info()

        return self.adapter_model

    def train(
        self,
        train_dataset: Dataset,
        eval_dataset: Optional[Dataset] = None,
        preprocessor = None
    ):
        """Main training function"""

        print("ğŸš€ Starting adapter training...")

        # Setup model
        self.setup_model()

        # Preprocess datasets
        if preprocessor is not None:
            print("ğŸ“Š Preprocessing datasets...")
            train_dataset = preprocessor.preprocess_dataset(train_dataset)
            if eval_dataset is not None:
                eval_dataset = preprocessor.preprocess_dataset(eval_dataset)

        # Create training arguments
        training_args = TrainingArguments(
            output_dir=self.training_config.output_dir,
            num_train_epochs=self.training_config.num_train_epochs,
            per_device_train_batch_size=self.training_config.per_device_train_batch_size,
            learning_rate=self.training_config.learning_rate,
            evaluation_strategy="steps",
            eval_steps=500,
            logging_steps=100,
            save_steps=500,
        )

        # Create data collator
        data_collator = DataCollatorWithPadding(
            tokenizer=self.tokenizer,
            padding=True
        )

        # Create optimizer chá»‰ cho adapter parameters
        optimizer = optim.AdamW(
            self.adapter_model.get_adapter_parameters(),
            lr=self.training_config.learning_rate
        )

        # Create trainer
        self.trainer = Trainer(
            model=self.adapter_model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
            optimizers=(optimizer, None)  # (optimizer, scheduler)
        )

        # Start training
        print("ğŸ‹ï¸ Training started...")
        train_result = self.trainer.train()

        # Save adapters
        self.save_adapters()

        print("âœ… Training completed!")
        return train_result

    def save_adapters(self):
        """Save chá»‰ adapter weights"""
        import os
        import torch

        save_dir = os.path.join(self.training_config.output_dir, "adapters")
        os.makedirs(save_dir, exist_ok=True)

        # Save adapter state dict
        adapter_state_dict = {}
        for name, param in self.adapter_model.named_parameters():
            if "adapter" in name:
                adapter_state_dict[name] = param

        torch.save(adapter_state_dict, os.path.join(save_dir, "adapter_model.bin"))

        # Save tokenizer
        self.tokenizer.save_pretrained(self.training_config.output_dir)

        print(f"ğŸ’¾ Adapters saved to {save_dir}")
```

**Giáº£i thÃ­ch:**
- `get_adapter_parameters()`: Chá»‰ láº¥y adapter parameters Ä‘á»ƒ optimize
- `DataCollatorWithPadding`: Tá»± Ä‘á»™ng pad sequences trong batch
- `optimizers=(optimizer, None)`: Custom optimizer cho adapters
- Save chá»‰ adapter weights, khÃ´ng save base model

---

## ğŸ¯ BÆ°á»›c 7: Táº¡o Example ÄÆ¡n Giáº£n

### 7.1 Táº¡o `examples/simple_adapter_example.py`

```python
"""
VÃ­ dá»¥ Ä‘Æ¡n giáº£n nháº¥t vá» adapter tuning
"""
from datasets import load_dataset

# Import our modules
from config import ModelConfig, AdapterConfig, TrainingConfig
from data import TextClassificationPreprocessor
from training import AdapterTrainer

def main():
    """VÃ­ dá»¥ train adapter cho sentiment analysis"""

    print("ğŸš€ Starting simple adapter example...")

    # 1. Setup configs
    model_config = ModelConfig(
        model_name_or_path="distilbert-base-uncased",  # Model nhá» Ä‘á»ƒ test
        num_labels=2,
        max_length=128  # Ngáº¯n Ä‘á»ƒ train nhanh
    )

    adapter_config = AdapterConfig(
        adapter_size=32,  # Adapter nhá»
        adapter_dropout=0.1,
        adapter_activation="relu",
        freeze_base_model=True
    )

    training_config = TrainingConfig(
        output_dir="./simple_results",
        num_train_epochs=1,  # 1 epoch Ä‘á»ƒ test nhanh
        per_device_train_batch_size=16,
        learning_rate=2e-3,  # Learning rate cao hÆ¡n cho adapters
    )

    # 2. Load dataset
    print("ğŸ“Š Loading dataset...")
    dataset = load_dataset("imdb")

    # Take small subset Ä‘á»ƒ test nhanh
    train_dataset = dataset["train"].select(range(100))
    eval_dataset = dataset["test"].select(range(50))

    # 3. Setup trainer
    trainer = AdapterTrainer(
        model_config=model_config,
        adapter_config=adapter_config,
        training_config=training_config
    )

    # 4. Setup preprocessor
    adapter_model = trainer.setup_model()
    tokenizer = trainer.tokenizer

    preprocessor = TextClassificationPreprocessor(
        tokenizer=tokenizer,
        text_column="text",
        label_column="label",
        max_length=128
    )

    # 5. Train
    print("ğŸ‹ï¸ Starting training...")
    trainer.train(
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        preprocessor=preprocessor
    )

    # 6. Test inference
    print("ğŸ§ª Testing inference...")
    test_texts = [
        "This movie is amazing!",
        "Terrible film, waste of time."
    ]

    # Simple inference test
    adapter_model.eval()
    for text in test_texts:
        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
        with torch.no_grad():
            outputs = adapter_model(**inputs)
            prediction = torch.argmax(outputs.logits, dim=-1)
            sentiment = "Positive" if prediction.item() == 1 else "Negative"
            print(f"'{text}' â†’ {sentiment}")

    print("âœ… Example completed!")

if __name__ == "__main__":
    main()
```

---

## ğŸ‰ HoÃ n ThÃ nh - Báº¡n ÄÃ£ CÃ³ Há»‡ Thá»‘ng Adapter Tuning!

### TÃ³m Táº¯t Nhá»¯ng GÃ¬ ÄÃ£ Implement:

1. âœ… **Core Adapter Layer**: Bottleneck architecture vá»›i residual connections
2. âœ… **Model Wrapper**: Tá»± Ä‘á»™ng add adapters vÃ o transformer layers
3. âœ… **Configuration System**: Quáº£n lÃ½ parameters dá»… dÃ ng
4. âœ… **Data Processing**: Tokenization vÃ  preprocessing
5. âœ… **Training System**: Train chá»‰ adapters, freeze base model
6. âœ… **Example**: VÃ­ dá»¥ hoÃ n chá»‰nh cÃ³ thá»ƒ cháº¡y Ä‘Æ°á»£c

### CÃ¡ch Cháº¡y:
```bash
cd adapter-tuning
python examples/simple_adapter_example.py
```

### Hiá»ƒu ÄÆ°á»£c GÃ¬:
- Adapter architecture vÃ  táº¡i sao hiá»‡u quáº£
- CÃ¡ch integrate adapters vÃ o pre-trained models
- Parameter freezing vÃ  selective training
- Training pipeline cho adapter tuning

### So SÃ¡nh Vá»›i Full Fine-tuning:
```
Full Fine-tuning:
- Train: 110M parameters
- Storage: 110M Ã— sá»‘ tasks
- Memory: Cao

Adapter Tuning:
- Train: 0.5M parameters (99.5% Ã­t hÆ¡n!)
- Storage: 110M + 0.5M Ã— sá»‘ tasks
- Memory: Tháº¥p hÆ¡n nhiá»u
- Performance: TÆ°Æ¡ng Ä‘Æ°Æ¡ng!
```

### BÆ°á»›c Tiáº¿p Theo:
1. Cháº¡y example Ä‘á»ƒ tháº¥y káº¿t quáº£
2. Thá»­ thay Ä‘á»•i adapter_size (16, 32, 64, 128)
3. Test vá»›i datasets khÃ¡c
4. Implement multi-task learning
5. So sÃ¡nh vá»›i LoRA

**ChÃºc má»«ng! Báº¡n Ä‘Ã£ hiá»ƒu vÃ  implement Ä‘Æ°á»£c Adapter Tuning tá»« sá»‘ 0! ğŸ‰**
