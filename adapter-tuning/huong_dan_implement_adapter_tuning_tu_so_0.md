# üîß H∆∞·ªõng D·∫´n Implement Adapter Tuning T·ª´ S·ªë 0

H∆∞·ªõng d·∫´n n√†y s·∫Ω gi√∫p b·∫°n hi·ªÉu v√† x√¢y d·ª±ng l·∫°i to√†n b·ªô h·ªá th·ªëng Adapter Tuning t·ª´ ƒë·∫ßu, t·ª´ng b∆∞·ªõc m·ªôt.

## üìö Ki·∫øn Th·ª©c C·∫ßn C√≥ Tr∆∞·ªõc

### 1. Python C∆° B·∫£n
- Classes v√† Objects
- Inheritance v√† Abstract classes
- Type hints
- Context managers

### 2. Deep Learning C∆° B·∫£n
- Neural Networks
- Transformer architecture
- Fine-tuning vs Transfer learning
- Parameter-efficient methods

### 3. Th∆∞ Vi·ªán C·∫ßn Bi·∫øt
- `torch`: PyTorch framework
- `transformers`: Hugging Face transformers
- `datasets`: Data loading v√† processing
- `sklearn`: Metrics calculation

---

## üéØ Adapter Tuning L√† G√¨?

### V·∫•n ƒê·ªÅ V·ªõi Full Fine-tuning
```
BERT-base: 110M parameters
Fine-tune cho 1 task ‚Üí Train to√†n b·ªô 110M parameters
Fine-tune cho 10 tasks ‚Üí 10 √ó 110M = 1.1B parameters storage!
```

### Gi·∫£i Ph√°p: Adapter Tuning
```
Base Model: 110M parameters (freeze - kh√¥ng train)
Adapter: 0.5M parameters (train)
‚Üí Ch·ªâ c·∫ßn 0.45% parameters ƒë·ªÉ ƒë·∫°t performance t∆∞∆°ng t·ª±!
```

### Ki·∫øn Tr√∫c Adapter
```
Input (768 dim) 
    ‚Üì
Down-projection (768 ‚Üí 64)  # Bottleneck
    ‚Üì
Activation (ReLU/GELU)
    ‚Üì
Up-projection (64 ‚Üí 768)
    ‚Üì
Residual Connection (+)
    ‚Üì
Output (768 dim)
```

---

## üèóÔ∏è B∆∞·ªõc 1: Hi·ªÉu Ki·∫øn Tr√∫c T·ªïng Th·ªÉ

### T·∫°i Sao C·∫ßn Nhi·ªÅu Files?
```
Nguy√™n t·∫Øc: "Single Responsibility Principle"

config/         ‚Üí Qu·∫£n l√Ω c·∫•u h√¨nh
adapters/       ‚Üí Core adapter implementation
data/           ‚Üí Data loading v√† preprocessing
training/       ‚Üí Training logic
evaluation/     ‚Üí Model evaluation
inference/      ‚Üí Production inference
examples/       ‚Üí V√≠ d·ª• s·ª≠ d·ª•ng
```

### Lu·ªìng Ho·∫°t ƒê·ªông
```
1. Config ‚Üí Thi·∫øt l·∫≠p parameters
2. Adapters ‚Üí T·∫°o adapter modules
3. Data ‚Üí Load v√† preprocess data
4. Training ‚Üí Train adapters (freeze base model)
5. Evaluation ‚Üí ƒê√°nh gi√° performance
6. Inference ‚Üí S·ª≠ d·ª•ng model ƒë√£ train
```

---

## üîß B∆∞·ªõc 2: Implement Core Adapter Layer

### 2.1 Hi·ªÉu Bottleneck Architecture

**T·∫°i sao d√πng Bottleneck?**
```python
# Thay v√¨ train full linear layer:
# 768 √ó 768 = 589,824 parameters

# Ta d√πng bottleneck:
# Down: 768 √ó 64 = 49,152 parameters
# Up: 64 √ó 768 = 49,152 parameters
# Total: 98,304 parameters (6x √≠t h∆°n!)
```

### 2.2 T·∫°o `adapters/adapter_layer.py`

```python
"""
Core adapter implementation - Tr√°i tim c·ªßa h·ªá th·ªëng
"""
import torch
import torch.nn as nn
import torch.nn.functional as F

class BottleneckAdapter(nn.Module):
    """
    Adapter v·ªõi ki·∫øn tr√∫c bottleneck
    
    Lu·ªìng: Input ‚Üí Down ‚Üí Activation ‚Üí Up ‚Üí Residual ‚Üí Output
    """
    
    def __init__(
        self,
        input_size: int,      # K√≠ch th∆∞·ªõc input (768 cho BERT)
        adapter_size: int,    # K√≠ch th∆∞·ªõc bottleneck (64, 128, etc.)
        dropout: float = 0.1, # Dropout ƒë·ªÉ tr√°nh overfitting
        activation: str = "relu"  # Activation function
    ):
        super().__init__()
        
        # L∆∞u parameters
        self.input_size = input_size
        self.adapter_size = adapter_size
        
        # Down-projection: gi·∫£m dimension
        self.down_project = nn.Linear(input_size, adapter_size)
        
        # Up-projection: tƒÉng dimension v·ªÅ ban ƒë·∫ßu
        self.up_project = nn.Linear(adapter_size, input_size)
        
        # Activation function
        if activation == "relu":
            self.activation = F.relu
        elif activation == "gelu":
            self.activation = F.gelu
        else:
            raise ValueError(f"Unsupported activation: {activation}")
        
        # Dropout layer
        self.dropout = nn.Dropout(dropout)
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        """Initialize adapter weights"""
        # Initialize down projection normally
        nn.init.normal_(self.down_project.weight, std=0.02)
        nn.init.zeros_(self.down_project.bias)
        
        # Initialize up projection to ZERO - quan tr·ªçng!
        # ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o adapter ban ƒë·∫ßu kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn model
        nn.init.zeros_(self.up_project.weight)
        nn.init.zeros_(self.up_project.bias)
    
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """
        Forward pass qua adapter
        
        Args:
            hidden_states: [batch_size, seq_len, input_size]
        Returns:
            output: [batch_size, seq_len, input_size]
        """
        # L∆∞u input cho residual connection
        residual = hidden_states
        
        # Down-projection
        hidden_states = self.down_project(hidden_states)
        
        # Activation
        hidden_states = self.activation(hidden_states)
        
        # Dropout
        hidden_states = self.dropout(hidden_states)
        
        # Up-projection
        hidden_states = self.up_project(hidden_states)
        
        # Residual connection - QUAN TR·ªåNG!
        # ƒê·∫£m b·∫£o gradient flow v√† stable training
        output = residual + hidden_states
        
        return output
```

**Gi·∫£i th√≠ch chi ti·∫øt:**
- `down_project`: Gi·∫£m dimension t·ª´ 768 ‚Üí 64 (compression)
- `activation`: Non-linearity ƒë·ªÉ h·ªçc complex patterns
- `up_project`: TƒÉng dimension t·ª´ 64 ‚Üí 768 (decompression)
- `residual`: Skip connection ƒë·ªÉ stable training
- `_init_weights()`: Initialize up_project = 0 ƒë·ªÉ adapter ban ƒë·∫ßu kh√¥ng ·∫£nh h∆∞·ªüng

---

## ü§ñ B∆∞·ªõc 3: Implement Model Wrapper

### 3.1 T·∫°i Sao C·∫ßn Model Wrapper?

```python
# Thay v√¨ modify tr·ª±c ti·∫øp transformer:
model = AutoModel.from_pretrained("bert-base-uncased")
# Ph·∫£i manually add adapters v√†o t·ª´ng layer...

# Ta d√πng wrapper ƒë·ªÉ t·ª± ƒë·ªông:
adapter_model = AdapterModel(model_config, adapter_config)
# T·ª± ƒë·ªông add adapters v√†o ƒë√∫ng v·ªã tr√≠!
```

### 3.2 T·∫°o `adapters/adapter_model.py`

```python
"""
Model wrapper ƒë·ªÉ add adapters v√†o pre-trained models
"""
import torch.nn as nn
from transformers import AutoModelForSequenceClassification

class AdapterModel(nn.Module):
    """Wrapper ƒë·ªÉ add adapters v√†o transformer model"""
    
    def __init__(self, model_config, adapter_config):
        super().__init__()
        
        # Load base model
        self.base_model = AutoModelForSequenceClassification.from_pretrained(
            model_config.model_name_or_path,
            num_labels=model_config.num_labels
        )
        
        # Store configs
        self.model_config = model_config
        self.adapter_config = adapter_config
        
        # Add adapters to model
        self._add_adapters()
        
        # Freeze base model parameters
        if adapter_config.freeze_base_model:
            self._freeze_base_model()
    
    def _add_adapters(self):
        """Add adapters to transformer layers"""
        
        # Get encoder layers (BERT example)
        if hasattr(self.base_model, 'bert'):
            encoder_layers = self.base_model.bert.encoder.layer
        else:
            raise ValueError("Unsupported model architecture")
        
        # Add adapter to each layer
        for layer_idx, layer in enumerate(encoder_layers):
            
            # Get hidden size from layer
            hidden_size = layer.output.dense.out_features
            
            # Create adapter
            adapter = BottleneckAdapter(
                input_size=hidden_size,
                adapter_size=self.adapter_config.adapter_size,
                dropout=self.adapter_config.adapter_dropout,
                activation=self.adapter_config.adapter_activation
            )
            
            # Add adapter to layer
            layer.adapter = adapter
            
            # Modify layer forward method
            self._modify_layer_forward(layer)
    
    def _modify_layer_forward(self, layer):
        """Modify layer forward ƒë·ªÉ include adapter"""
        
        # L∆∞u original forward method
        original_forward = layer.forward
        
        def forward_with_adapter(hidden_states, attention_mask=None, **kwargs):
            # G·ªçi original forward
            outputs = original_forward(hidden_states, attention_mask, **kwargs)
            
            # Extract hidden states
            if isinstance(outputs, tuple):
                hidden_states = outputs[0]
            else:
                hidden_states = outputs
            
            # Apply adapter
            if hasattr(layer, 'adapter'):
                hidden_states = layer.adapter(hidden_states)
            
            # Return in original format
            if isinstance(outputs, tuple):
                return (hidden_states,) + outputs[1:]
            else:
                return hidden_states
        
        # Replace forward method
        layer.forward = forward_with_adapter
    
    def _freeze_base_model(self):
        """Freeze base model parameters"""
        for name, param in self.base_model.named_parameters():
            if "adapter" not in name:
                param.requires_grad = False
    
    def forward(self, **kwargs):
        """Forward pass through model"""
        return self.base_model(**kwargs)
    
    def get_adapter_parameters(self):
        """Get only adapter parameters for optimization"""
        return [p for n, p in self.named_parameters() if "adapter" in n]
    
    def print_adapter_info(self):
        """Print adapter information"""
        total_params = sum(p.numel() for p in self.parameters())
        adapter_params = sum(p.numel() for p in self.get_adapter_parameters())
        
        print(f"Total parameters: {total_params:,}")
        print(f"Adapter parameters: {adapter_params:,}")
        print(f"Adapter percentage: {adapter_params/total_params*100:.2f}%")
```

**Gi·∫£i th√≠ch chi ti·∫øt:**
- `_add_adapters()`: T·ª± ƒë·ªông detect v√† add adapters v√†o m·ªói transformer layer
- `_modify_layer_forward()`: Modify forward method ƒë·ªÉ call adapter sau original computation
- `_freeze_base_model()`: Freeze t·∫•t c·∫£ parameters tr·ª´ adapters
- `get_adapter_parameters()`: L·∫•y ch·ªâ adapter parameters ƒë·ªÉ optimize

---

## ‚è∞ T·∫°m D·ª´ng - Checkpoint 1

ƒê·∫øn ƒë√¢y b·∫°n ƒë√£ hi·ªÉu:
1. ‚úÖ Adapter architecture v√† t·∫°i sao n√≥ hi·ªáu qu·∫£
2. ‚úÖ C√°ch implement bottleneck adapter
3. ‚úÖ C√°ch integrate adapter v√†o pre-trained model
4. ‚úÖ Parameter freezing v√† selective training

**Ti·∫øp theo**: Ch√∫ng ta s·∫Ω implement data processing, training system, v√† evaluation.

---

## üìä B∆∞·ªõc 4: Implement Configuration System

### 4.1 T·∫°i Sao C·∫ßn Config System?

```python
# Thay v√¨ hard-code:
adapter = BottleneckAdapter(768, 64, 0.1, "relu")
model = AutoModel.from_pretrained("bert-base-uncased")

# Ta d√πng config ƒë·ªÉ d·ªÖ thay ƒë·ªïi:
adapter_config = AdapterConfig(adapter_size=64, dropout=0.1)
model_config = ModelConfig(model_name="bert-base-uncased")
```

### 4.2 T·∫°o `config/adapter_config.py`

```python
"""
Configuration cho adapters
"""
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class AdapterConfig:
    """Configuration cho adapter modules"""

    # Basic adapter parameters
    adapter_size: int = 64              # Bottleneck dimension
    adapter_dropout: float = 0.1        # Dropout rate
    adapter_activation: str = "relu"    # Activation function

    # Adapter placement
    adapter_location: str = "both"      # "attention", "feedforward", "both"
    adapter_layers: Optional[List[int]] = None  # Which layers (None = all)

    # Training settings
    freeze_base_model: bool = True      # Freeze base model
    train_adapter_only: bool = True     # Train only adapters

    # Advanced options
    use_residual: bool = True           # Use residual connections
    adapter_init_range: float = 1e-3   # Weight initialization range

    def __post_init__(self):
        """Validate configuration"""
        valid_activations = ["relu", "gelu", "swish", "tanh"]
        if self.adapter_activation not in valid_activations:
            raise ValueError(f"activation must be one of {valid_activations}")

        valid_locations = ["attention", "feedforward", "both"]
        if self.adapter_location not in valid_locations:
            raise ValueError(f"location must be one of {valid_locations}")

    def get_adapter_size(self, hidden_size: int) -> int:
        """Calculate adapter size based on hidden size"""
        return self.adapter_size

    def should_add_adapter(self, layer_idx: int) -> bool:
        """Check if should add adapter to specific layer"""
        if self.adapter_layers is None:
            return True
        return layer_idx in self.adapter_layers
```

### 4.3 T·∫°o `config/model_config.py`

```python
"""
Configuration cho base models
"""
from dataclasses import dataclass
from typing import Optional

@dataclass
class ModelConfig:
    """Configuration cho base model"""

    # Model identification
    model_name_or_path: str = "bert-base-uncased"
    tokenizer_name_or_path: Optional[str] = None

    # Model parameters
    num_labels: int = 2                 # Number of output labels
    max_length: int = 512               # Max sequence length

    # Task configuration
    task_type: str = "classification"   # Task type

    def __post_init__(self):
        if self.tokenizer_name_or_path is None:
            self.tokenizer_name_or_path = self.model_name_or_path
```

**Gi·∫£i th√≠ch:**
- `@dataclass`: T·ª± ƒë·ªông t·∫°o `__init__`, `__repr__` methods
- `__post_init__`: Validation sau khi t·∫°o object
- Type hints: Gi√∫p IDE hi·ªÉu v√† check types

---

## üìä B∆∞·ªõc 5: Implement Data Processing

### 5.1 T·∫°i Sao C·∫ßn Data Processing?

```
Raw text: "This movie is great!"
‚Üì Tokenization
Token IDs: [101, 2023, 3185, 2003, 2307, 999, 102]
‚Üì Padding/Truncation
Fixed length: [101, 2023, 3185, 2003, 2307, 999, 102, 0, 0, 0]
‚Üì Attention mask
Mask: [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]
```

### 5.2 T·∫°o `data/preprocessing.py`

```python
"""
Data preprocessing cho adapter tuning
"""
from abc import ABC, abstractmethod
from typing import Dict, List
from datasets import Dataset
from transformers import PreTrainedTokenizer

class DataPreprocessor(ABC):
    """Abstract base class cho preprocessing"""

    def __init__(self, tokenizer: PreTrainedTokenizer, max_length: int = 512):
        self.tokenizer = tokenizer
        self.max_length = max_length

    @abstractmethod
    def preprocess_function(self, examples: Dict[str, List]) -> Dict[str, List]:
        """Method n√†y ph·∫£i ƒë∆∞·ª£c implement b·ªüi subclasses"""
        pass

    def preprocess_dataset(self, dataset: Dataset) -> Dataset:
        """Apply preprocessing l√™n to√†n b·ªô dataset"""
        return dataset.map(
            self.preprocess_function,
            batched=True,
            desc="Preprocessing dataset"
        )

class TextClassificationPreprocessor(DataPreprocessor):
    """Preprocessor cho text classification"""

    def __init__(
        self,
        tokenizer: PreTrainedTokenizer,
        text_column: str = "text",
        label_column: str = "label",
        max_length: int = 512
    ):
        super().__init__(tokenizer, max_length)
        self.text_column = text_column
        self.label_column = label_column

    def preprocess_function(self, examples: Dict[str, List]) -> Dict[str, List]:
        """Tokenize texts v√† prepare labels"""

        # Tokenize texts
        result = self.tokenizer(
            examples[self.text_column],
            padding=True,           # Pad v·ªÅ c√πng ƒë·ªô d√†i
            truncation=True,        # C·∫Øt n·∫øu qu√° d√†i
            max_length=self.max_length,
            return_tensors=None     # Return Python lists
        )

        # Add labels
        if self.label_column in examples:
            result["labels"] = examples[self.label_column]

        return result
```

**Gi·∫£i th√≠ch:**
- `ABC`: Abstract Base Class - class cha kh√¥ng th·ªÉ instantiate
- `@abstractmethod`: Method b·∫Øt bu·ªôc ph·∫£i implement
- `batched=True`: X·ª≠ l√Ω nhi·ªÅu examples c√πng l√∫c ƒë·ªÉ nhanh h∆°n
- `padding=True`: Th√™m padding tokens ƒë·ªÉ c√πng ƒë·ªô d√†i

---

## üèãÔ∏è B∆∞·ªõc 6: Implement Training System

### 6.1 Training Flow

```
1. Setup model + data
2. Create optimizer (ch·ªâ cho adapter parameters)
3. Training loop v·ªõi monitoring
4. Save adapters (kh√¥ng save base model)
```

### 6.2 T·∫°o `training/trainer.py`

```python
"""
Adapter trainer implementation
"""
from typing import Optional
from datasets import Dataset
from transformers import Trainer, TrainingArguments, DataCollatorWithPadding
import torch.optim as optim

class AdapterTrainer:
    """Main trainer cho adapter tuning"""

    def __init__(
        self,
        model_config,
        adapter_config,
        training_config,
        task_type: str = "classification"
    ):
        self.model_config = model_config
        self.adapter_config = adapter_config
        self.training_config = training_config
        self.task_type = task_type

        # Initialize components
        self.adapter_model = None
        self.tokenizer = None
        self.trainer = None

    def setup_model(self):
        """Setup adapter model v√† tokenizer"""
        if self.adapter_model is None:
            # Load tokenizer
            from transformers import AutoTokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_config.tokenizer_name_or_path
            )

            # Add pad token n·∫øu ch∆∞a c√≥
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            # Create adapter model
            self.adapter_model = AdapterModel(
                self.model_config,
                self.adapter_config
            )

            print("‚úÖ Adapter model setup completed")
            self.adapter_model.print_adapter_info()

        return self.adapter_model

    def train(
        self,
        train_dataset: Dataset,
        eval_dataset: Optional[Dataset] = None,
        preprocessor = None
    ):
        """Main training function"""

        print("üöÄ Starting adapter training...")

        # Setup model
        self.setup_model()

        # Preprocess datasets
        if preprocessor is not None:
            print("üìä Preprocessing datasets...")
            train_dataset = preprocessor.preprocess_dataset(train_dataset)
            if eval_dataset is not None:
                eval_dataset = preprocessor.preprocess_dataset(eval_dataset)

        # Create training arguments
        training_args = TrainingArguments(
            output_dir=self.training_config.output_dir,
            num_train_epochs=self.training_config.num_train_epochs,
            per_device_train_batch_size=self.training_config.per_device_train_batch_size,
            learning_rate=self.training_config.learning_rate,
            evaluation_strategy="steps",
            eval_steps=500,
            logging_steps=100,
            save_steps=500,
        )

        # Create data collator
        data_collator = DataCollatorWithPadding(
            tokenizer=self.tokenizer,
            padding=True
        )

        # Create optimizer ch·ªâ cho adapter parameters
        optimizer = optim.AdamW(
            self.adapter_model.get_adapter_parameters(),
            lr=self.training_config.learning_rate
        )

        # Create trainer
        self.trainer = Trainer(
            model=self.adapter_model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
            optimizers=(optimizer, None)  # (optimizer, scheduler)
        )

        # Start training
        print("üèãÔ∏è Training started...")
        train_result = self.trainer.train()

        # Save adapters
        self.save_adapters()

        print("‚úÖ Training completed!")
        return train_result

    def save_adapters(self):
        """Save ch·ªâ adapter weights"""
        import os
        import torch

        save_dir = os.path.join(self.training_config.output_dir, "adapters")
        os.makedirs(save_dir, exist_ok=True)

        # Save adapter state dict
        adapter_state_dict = {}
        for name, param in self.adapter_model.named_parameters():
            if "adapter" in name:
                adapter_state_dict[name] = param

        torch.save(adapter_state_dict, os.path.join(save_dir, "adapter_model.bin"))

        # Save tokenizer
        self.tokenizer.save_pretrained(self.training_config.output_dir)

        print(f"üíæ Adapters saved to {save_dir}")
```

**Gi·∫£i th√≠ch:**
- `get_adapter_parameters()`: Ch·ªâ l·∫•y adapter parameters ƒë·ªÉ optimize
- `DataCollatorWithPadding`: T·ª± ƒë·ªông pad sequences trong batch
- `optimizers=(optimizer, None)`: Custom optimizer cho adapters
- Save ch·ªâ adapter weights, kh√¥ng save base model

---

## üéØ B∆∞·ªõc 7: T·∫°o Example ƒê∆°n Gi·∫£n

### 7.1 T·∫°o `examples/simple_adapter_example.py`

```python
"""
V√≠ d·ª• ƒë∆°n gi·∫£n nh·∫•t v·ªÅ adapter tuning
"""
from datasets import load_dataset

# Import our modules
from config import ModelConfig, AdapterConfig, TrainingConfig
from data import TextClassificationPreprocessor
from training import AdapterTrainer

def main():
    """V√≠ d·ª• train adapter cho sentiment analysis"""

    print("üöÄ Starting simple adapter example...")

    # 1. Setup configs
    model_config = ModelConfig(
        model_name_or_path="distilbert-base-uncased",  # Model nh·ªè ƒë·ªÉ test
        num_labels=2,
        max_length=128  # Ng·∫Øn ƒë·ªÉ train nhanh
    )

    adapter_config = AdapterConfig(
        adapter_size=32,  # Adapter nh·ªè
        adapter_dropout=0.1,
        adapter_activation="relu",
        freeze_base_model=True
    )

    training_config = TrainingConfig(
        output_dir="./simple_results",
        num_train_epochs=1,  # 1 epoch ƒë·ªÉ test nhanh
        per_device_train_batch_size=16,
        learning_rate=2e-3,  # Learning rate cao h∆°n cho adapters
    )

    # 2. Load dataset
    print("üìä Loading dataset...")
    dataset = load_dataset("imdb")

    # Take small subset ƒë·ªÉ test nhanh
    train_dataset = dataset["train"].select(range(100))
    eval_dataset = dataset["test"].select(range(50))

    # 3. Setup trainer
    trainer = AdapterTrainer(
        model_config=model_config,
        adapter_config=adapter_config,
        training_config=training_config
    )

    # 4. Setup preprocessor
    adapter_model = trainer.setup_model()
    tokenizer = trainer.tokenizer

    preprocessor = TextClassificationPreprocessor(
        tokenizer=tokenizer,
        text_column="text",
        label_column="label",
        max_length=128
    )

    # 5. Train
    print("üèãÔ∏è Starting training...")
    trainer.train(
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        preprocessor=preprocessor
    )

    # 6. Test inference
    print("üß™ Testing inference...")
    test_texts = [
        "This movie is amazing!",
        "Terrible film, waste of time."
    ]

    # Simple inference test
    adapter_model.eval()
    for text in test_texts:
        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
        with torch.no_grad():
            outputs = adapter_model(**inputs)
            prediction = torch.argmax(outputs.logits, dim=-1)
            sentiment = "Positive" if prediction.item() == 1 else "Negative"
            print(f"'{text}' ‚Üí {sentiment}")

    print("‚úÖ Example completed!")

if __name__ == "__main__":
    main()
```

---

## üéâ Ho√†n Th√†nh - B·∫°n ƒê√£ C√≥ H·ªá Th·ªëng Adapter Tuning!

### T√≥m T·∫Øt Nh·ªØng G√¨ ƒê√£ Implement:

1. ‚úÖ **Core Adapter Layer**: Bottleneck architecture v·ªõi residual connections
2. ‚úÖ **Model Wrapper**: T·ª± ƒë·ªông add adapters v√†o transformer layers
3. ‚úÖ **Configuration System**: Qu·∫£n l√Ω parameters d·ªÖ d√†ng
4. ‚úÖ **Data Processing**: Tokenization v√† preprocessing
5. ‚úÖ **Training System**: Train ch·ªâ adapters, freeze base model
6. ‚úÖ **Example**: V√≠ d·ª• ho√†n ch·ªânh c√≥ th·ªÉ ch·∫°y ƒë∆∞·ª£c

### C√°ch Ch·∫°y:
```bash
cd adapter-tuning
python examples/simple_adapter_example.py
```

### Hi·ªÉu ƒê∆∞·ª£c G√¨:
- Adapter architecture v√† t·∫°i sao hi·ªáu qu·∫£
- C√°ch integrate adapters v√†o pre-trained models
- Parameter freezing v√† selective training
- Training pipeline cho adapter tuning

### So S√°nh V·ªõi Full Fine-tuning:
```
Full Fine-tuning:
- Train: 110M parameters
- Storage: 110M √ó s·ªë tasks
- Memory: Cao

Adapter Tuning:
- Train: 0.5M parameters (99.5% √≠t h∆°n!)
- Storage: 110M + 0.5M √ó s·ªë tasks
- Memory: Th·∫•p h∆°n nhi·ªÅu
- Performance: T∆∞∆°ng ƒë∆∞∆°ng!
```

### B∆∞·ªõc Ti·∫øp Theo:
1. Ch·∫°y example ƒë·ªÉ th·∫•y k·∫øt qu·∫£
2. Th·ª≠ thay ƒë·ªïi adapter_size (16, 32, 64, 128)
3. Test v·ªõi datasets kh√°c
4. Implement multi-task learning
5. So s√°nh v·ªõi LoRA

**Ch√∫c m·ª´ng! B·∫°n ƒë√£ hi·ªÉu v√† implement ƒë∆∞·ª£c Adapter Tuning t·ª´ s·ªë 0! üéâ**
