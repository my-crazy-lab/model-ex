# ğŸ”§ Adapter Tuning Implementation

This project implements Adapter Tuning based on the checklist in `fine-tuning/Adapter Tuning.md`.

## ğŸ“‹ What is Adapter Tuning?

Adapter Tuning is a parameter-efficient fine-tuning method that:
- Inserts small neural network modules (adapters) into pre-trained models
- Freezes the original model parameters
- Only trains the adapter parameters
- Achieves comparable performance to full fine-tuning with much fewer parameters

## ğŸ—ï¸ Architecture

```
Original Transformer Layer:
Input â†’ Self-Attention â†’ Add&Norm â†’ Feed-Forward â†’ Add&Norm â†’ Output

With Adapters:
Input â†’ Self-Attention â†’ Add&Norm â†’ Adapter â†’ Feed-Forward â†’ Add&Norm â†’ Adapter â†’ Output
```

Each Adapter is a simple 2-layer MLP:
```
Input â†’ Linear(down) â†’ ReLU â†’ Linear(up) â†’ Residual Connection â†’ Output
```

## ğŸ“ Project Structure

```
adapter-tuning/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ setup.py                    # Package setup
â”œâ”€â”€ config/                     # Configuration files
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_config.py         # Model and adapter configurations
â”‚   â””â”€â”€ training_config.py      # Training configurations
â”œâ”€â”€ adapters/                   # Adapter implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ adapter_layer.py        # Core adapter layer
â”‚   â”œâ”€â”€ adapter_model.py        # Model with adapters
â”‚   â””â”€â”€ adapter_config.py       # Adapter-specific configs
â”œâ”€â”€ data/                       # Data processing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py          # Data loading utilities
â”‚   â””â”€â”€ preprocessing.py        # Data preprocessing
â”œâ”€â”€ training/                   # Training scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ trainer.py              # Training logic
â”‚   â””â”€â”€ utils.py                # Training utilities
â”œâ”€â”€ evaluation/                 # Evaluation scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ evaluator.py            # Model evaluation
â”‚   â””â”€â”€ metrics.py              # Evaluation metrics
â”œâ”€â”€ inference/                  # Inference scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ pipeline.py             # Inference pipeline
â”œâ”€â”€ examples/                   # Example scripts
â”‚   â”œâ”€â”€ text_classification.py
â”‚   â”œâ”€â”€ sentiment_analysis.py
â”‚   â””â”€â”€ multi_task_learning.py
â””â”€â”€ notebooks/                  # Jupyter notebooks
    â”œâ”€â”€ 01_adapter_basics.ipynb
    â”œâ”€â”€ 02_training_adapters.ipynb
    â””â”€â”€ 03_multi_adapter_management.ipynb
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Install the package
pip install -e .
```

### 2. Basic Usage

```python
from config import ModelConfig, AdapterConfig, TrainingConfig
from adapters import AdapterModel
from training import AdapterTrainer

# Configure model with adapters
model_config = ModelConfig(
    model_name_or_path="bert-base-uncased",
    num_labels=2
)

adapter_config = AdapterConfig(
    adapter_size=64,
    adapter_dropout=0.1,
    adapter_activation="relu"
)

# Create model with adapters
model = AdapterModel(model_config, adapter_config)

# Train adapters
trainer = AdapterTrainer(model_config, adapter_config, training_config)
trainer.train(train_dataset, eval_dataset)
```

### 3. Command Line Usage

```bash
# Train adapters for text classification
python examples/text_classification.py --dataset imdb --adapter-size 64

# Multi-task learning with multiple adapters
python examples/multi_task_learning.py --tasks sentiment,classification
```

## ğŸ”§ Key Features

### âœ… Core Adapter Implementation
- **Bottleneck Architecture**: Down-projection â†’ Activation â†’ Up-projection
- **Residual Connections**: Skip connections for stable training
- **Flexible Placement**: Insert adapters after any transformer layer
- **Parameter Efficiency**: Only 0.5-3% additional parameters

### âœ… Multiple Adapter Types
- **Standard Adapters**: Basic bottleneck adapters
- **Parallel Adapters**: Multiple adapters in parallel
- **Sequential Adapters**: Stacked adapters for complex tasks
- **Task-Specific Adapters**: Different adapters for different tasks

### âœ… Advanced Features
- **Multi-Task Learning**: Train multiple adapters simultaneously
- **Adapter Fusion**: Combine multiple adapters intelligently
- **Dynamic Adapter Selection**: Choose adapters based on input
- **Adapter Composition**: Stack and combine adapters

### âœ… Production Ready
- **Efficient Inference**: Fast adapter switching
- **Model Serialization**: Save/load adapters independently
- **Batch Processing**: Handle multiple tasks in one batch
- **Memory Optimization**: Minimal memory overhead

## ğŸ“Š Supported Tasks

- **Text Classification**: Sentiment analysis, topic classification
- **Named Entity Recognition**: Token-level classification
- **Question Answering**: Reading comprehension tasks
- **Text Generation**: Conditional text generation
- **Multi-Task Learning**: Multiple tasks with shared backbone

## ğŸ§  Supported Models

- **BERT family**: bert-base-uncased, bert-large, roberta, distilbert
- **GPT family**: gpt2, gpt-neo, gpt-j
- **T5 family**: t5-small, t5-base, t5-large
- **Custom models**: Any transformer-based model

## ğŸ“ˆ Performance Benefits

### Memory Efficiency
```
Full Fine-tuning: 110M parameters (BERT-base)
Adapter Tuning: 110M + 0.5M = 110.5M parameters (0.45% increase)
```

### Training Speed
```
Full Fine-tuning: Train all 110M parameters
Adapter Tuning: Train only 0.5M parameters (220x fewer)
```

### Model Sharing
```
Base Model: 110M parameters (shared)
Task A Adapter: 0.5M parameters
Task B Adapter: 0.5M parameters
Total Storage: 111M parameters (vs 220M for two full models)
```

## ğŸ”¬ Experimental Features

- **Adapter Pruning**: Remove unnecessary adapter parameters
- **Knowledge Distillation**: Transfer knowledge between adapters
- **Meta-Learning**: Learn to adapt quickly to new tasks
- **Continual Learning**: Add new tasks without forgetting old ones

## ğŸ“– Documentation

See the `notebooks/` directory for detailed tutorials:
1. **Adapter Basics**: Understanding adapter architecture
2. **Training Adapters**: Step-by-step training guide
3. **Multi-Adapter Management**: Managing multiple adapters

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Implement your changes
4. Add tests and documentation
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- [Adapter-Hub](https://adapterhub.ml/) for the adapter ecosystem
- [Houlsby et al. (2019)](https://arxiv.org/abs/1902.00751) for the original adapter paper
- Hugging Face for the transformers library
