# ğŸ” Interpretability & Explainability - AI Model Understanding System

This project implements comprehensive interpretability and explainability techniques for AI models, helping understand how models make decisions and why they produce specific outputs.

## ğŸ“‹ What is Model Interpretability?

Model Interpretability is the ability to understand and explain how AI models make decisions:
- **Local Explanations**: Why did the model make this specific prediction?
- **Global Explanations**: How does the model behave overall?
- **Feature Importance**: Which inputs matter most?
- **Decision Boundaries**: How does the model separate different classes?

## ğŸ¯ Why Interpretability Matters

### Business Impact
```
Traditional AI: "Black Box" Decisions
- Model predicts: "Loan rejected"
- Business question: "Why was it rejected?"
- AI response: "Complex mathematical calculation"
- Result: No actionable insights

Interpretable AI: Transparent Decisions
- Model predicts: "Loan rejected"
- Explanation: "Income too low (40%), debt ratio high (35%), credit history (25%)"
- Business action: "Improve income verification process"
- Result: Better decision making
```

### Trust and Compliance
```
Healthcare AI Example:
- Model: "High cancer risk detected"
- Doctor needs: "Which symptoms/features indicate risk?"
- Interpretability: "Irregular cell patterns (60%), size anomalies (25%), texture (15%)"
- Outcome: Doctor can validate and trust the diagnosis
```

## ğŸ“ Project Structure

```
interpretability-explainability/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ setup.py                     # Package setup
â”œâ”€â”€ src/                         # Source code
â”‚   â”œâ”€â”€ explainers/              # Explanation methods
â”‚   â”‚   â”œâ”€â”€ lime_explainer.py    # LIME implementation
â”‚   â”‚   â”œâ”€â”€ shap_explainer.py    # SHAP implementation
â”‚   â”‚   â”œâ”€â”€ captum_explainer.py  # Captum integration
â”‚   â”‚   â””â”€â”€ attention_viz.py     # Attention visualization
â”‚   â”œâ”€â”€ models/                  # Model wrappers
â”‚   â”‚   â”œâ”€â”€ text_classifier.py   # Text classification models
â”‚   â”‚   â”œâ”€â”€ image_classifier.py  # Image classification models
â”‚   â”‚   â””â”€â”€ transformer_model.py # Transformer models
â”‚   â”œâ”€â”€ visualizers/             # Visualization tools
â”‚   â”‚   â”œâ”€â”€ plot_explanations.py # Explanation plots
â”‚   â”‚   â”œâ”€â”€ attention_plots.py   # Attention visualizations
â”‚   â”‚   â””â”€â”€ dashboard.py         # Interactive dashboard
â”‚   â”œâ”€â”€ utils/                   # Utility functions
â”‚   â”‚   â”œâ”€â”€ data_processing.py   # Data preprocessing
â”‚   â”‚   â”œâ”€â”€ model_utils.py       # Model utilities
â”‚   â”‚   â””â”€â”€ evaluation.py       # Evaluation metrics
â”‚   â””â”€â”€ experiments/             # Experiment scripts
â”‚       â”œâ”€â”€ text_analysis.py     # Text model analysis
â”‚       â”œâ”€â”€ image_analysis.py    # Image model analysis
â”‚       â””â”€â”€ comparative_study.py # Compare methods
â”œâ”€â”€ examples/                    # Complete examples
â”‚   â”œâ”€â”€ sentiment_analysis.py    # Sentiment analysis explanation
â”‚   â”œâ”€â”€ medical_diagnosis.py     # Medical AI explanation
â”‚   â”œâ”€â”€ financial_prediction.py  # Financial model explanation
â”‚   â””â”€â”€ multimodal_analysis.py   # Multimodal explanation
â”œâ”€â”€ notebooks/                   # Jupyter notebooks
â”‚   â”œâ”€â”€ lime_tutorial.ipynb      # LIME tutorial
â”‚   â”œâ”€â”€ shap_tutorial.ipynb      # SHAP tutorial
â”‚   â”œâ”€â”€ attention_analysis.ipynb # Attention analysis
â”‚   â””â”€â”€ comparative_analysis.ipynb # Method comparison
â”œâ”€â”€ tests/                       # Test files
â””â”€â”€ docs/                        # Documentation
    â”œâ”€â”€ methods_comparison.md
    â”œâ”€â”€ best_practices.md
    â””â”€â”€ troubleshooting.md
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone <repository-url>
cd interpretability-explainability

# Install dependencies
pip install -r requirements.txt

# Install the package
pip install -e .
```

### 2. Basic Usage

```python
from src.explainers import LIMEExplainer, SHAPExplainer
from src.models import TextClassifier
from src.visualizers import plot_explanations

# Load model and data
model = TextClassifier.from_pretrained('bert-base-uncased')
text = "This movie is absolutely fantastic!"

# LIME Explanation
lime_explainer = LIMEExplainer(model)
lime_explanation = lime_explainer.explain_instance(text)

# SHAP Explanation
shap_explainer = SHAPExplainer(model)
shap_values = shap_explainer.explain_instance(text)

# Visualize explanations
plot_explanations(lime_explanation, shap_values, text)
```

### 3. Run Examples

```bash
# Sentiment analysis explanation
python examples/sentiment_analysis.py

# Medical diagnosis explanation
python examples/medical_diagnosis.py

# Financial prediction explanation
python examples/financial_prediction.py

# Comparative analysis
python examples/comparative_study.py
```

## ğŸ”§ Key Features

### âœ… Multiple Explanation Methods
- **LIME (Local Interpretable Model-agnostic Explanations)**
- **SHAP (SHapley Additive exPlanations)**
- **Captum (PyTorch interpretability)**
- **Attention Visualization**
- **Gradient-based methods**

### âœ… Model Support
- **Text Models**: BERT, RoBERTa, GPT, T5
- **Image Models**: ResNet, VGG, EfficientNet
- **Tabular Models**: XGBoost, Random Forest, Neural Networks
- **Multimodal Models**: CLIP, ViLBERT

### âœ… Visualization Tools
- **Interactive plots** with Plotly
- **Attention heatmaps** for transformers
- **Feature importance charts**
- **Decision boundary visualization**
- **Comparative analysis dashboards**

### âœ… Explanation Types
- **Local explanations** for individual predictions
- **Global explanations** for model behavior
- **Counterfactual explanations**
- **Feature attribution**
- **Layer-wise relevance propagation**

## ğŸ“Š Explanation Methods Comparison

| Method | Type | Model Support | Pros | Cons |
|--------|------|---------------|------|------|
| LIME | Local | Model-agnostic | Fast, intuitive | Unstable, sampling-based |
| SHAP | Local/Global | Model-agnostic | Theoretically grounded | Slow for large models |
| Captum | Local/Global | PyTorch only | Many algorithms | Framework specific |
| Attention | Local | Transformers only | Direct model insight | Not always meaningful |

## ğŸ¯ Use Cases

### 1. Healthcare AI
```python
# Medical diagnosis explanation
diagnosis_model = MedicalClassifier()
patient_data = load_patient_data("patient_123.json")

# Explain diagnosis
explanation = explain_medical_prediction(
    model=diagnosis_model,
    patient_data=patient_data,
    methods=['lime', 'shap', 'attention']
)

# Generate doctor-friendly report
report = generate_medical_report(explanation)
```

### 2. Financial Services
```python
# Loan approval explanation
loan_model = LoanApprovalModel()
application = load_loan_application("app_456.json")

# Explain decision
explanation = explain_loan_decision(
    model=loan_model,
    application=application,
    regulatory_requirements=True
)

# Generate compliance report
compliance_report = generate_compliance_report(explanation)
```

### 3. Content Moderation
```python
# Content moderation explanation
moderation_model = ContentModerationModel()
content = "User-generated content text"

# Explain moderation decision
explanation = explain_moderation_decision(
    model=moderation_model,
    content=content,
    show_toxic_words=True
)

# Generate moderation report
moderation_report = generate_moderation_report(explanation)
```

## ğŸ“ˆ Performance Metrics

### Explanation Quality Metrics
```python
# Faithfulness: How well does explanation reflect model behavior?
faithfulness_score = evaluate_faithfulness(model, explanations, test_data)

# Stability: How consistent are explanations for similar inputs?
stability_score = evaluate_stability(explainer, similar_inputs)

# Comprehensibility: How understandable are explanations to humans?
comprehensibility_score = evaluate_comprehensibility(explanations, human_ratings)

# Efficiency: How fast can explanations be generated?
efficiency_metrics = measure_explanation_time(explainer, test_cases)
```

### Benchmark Results
```
Method Comparison on IMDB Sentiment Analysis:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Method      â”‚ Faithfulnessâ”‚ Stability   â”‚ Speed (s)   â”‚ Memory (MB) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LIME        â”‚ 0.78        â”‚ 0.65        â”‚ 2.3         â”‚ 150         â”‚
â”‚ SHAP        â”‚ 0.85        â”‚ 0.82        â”‚ 8.7         â”‚ 300         â”‚
â”‚ Captum IG   â”‚ 0.91        â”‚ 0.88        â”‚ 1.2         â”‚ 200         â”‚
â”‚ Attention   â”‚ 0.72        â”‚ 0.90        â”‚ 0.1         â”‚ 50          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”¬ Advanced Features

### 1. Counterfactual Explanations
```python
# Generate counterfactual examples
counterfactuals = generate_counterfactuals(
    model=model,
    instance=original_input,
    target_class="positive",
    max_changes=3
)

# Example output:
# Original: "This movie is terrible" â†’ Negative (0.95)
# Counterfactual: "This movie is great" â†’ Positive (0.87)
# Changes: ["terrible" â†’ "great"]
```

### 2. Model Comparison
```python
# Compare explanations across different models
models = [bert_model, roberta_model, distilbert_model]
comparison = compare_model_explanations(
    models=models,
    test_data=test_dataset,
    methods=['lime', 'shap']
)

# Analyze differences in decision-making
decision_differences = analyze_model_differences(comparison)
```

### 3. Explanation Aggregation
```python
# Aggregate explanations across multiple instances
global_explanation = aggregate_explanations(
    explainer=shap_explainer,
    dataset=validation_set,
    aggregation_method="mean"
)

# Identify most important features globally
top_features = get_top_global_features(global_explanation, top_k=10)
```

## ğŸ¨ Visualization Examples

### 1. LIME Text Explanation
```python
# Text classification with word importance
lime_viz = visualize_lime_text(
    explanation=lime_explanation,
    original_text=text,
    prediction_class="positive",
    show_probabilities=True
)
```

### 2. SHAP Summary Plot
```python
# Feature importance summary
shap_summary = plot_shap_summary(
    shap_values=shap_values,
    feature_names=feature_names,
    plot_type="violin"
)
```

### 3. Attention Heatmap
```python
# Transformer attention visualization
attention_viz = visualize_attention(
    model=transformer_model,
    text=input_text,
    layer=6,
    head=8,
    highlight_tokens=True
)
```

## ğŸ§ª Testing and Validation

### Unit Tests
```bash
# Run all tests
python -m pytest tests/

# Run specific test categories
python -m pytest tests/test_explainers.py
python -m pytest tests/test_visualizers.py
python -m pytest tests/test_models.py
```

### Integration Tests
```bash
# Test end-to-end workflows
python -m pytest tests/integration/

# Test with different model types
python -m pytest tests/integration/test_text_models.py
python -m pytest tests/integration/test_image_models.py
```

## ğŸ“š Documentation

### Tutorials
- **Getting Started**: Basic explanation workflow
- **LIME Deep Dive**: Advanced LIME techniques
- **SHAP Mastery**: Comprehensive SHAP usage
- **Attention Analysis**: Understanding transformer attention
- **Custom Explainers**: Building your own explanation methods

### Best Practices
- **Choosing the Right Method**: When to use which explainer
- **Interpretation Guidelines**: How to read explanations correctly
- **Validation Strategies**: Ensuring explanation quality
- **Performance Optimization**: Making explanations faster

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Implement your changes
4. Add tests and documentation
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- [LIME](https://github.com/marcotcr/lime) - Local Interpretable Model-agnostic Explanations
- [SHAP](https://github.com/slundberg/shap) - SHapley Additive exPlanations
- [Captum](https://captum.ai/) - PyTorch model interpretability
- [BertViz](https://github.com/jessevig/bertviz) - Attention visualization for transformers
