# âœ… Checklist há»c Full Fine-tuning

## ğŸ“¦ 1. Chuáº©n bá»‹ mÃ´i trÆ°á»ng
- [ ] CÃ i Ä‘áº·t Python >= 3.8
- [ ] CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t:
  - [ ] `transformers`
  - [ ] `datasets`
  - [ ] `torch`
  - [ ] `accelerate` (náº¿u dÃ¹ng Ä‘a GPU)
  - [ ] `evaluate` (Ä‘Ã¡nh giÃ¡ hiá»‡u nÄƒng)

## ğŸ“š 2. Chuáº©n bá»‹ dá»¯ liá»‡u
- [ ] Chuáº©n bá»‹ táº­p dá»¯ liá»‡u Ä‘á»§ lá»›n vÃ  cháº¥t lÆ°á»£ng cao phÃ¹ há»£p task
- [ ] Tiá»n xá»­ lÃ½ dá»¯ liá»‡u:
  - [ ] Tokenization
  - [ ] Xá»­ lÃ½ Ä‘áº·c biá»‡t (padding, truncation)
- [ ] PhÃ¢n chia dá»¯ liá»‡u train/validation/test rÃµ rÃ ng

## ğŸ§  3. Chuáº©n bá»‹ mÃ´ hÃ¬nh
- [ ] Chá»n mÃ´ hÃ¬nh pretrained phÃ¹ há»£p vá»›i task (vd: BERT, GPT, T5)
- [ ] Load mÃ´ hÃ¬nh vÃ  tokenizer

## ğŸ”§ 4. Thiáº¿t láº­p fine-tuning
- [ ] Äáº·t toÃ n bá»™ tham sá»‘ model á»Ÿ cháº¿ Ä‘á»™ trainable
- [ ] Cáº¥u hÃ¬nh optimizer (AdamW phá»• biáº¿n)
- [ ] Chá»n learning rate, scheduler phÃ¹ há»£p (vd: linear warmup)
- [ ] Thiáº¿t láº­p batch size vÃ  sá»‘ epoch dá»±a trÃªn tÃ i nguyÃªn
- [ ] (Tuá»³ chá»n) DÃ¹ng gradient clipping Ä‘á»ƒ á»•n Ä‘á»‹nh training

## ğŸš€ 5. Huáº¥n luyá»‡n mÃ´ hÃ¬nh
- [ ] Cháº¡y fine-tuning trÃªn táº­p train
- [ ] Theo dÃµi loss vÃ  metric trÃªn táº­p validation
- [ ] Kiá»ƒm soÃ¡t overfitting (early stopping, regularization náº¿u cáº§n)

## ğŸ’¾ 6. LÆ°u vÃ  kiá»ƒm thá»­ mÃ´ hÃ¬nh
- [ ] LÆ°u mÃ´ hÃ¬nh fine-tuned
- [ ] ÄÃ¡nh giÃ¡ trÃªn táº­p test Ä‘á»ƒ kiá»ƒm tra hiá»‡u nÄƒng cuá»‘i cÃ¹ng
- [ ] So sÃ¡nh káº¿t quáº£ vá»›i baseline hoáº·c mÃ´ hÃ¬nh chÆ°a fine-tune

## âš™ï¸ 7. Tá»‘i Æ°u vÃ  triá»ƒn khai
- [ ] (Tuá»³ chá»n) Pruning, quantization Ä‘á»ƒ giáº£m model size
- [ ] Chuáº©n bá»‹ pipeline inference hiá»‡u quáº£
- [ ] Triá»ƒn khai model lÃªn mÃ´i trÆ°á»ng sáº£n xuáº¥t (API, embedded devices)

## ğŸ“ 8. Quáº£n lÃ½ experiment
- [ ] Ghi láº¡i chi tiáº¿t hyperparameters, káº¿t quáº£
- [ ] Sá»­ dá»¥ng cÃ´ng cá»¥ theo dÃµi experiment (wandb, tensorboard)
- [ ] ÄÃ¡nh giÃ¡ hiá»‡u nÄƒng, tÃ i nguyÃªn tiÃªu thá»¥
