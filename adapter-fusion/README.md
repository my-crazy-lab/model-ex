# ğŸ”— Adapter Fusion Implementation

This project implements Adapter Fusion based on the checklist in `fine-tuning/Adapter Fusion.md`.

## ğŸ“‹ What is Adapter Fusion?

Adapter Fusion is an advanced parameter-efficient fine-tuning method that:
- Trains multiple task-specific adapters independently
- Combines knowledge from multiple adapters using fusion mechanisms
- Enables multi-task learning without catastrophic forgetting
- Achieves superior performance compared to individual adapters

## ğŸ—ï¸ Architecture

```
Base Model (Frozen)
    â†“
Task A Adapter â†’ \
Task B Adapter â†’ â†’ Fusion Layer â†’ Combined Output
Task C Adapter â†’ /
```

### Fusion Mechanisms

1. **Attention-based Fusion**: Uses attention to weight adapter outputs
2. **Weighted Average**: Learnable weights for combining adapters
3. **Gating**: Dynamic selection of adapters based on input

## ğŸ“ Project Structure

```
adapter-fusion/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ setup.py                    # Package setup
â”œâ”€â”€ config/                     # Configuration files
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_config.py         # Model configurations
â”‚   â”œâ”€â”€ adapter_config.py       # Adapter configurations
â”‚   â”œâ”€â”€ fusion_config.py        # Fusion-specific configs
â”‚   â””â”€â”€ training_config.py      # Training configurations
â”œâ”€â”€ fusion/                     # Fusion implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ fusion_layer.py         # Core fusion mechanisms
â”‚   â”œâ”€â”€ adapter_manager.py      # Multi-adapter management
â”‚   â””â”€â”€ fusion_model.py         # Model with fusion
â”œâ”€â”€ adapters/                   # Individual adapter implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ adapter_layer.py        # Basic adapter layer
â”‚   â””â”€â”€ task_adapters.py        # Task-specific adapters
â”œâ”€â”€ data/                       # Data processing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ multi_task_loader.py    # Multi-task data loading
â”‚   â””â”€â”€ task_datasets.py        # Task-specific datasets
â”œâ”€â”€ training/                   # Training scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ adapter_trainer.py      # Individual adapter training
â”‚   â”œâ”€â”€ fusion_trainer.py       # Fusion training
â”‚   â””â”€â”€ multi_task_trainer.py   # Multi-task training
â”œâ”€â”€ evaluation/                 # Evaluation scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ fusion_evaluator.py     # Fusion evaluation
â”‚   â””â”€â”€ task_evaluator.py       # Task-specific evaluation
â”œâ”€â”€ inference/                  # Inference scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ fusion_pipeline.py      # Fusion inference pipeline
â”œâ”€â”€ examples/                   # Example scripts
â”‚   â”œâ”€â”€ train_individual_adapters.py
â”‚   â”œâ”€â”€ fusion_training.py
â”‚   â””â”€â”€ multi_task_fusion.py
â””â”€â”€ notebooks/                  # Jupyter notebooks
    â”œâ”€â”€ 01_adapter_fusion_basics.ipynb
    â”œâ”€â”€ 02_training_workflow.ipynb
    â””â”€â”€ 03_fusion_analysis.ipynb
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Install the package
pip install -e .
```

### 2. Basic Workflow

#### Step 1: Train Individual Adapters
```python
from training import AdapterTrainer
from config import ModelConfig, AdapterConfig

# Train adapter for each task
for task_name in ["sentiment", "nli", "qa"]:
    trainer = AdapterTrainer(task_name)
    trainer.train(task_dataset)
    trainer.save_adapter(f"./adapters/{task_name}")
```

#### Step 2: Fusion Training
```python
from training import FusionTrainer
from config import FusionConfig

# Load trained adapters and perform fusion
fusion_config = FusionConfig(
    adapter_paths=["./adapters/sentiment", "./adapters/nli", "./adapters/qa"],
    fusion_method="attention"
)

fusion_trainer = FusionTrainer(fusion_config)
fusion_trainer.train_fusion(multi_task_dataset)
```

#### Step 3: Inference
```python
from inference import FusionPipeline

# Use fused model for inference
pipeline = FusionPipeline.from_pretrained("./fusion_model")
result = pipeline.predict("This movie is great!", task="sentiment")
```

## ğŸ”§ Key Features

### âœ… Multi-Task Adapter Training
- **Independent Training**: Train adapters for different tasks separately
- **Task-Specific Optimization**: Optimized for each task's characteristics
- **Modular Design**: Easy to add/remove tasks

### âœ… Advanced Fusion Mechanisms
- **Attention Fusion**: Learn to attend to relevant adapters
- **Weighted Fusion**: Learnable combination weights
- **Dynamic Fusion**: Input-dependent adapter selection
- **Hierarchical Fusion**: Multi-level fusion strategies

### âœ… Flexible Architecture
- **Multiple Fusion Points**: Fusion at different transformer layers
- **Adapter Composition**: Stack and combine different adapter types
- **Task Routing**: Automatic task detection and routing

### âœ… Training Strategies
- **Sequential Training**: Train adapters then fusion
- **Joint Training**: Train adapters and fusion together
- **Continual Learning**: Add new tasks without forgetting

## ğŸ“Š Supported Tasks

### Text Classification
- Sentiment Analysis (SST-2, IMDB)
- Natural Language Inference (MNLI, SNLI)
- Topic Classification (AG News)

### Token Classification
- Named Entity Recognition (CoNLL-2003)
- Part-of-Speech Tagging

### Question Answering
- Reading Comprehension (SQuAD)
- Multiple Choice QA

### Text Generation
- Summarization
- Translation

## ğŸ§  Fusion Methods

### 1. Attention-based Fusion
```python
fusion_config = FusionConfig(
    fusion_method="attention",
    num_attention_heads=8,
    attention_dropout=0.1
)
```

### 2. Weighted Average Fusion
```python
fusion_config = FusionConfig(
    fusion_method="weighted",
    learnable_weights=True,
    weight_initialization="uniform"
)
```

### 3. Gating Fusion
```python
fusion_config = FusionConfig(
    fusion_method="gating",
    gate_activation="sigmoid",
    gate_bias=True
)
```

## ğŸ“ˆ Performance Benefits

### Parameter Efficiency
```
Base Model: 110M parameters (shared)
Task A Adapter: 0.5M parameters
Task B Adapter: 0.5M parameters  
Task C Adapter: 0.5M parameters
Fusion Layer: 0.1M parameters
Total: 111.6M parameters (vs 330M for separate models)
```

### Knowledge Transfer
- Cross-task knowledge sharing
- Improved performance on low-resource tasks
- Better generalization

### Scalability
- Easy to add new tasks
- Modular architecture
- Efficient inference

## ğŸ”¬ Advanced Features

### Multi-Level Fusion
- Fusion at multiple transformer layers
- Layer-specific fusion strategies
- Hierarchical knowledge combination

### Dynamic Adapter Selection
- Input-dependent adapter activation
- Sparse adapter usage
- Efficient computation

### Continual Learning
- Add new adapters without retraining
- Prevent catastrophic forgetting
- Incremental knowledge accumulation

## ğŸ“– Documentation

See the `notebooks/` directory for detailed tutorials:
1. **Adapter Fusion Basics**: Understanding fusion mechanisms
2. **Training Workflow**: Step-by-step training guide
3. **Fusion Analysis**: Analyzing fusion behavior

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Implement your changes
4. Add tests and documentation
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- [AdapterFusion Paper](https://arxiv.org/abs/2005.00247) for the original fusion concept
- [Adapter-Hub](https://adapterhub.ml/) for the adapter ecosystem
- Hugging Face for the transformers library
