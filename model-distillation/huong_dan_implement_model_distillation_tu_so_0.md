# ğŸ“ HÆ°á»›ng Dáº«n Implement Model Distillation Tá»« Sá»‘ 0

HÆ°á»›ng dáº«n nÃ y sáº½ giÃºp báº¡n hiá»ƒu vÃ  xÃ¢y dá»±ng láº¡i toÃ n bá»™ há»‡ thá»‘ng Model Distillation tá»« Ä‘áº§u, tá»«ng bÆ°á»›c má»™t.

## ğŸ“š Kiáº¿n Thá»©c Cáº§n CÃ³ TrÆ°á»›c

### 1. Deep Learning Fundamentals
- Neural network training vÃ  backpropagation
- Loss functions vÃ  optimization
- Model compression techniques

### 2. Transformer Architecture
- Attention mechanism vÃ  multi-head attention
- Layer structure vÃ  hidden states
- Model size vÃ  parameter counting

### 3. Knowledge Transfer Concepts
- Teacher-student paradigm
- Soft targets vs hard targets
- Temperature scaling

---

## ğŸ“ Model Distillation LÃ  GÃ¬?

### Váº¥n Äá» Vá»›i Large Models
```
BERT-large: 340M parameters (1.3GB)
â†’ High memory usage (4GB GPU)
â†’ Slow inference (100ms per batch)
â†’ Expensive deployment
â†’ Not suitable for mobile/edge devices
```

### Giáº£i PhÃ¡p: Knowledge Distillation
```
Teacher (BERT-large): 340M parameters
â†“ Knowledge Transfer
Student (DistilBERT): 66M parameters (6x smaller)

Results:
â†’ 6x parameter reduction
â†’ 2x faster inference
â†’ 97% performance retention
â†’ Mobile-friendly deployment
```

### Distillation vs Other Compression
```python
# Pruning: Remove weights
pruned_model = remove_small_weights(model, threshold=0.01)

# Quantization: Reduce precision
quantized_model = convert_to_int8(model)

# Distillation: Transfer knowledge
student_model = distill_knowledge(teacher_model, student_architecture)
```

---

## ğŸ—ï¸ BÆ°á»›c 1: Hiá»ƒu Kiáº¿n TrÃºc Knowledge Distillation

### Teacher-Student Paradigm
```python
# Teacher model (large, pre-trained)
teacher = BertForSequenceClassification.from_pretrained("bert-large-uncased")
teacher.eval()  # Always in evaluation mode

# Student model (small, to be trained)
student = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")
student.train()  # Training mode
```

### Temperature Scaling
```python
def temperature_scaled_softmax(logits, temperature):
    """
    Temperature scaling makes probability distribution softer
    
    High temperature (T=4): [0.4, 0.35, 0.25] - softer, more information
    Low temperature (T=1):  [0.7, 0.2, 0.1]  - harder, less information
    """
    return F.softmax(logits / temperature, dim=-1)

# Teacher soft targets
teacher_probs = temperature_scaled_softmax(teacher_logits, temperature=4.0)

# Student predictions  
student_probs = temperature_scaled_softmax(student_logits, temperature=4.0)
```

### Knowledge Transfer Loss
```python
def distillation_loss(student_logits, teacher_logits, true_labels, alpha, temperature):
    """
    Combined loss: soft targets + hard targets
    
    Î± = 0.7: 70% knowledge distillation, 30% task learning
    """
    # Soft target loss (knowledge from teacher)
    kl_loss = F.kl_div(
        F.log_softmax(student_logits / temperature, dim=-1),
        F.softmax(teacher_logits / temperature, dim=-1),
        reduction='batchmean'
    ) * (temperature ** 2)
    
    # Hard target loss (ground truth)
    ce_loss = F.cross_entropy(student_logits, true_labels)
    
    # Combined loss
    return alpha * kl_loss + (1 - alpha) * ce_loss
```

---

## ğŸ”§ BÆ°á»›c 2: Implement Distillation Loss Functions

### 2.1 Táº¡o `distillation/losses.py`

```python
"""
Distillation loss functions - TrÃ¡i tim cá»§a knowledge transfer
"""
import torch
import torch.nn as nn
import torch.nn.functional as F

class DistillationLoss(nn.Module):
    """Comprehensive distillation loss functions"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # Task loss (standard cross-entropy)
        self.task_loss_fn = nn.CrossEntropyLoss(label_smoothing=config.label_smoothing)
        
        print("âœ… DistillationLoss initialized")
    
    def kl_divergence_loss(self, student_logits, teacher_logits, temperature):
        """
        KL divergence loss for logit distillation
        
        KL(P||Q) = Î£ P(x) * log(P(x) / Q(x))
        Where P = teacher, Q = student
        """
        # Apply temperature scaling
        student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)
        teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)
        
        # KL divergence loss
        kl_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean')
        
        # Scale by temperature squared (important!)
        return kl_loss * (temperature ** 2)
    
    def feature_distillation_loss(self, student_features, teacher_features):
        """
        Feature distillation loss for intermediate representations
        
        Match hidden states between teacher and student layers
        """
        total_loss = 0.0
        num_layers = 0
        
        for student_feat, teacher_feat in zip(student_features, teacher_features):
            # Project student features if dimensions don't match
            if student_feat.size(-1) != teacher_feat.size(-1):
                # Simple linear projection
                projection = nn.Linear(student_feat.size(-1), teacher_feat.size(-1)).to(student_feat.device)
                student_feat = projection(student_feat)
            
            # Normalize features if configured
            if self.config.normalize_features:
                student_feat = F.normalize(student_feat, p=2, dim=-1)
                teacher_feat = F.normalize(teacher_feat, p=2, dim=-1)
            
            # MSE loss between features
            layer_loss = F.mse_loss(student_feat, teacher_feat)
            total_loss += layer_loss
            num_layers += 1
        
        return total_loss / max(num_layers, 1)
    
    def attention_distillation_loss(self, student_attentions, teacher_attentions):
        """
        Attention distillation loss for attention patterns
        
        Transfer attention patterns from teacher to student
        """
        total_loss = 0.0
        num_layers = 0
        
        for student_attn, teacher_attn in zip(student_attentions, teacher_attentions):
            # Handle different number of attention heads
            if student_attn.size(1) != teacher_attn.size(1):
                # Average teacher attention heads to match student
                teacher_attn = teacher_attn.mean(dim=1, keepdim=True).expand(-1, student_attn.size(1), -1, -1)
            
            # MSE loss between attention matrices
            layer_loss = F.mse_loss(student_attn, teacher_attn)
            total_loss += layer_loss
            num_layers += 1
        
        return total_loss / max(num_layers, 1)
    
    def compute_combined_loss(self, student_outputs, teacher_outputs, labels, temperature, step=0):
        """
        Compute combined distillation loss
        
        Combines multiple loss types based on configuration
        """
        losses = {}
        
        # Task loss (hard targets)
        if labels is not None:
            task_loss = self.task_loss_fn(student_outputs['logits'], labels)
            losses['task_loss'] = task_loss
        
        # Logit distillation loss (soft targets)
        if 'logits' in teacher_outputs:
            distillation_loss = self.kl_divergence_loss(
                student_outputs['logits'],
                teacher_outputs['logits'],
                temperature
            )
            losses['distillation_loss'] = distillation_loss
        
        # Feature distillation loss
        if ('hidden_states' in student_outputs and 'hidden_states' in teacher_outputs and
            self.config.distillation_type in ['feature', 'combined']):
            
            feature_loss = self.feature_distillation_loss(
                student_outputs['hidden_states'],
                teacher_outputs['hidden_states']
            )
            losses['feature_loss'] = feature_loss
        
        # Attention distillation loss
        if ('attentions' in student_outputs and 'attentions' in teacher_outputs and
            self.config.distillation_type in ['attention', 'combined']):
            
            attention_loss = self.attention_distillation_loss(
                student_outputs['attentions'],
                teacher_outputs['attentions']
            )
            losses['attention_loss'] = attention_loss
        
        # Combine losses with weights
        loss_weights = self.config.get_loss_weights()
        total_loss = 0.0
        
        if 'task_loss' in losses:
            total_loss += loss_weights.get('task', 0.0) * losses['task_loss']
        
        if 'distillation_loss' in losses:
            total_loss += loss_weights.get('distillation', 0.0) * losses['distillation_loss']
        
        if 'feature_loss' in losses:
            total_loss += loss_weights.get('feature', 0.0) * losses['feature_loss']
        
        if 'attention_loss' in losses:
            total_loss += loss_weights.get('attention', 0.0) * losses['attention_loss']
        
        losses['total_loss'] = total_loss
        losses['temperature'] = torch.tensor(temperature)
        
        return losses
```

**Giáº£i thÃ­ch chi tiáº¿t:**
- `kl_divergence_loss()`: KL divergence cho soft targets
- `temperature ** 2`: Scaling factor quan trá»ng tá»« paper gá»‘c
- `feature_distillation_loss()`: Match intermediate representations
- `attention_distillation_loss()`: Transfer attention patterns
- Combined loss vá»›i configurable weights

---

## â° Táº¡m Dá»«ng - Checkpoint 1

Äáº¿n Ä‘Ã¢y báº¡n Ä‘Ã£ hiá»ƒu:
1. âœ… Model Distillation concept vÃ  teacher-student paradigm
2. âœ… Temperature scaling vÃ  soft targets
3. âœ… KL divergence loss implementation
4. âœ… Feature vÃ  attention distillation
5. âœ… Combined loss functions

**Tiáº¿p theo**: ChÃºng ta sáº½ implement complete distiller, teacher/student models, vÃ  training system.

---

## ğŸ« BÆ°á»›c 3: Implement Teacher vÃ  Student Models

### 3.1 Táº¡o `distillation/teacher_model.py`

```python
"""
Teacher model wrapper
"""
import torch
import torch.nn as nn
from transformers import AutoModelForSequenceClassification

class TeacherModel(nn.Module):
    """Teacher model wrapper for knowledge distillation"""

    def __init__(self, config):
        super().__init__()
        self.config = config

        # Load pre-trained teacher model
        self.model = AutoModelForSequenceClassification.from_pretrained(
            config.model_name_or_path,
            num_labels=config.num_labels,
            output_hidden_states=True,
            output_attentions=True
        )

        # Freeze all parameters (teacher doesn't train)
        for param in self.model.parameters():
            param.requires_grad = False

        # Always in evaluation mode
        self.model.eval()

        print(f"âœ… Teacher model loaded: {config.model_name_or_path}")
        print(f"Parameters: {sum(p.numel() for p in self.model.parameters()):,}")

    def forward(self, **kwargs):
        """Forward pass through teacher model"""
        with torch.no_grad():  # No gradients for teacher
            return self.model(**kwargs)

    def get_soft_targets(self, logits, temperature):
        """Get soft targets from teacher logits"""
        return torch.softmax(logits / temperature, dim=-1)

class StudentModel(nn.Module):
    """Student model wrapper for knowledge distillation"""

    def __init__(self, config):
        super().__init__()
        self.config = config

        # Load student model (smaller architecture)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            config.model_name_or_path,
            num_labels=config.num_labels,
            output_hidden_states=True,
            output_attentions=True
        )

        print(f"âœ… Student model loaded: {config.model_name_or_path}")
        print(f"Parameters: {sum(p.numel() for p in self.model.parameters()):,}")

    def forward(self, **kwargs):
        """Forward pass through student model"""
        return self.model(**kwargs)

    def save_pretrained(self, save_directory):
        """Save student model"""
        self.model.save_pretrained(save_directory)
```

### 3.2 Táº¡o `distillation/distiller.py`

```python
"""
Main distillation orchestrator
"""
import torch
from tqdm import tqdm

class Distiller:
    """Main distillation class"""

    def __init__(self, teacher, student, config, tokenizer=None):
        self.teacher = teacher
        self.student = student
        self.config = config
        self.tokenizer = tokenizer

        # Initialize loss function
        self.loss_fn = DistillationLoss(config)

        # Training state
        self.current_step = 0
        self.current_temperature = config.temperature

        # Device setup
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.teacher.to(self.device)
        self.student.to(self.device)

        print("ğŸ“ Distiller initialized")
        self._print_model_comparison()

    def _print_model_comparison(self):
        """Print teacher vs student comparison"""
        teacher_params = sum(p.numel() for p in self.teacher.parameters())
        student_params = sum(p.numel() for p in self.student.parameters())

        compression_ratio = teacher_params / student_params
        size_reduction = (1 - student_params / teacher_params) * 100

        print("\nğŸ“Š MODEL COMPARISON:")
        print("=" * 50)
        print(f"Teacher parameters: {teacher_params:,}")
        print(f"Student parameters: {student_params:,}")
        print(f"Compression ratio: {compression_ratio:.1f}x")
        print(f"Size reduction: {size_reduction:.1f}%")
        print("=" * 50)

    def distill(self, train_dataloader, eval_dataloader=None, num_epochs=3, save_dir="./distilled_model"):
        """Main distillation training loop"""

        print("ğŸ“ Starting knowledge distillation...")

        # Setup optimizer
        optimizer = torch.optim.AdamW(
            self.student.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )

        # Training loop
        self.student.train()
        self.teacher.eval()  # Teacher always in eval mode

        for epoch in range(num_epochs):
            print(f"Epoch {epoch + 1}/{num_epochs}")

            epoch_losses = self._train_epoch(train_dataloader, optimizer, epoch)

            # Evaluation
            if eval_dataloader is not None:
                eval_stats = self._evaluate(eval_dataloader)
                print(f"Eval accuracy: {eval_stats.get('accuracy', 0):.4f}")

        # Save final model
        self.save_student_model(save_dir)
        print("âœ… Distillation completed!")

    def _train_epoch(self, dataloader, optimizer, epoch):
        """Train for one epoch"""

        epoch_losses = {"total_loss": 0.0, "distillation_loss": 0.0, "task_loss": 0.0}
        num_batches = len(dataloader)

        with tqdm(dataloader, desc=f"Training Epoch {epoch + 1}") as pbar:
            for batch_idx, batch in enumerate(pbar):
                # Move batch to device
                batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v
                        for k, v in batch.items()}

                # Forward pass
                losses = self._forward_step(batch)

                # Backward pass
                optimizer.zero_grad()
                losses['total_loss'].backward()

                # Gradient clipping
                if self.config.max_grad_norm > 0:
                    torch.nn.utils.clip_grad_norm_(
                        self.student.parameters(),
                        self.config.max_grad_norm
                    )

                optimizer.step()

                # Update epoch losses
                for key in epoch_losses:
                    if key in losses:
                        epoch_losses[key] += losses[key].item()

                # Update progress bar
                pbar.set_postfix({
                    'loss': f"{losses['total_loss'].item():.4f}",
                    'temp': f"{self.current_temperature:.2f}"
                })

                self.current_step += 1

        # Average epoch losses
        for key in epoch_losses:
            epoch_losses[key] /= num_batches

        return epoch_losses

    def _forward_step(self, batch):
        """Forward step for distillation"""

        # Teacher forward pass (no gradients)
        with torch.no_grad():
            teacher_outputs = self.teacher(
                input_ids=batch['input_ids'],
                attention_mask=batch.get('attention_mask'),
                output_hidden_states=True,
                output_attentions=True
            )

        # Student forward pass
        student_outputs = self.student(
            input_ids=batch['input_ids'],
            attention_mask=batch.get('attention_mask'),
            output_hidden_states=True,
            output_attentions=True
        )

        # Compute distillation losses
        losses = self.loss_fn.compute_combined_loss(
            student_outputs,
            teacher_outputs,
            batch.get('labels'),
            self.current_temperature,
            self.current_step
        )

        return losses

    def _evaluate(self, dataloader):
        """Evaluate student model"""
        self.student.eval()

        total_correct = 0
        total_samples = 0

        with torch.no_grad():
            for batch in tqdm(dataloader, desc="Evaluating"):
                batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v
                        for k, v in batch.items()}

                outputs = self.student(**batch)

                if 'labels' in batch:
                    predictions = torch.argmax(outputs.logits, dim=-1)
                    correct = (predictions == batch['labels']).sum().item()
                    total_correct += correct
                    total_samples += batch['labels'].size(0)

        self.student.train()

        return {"accuracy": total_correct / total_samples if total_samples > 0 else 0.0}

    def save_student_model(self, save_dir):
        """Save the distilled student model"""
        import os
        os.makedirs(save_dir, exist_ok=True)

        # Save student model
        self.student.save_pretrained(save_dir)

        # Save tokenizer if available
        if self.tokenizer is not None:
            self.tokenizer.save_pretrained(save_dir)

        print(f"ğŸ’¾ Distilled model saved to {save_dir}")
```

---

## ğŸ¯ BÆ°á»›c 4: Complete Example

### 4.1 Táº¡o `examples/text_classification_distillation.py`

```python
"""
Complete distillation example
"""
from datasets import load_dataset
from transformers import AutoTokenizer
from torch.utils.data import DataLoader

def main():
    """Main distillation example"""

    print("ğŸ“ Model Distillation Example")

    # Load dataset
    dataset = load_dataset("imdb")
    train_data = dataset["train"].select(range(1000))  # Small sample
    test_data = dataset["test"].select(range(200))

    # Setup configurations
    from config import TeacherConfig, StudentConfig, DistillationConfig

    teacher_config = TeacherConfig(
        model_name_or_path="bert-base-uncased",  # Teacher: BERT-base
        task_type="classification",
        num_labels=2
    )

    student_config = StudentConfig(
        model_name_or_path="distilbert-base-uncased",  # Student: DistilBERT
        task_type="classification",
        num_labels=2
    )

    distillation_config = DistillationConfig(
        distillation_type="logit",
        temperature=4.0,
        alpha=0.7,  # 70% distillation loss
        beta=0.3,   # 30% task loss
        learning_rate=5e-5
    )

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(teacher_config.model_name_or_path)

    # Preprocess data
    def preprocess_function(examples):
        return tokenizer(
            examples["text"],
            padding=True,
            truncation=True,
            max_length=128
        )

    train_dataset = train_data.map(preprocess_function, batched=True)
    test_dataset = test_data.map(preprocess_function, batched=True)

    # Create data loaders
    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)

    # Create teacher and student models
    from distillation import TeacherModel, StudentModel, Distiller

    teacher = TeacherModel(teacher_config)
    student = StudentModel(student_config)

    # Create distiller
    distiller = Distiller(teacher, student, distillation_config, tokenizer)

    # Perform distillation
    distiller.distill(
        train_dataloader=train_dataloader,
        eval_dataloader=test_dataloader,
        num_epochs=3,
        save_dir="./distilled_model"
    )

    # Compare models
    comparison = distiller.compare_models(test_dataloader)

    print("\nğŸ“Š RESULTS:")
    print(f"Teacher accuracy: {comparison['teacher']['accuracy']:.4f}")
    print(f"Student accuracy: {comparison['student']['accuracy']:.4f}")
    print(f"Performance retention: {comparison['performance_retention']:.1f}%")
    print(f"Compression ratio: {comparison['compression_ratio']:.1f}x")

    print("\nâœ… Distillation completed!")

if __name__ == "__main__":
    import torch
    main()
```

---

## ğŸ‰ HoÃ n ThÃ nh - Báº¡n ÄÃ£ CÃ³ Há»‡ Thá»‘ng Model Distillation!

### TÃ³m Táº¯t Nhá»¯ng GÃ¬ ÄÃ£ Implement:

1. âœ… **Distillation Loss Functions**: KL divergence, feature matching, attention transfer
2. âœ… **Teacher-Student Models**: Proper model wrappers vá»›i gradient handling
3. âœ… **Complete Distiller**: Training orchestration vá»›i temperature scheduling
4. âœ… **Multiple Distillation Types**: Logit, feature, attention, combined
5. âœ… **Complete Example**: End-to-end distillation workflow

### CÃ¡ch Cháº¡y:
```bash
cd model-distillation
python examples/text_classification_distillation.py
```

### Hiá»‡u Quáº£ Äáº¡t ÄÆ°á»£c:
```
BERT-base â†’ DistilBERT:
- Parameters: 110M â†’ 66M (6x reduction)
- Model size: 440MB â†’ 255MB (5x reduction)
- Inference speed: 1x â†’ 2x faster
- Performance: 100% â†’ 97% (3% drop)
- Memory usage: 4GB â†’ 1GB (4x reduction)
```

### So SÃ¡nh Compression Methods:
```
Method          | Size Reduction | Performance | Speed | Complexity
----------------|----------------|-------------|-------|----------
Pruning         | 50-90%        | 85-95%      | 2-5x  | Medium
Quantization    | 75%           | 90-98%      | 2-4x  | Low
Distillation    | 60-95%        | 85-98%      | 2-10x | Medium
Combined        | 95-99%        | 80-95%      | 5-20x | High
```

### Khi NÃ o DÃ¹ng Model Distillation:
- âœ… Deploy models on mobile/edge devices
- âœ… Reduce inference costs in production
- âœ… Speed up real-time applications
- âœ… Maintain good performance with smaller models
- âœ… Transfer knowledge across architectures

### BÆ°á»›c Tiáº¿p Theo:
1. Cháº¡y example Ä‘á»ƒ tháº¥y káº¿t quáº£
2. Thá»­ different teacher-student pairs
3. Experiment vá»›i different temperatures (1, 2, 4, 8)
4. Test feature vÃ  attention distillation
5. Combine vá»›i quantization cho extreme compression

**ChÃºc má»«ng! Báº¡n Ä‘Ã£ hiá»ƒu vÃ  implement Ä‘Æ°á»£c Model Distillation tá»« sá»‘ 0! ğŸ“**
