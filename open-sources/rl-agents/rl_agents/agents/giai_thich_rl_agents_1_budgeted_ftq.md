# üéØ Gi·∫£i Th√≠ch Chi Ti·∫øt: Budgeted FTQ (Budgeted Fitted Q-Iteration)

## üìö T·ªïng Quan Thu·∫≠t To√°n

**Budgeted FTQ** l√† m·ªôt thu·∫≠t to√°n Reinforcement Learning n√¢ng cao ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ gi·∫£i quy·∫øt **Budgeted Markov Decision Processes (BMDPs)**. ƒê√¢y l√† extension c·ªßa Fitted Q-Iteration ƒë·ªÉ handle c√°c b√†i to√°n c√≥ **resource constraints** (r√†ng bu·ªôc t√†i nguy√™n).

---

## üéØ V·∫•n ƒê·ªÅ C·∫ßn Gi·∫£i Quy·∫øt

### Traditional MDP vs Budgeted MDP

**Traditional MDP:**
```
State ‚Üí Action ‚Üí Reward + Next State
Goal: Maximize cumulative reward
```

**Budgeted MDP:**
```
State ‚Üí Action ‚Üí Reward + Cost + Next State
Goal: Maximize reward while staying within budget constraint
Constraint: Œ£ costs ‚â§ Budget
```

### V√≠ D·ª• Th·ª±c T·∫ø:
- **Autonomous Driving**: Maximize safety while minimizing fuel consumption
- **Resource Management**: Maximize profit while staying within budget
- **Energy Systems**: Maximize performance while limiting energy usage

---

## üèóÔ∏è Ki·∫øn Tr√∫c Budgeted FTQ

### Core Components:

```python
class BFTQAgent(AbstractAgent):
    def __init__(self, env, config=None):
        # 1. Budgeted Fitted Q-Learning Algorithm
        self.bftq = BudgetedFittedQ(...)
        
        # 2. Exploration Policy v·ªõi budget awareness
        self.exploration_policy = EpsilonGreedyBudgetedPolicy(...)
        
        # 3. Budget tracking
        self.beta = 0  # Current budget
        
        # 4. Neural Network cho Q-value v√† Cost estimation
        network = BudgetedMLP(...)
```

---

## üìñ Ph√¢n T√≠ch T·ª´ng D√≤ng Code

### D√≤ng 16-29: Class Initialization
```python
class BFTQAgent(AbstractAgent):
    def __init__(self, env, config=None):
        super(BFTQAgent, self).__init__(config)
        self.batched = True  # Supports batch processing
        if not self.config["epochs"]:
            # Auto-calculate epochs based on discount factor
            self.config["epochs"] = int(1 / np.log(1 / self.config["gamma"]))
        self.env = env
        self.bftq = None  # Will be initialized in reset()
        self.exploration_policy = None
        self.beta = self.previous_beta = 0  # Budget tracking
        self.training = True
        self.previous_state = None
```

**Gi·∫£i th√≠ch:**
- `batched = True`: Agent c√≥ th·ªÉ x·ª≠ l√Ω multiple experiences c√πng l√∫c
- `epochs`: S·ªë iterations cho fitted Q-iteration, auto-calculated t·ª´ gamma
- `beta`: Current budget available cho agent
- `training`: Flag ƒë·ªÉ switch gi·ªØa training v√† evaluation mode

### D√≤ng 33-80: Default Configuration
```python
@classmethod
def default_config(cls):
    return {
        "gamma": 0.9,        # Discount factor cho rewards
        "gamma_c": 0.9,      # Discount factor cho costs
        "epochs": None,      # Auto-calculated
        "delta_stop": 0.,    # Stopping criterion
        "memory_capacity": 10000,  # Experience replay buffer size
        "beta": 0,           # Default budget
        "betas_for_duplication": "np.arange(0, 1, 0.1)",    # Budget discretization
        "betas_for_discretisation": "np.arange(0, 1, 0.1)",
        "exploration": {
            "temperature": 1.0,        # Initial exploration temperature
            "final_temperature": 0.1,  # Final exploration temperature
            "tau": 5000               # Temperature decay rate
        },
        # ... network architecture config
    }
```

**Key Parameters:**
- **gamma vs gamma_c**: Separate discount factors cho rewards v√† costs
- **betas_for_discretisation**: Discretize budget space cho tractable computation
- **temperature**: Controls exploration-exploitation trade-off

### D√≤ng 82-92: Action Selection
```python
def act(self, state):
    """
    Run the exploration policy to pick actions and budgets
    """
    # Choose budget: random during training, fixed during evaluation
    self.beta = self.np_random.uniform() if self.training else self.config["beta"]
    
    state = state.flatten()
    self.previous_state, self.previous_beta = state, self.beta
    
    # Execute policy v·ªõi current state v√† budget
    action, self.beta = self.exploration_policy.execute(state, self.beta)
    return action
```

**Gi·∫£i th√≠ch:**
- **Budget Selection**: Random budget during training ƒë·ªÉ explore different budget levels
- **Policy Execution**: Policy takes both state v√† budget l√†m input
- **Budget Update**: Policy c√≥ th·ªÉ modify budget (budget allocation)

### D√≤ng 94-103: Experience Recording
```python
def record(self, state, action, reward, next_state, done, info):
    """
    Record a transition to update the BFTQ policy
    """
    if not self.training:
        return
    
    # Store transition v·ªõi cost information
    self.bftq.push(state.flatten(), action, reward, 
                   next_state.flatten(), done, info["cost"])
```

**Gi·∫£i th√≠ch:**
- **Cost Tracking**: Record c·∫£ reward v√† cost t·ª´ environment
- **Experience Replay**: Store transitions cho batch learning
- **info["cost"]**: Environment ph·∫£i provide cost information

### D√≤ng 105-113: Model Update
```python
def update(self):
    """
    Fit a budgeted policy on the batch by running the BFTQ algorithm.
    """
    # Reset v√† run BFTQ algorithm
    self.bftq.reset()
    network = self.bftq.run()
    
    # Update greedy policy v·ªõi new network
    self.exploration_policy.pi_greedy.set_network(network)
```

**Gi·∫£i th√≠ch:**
- **BFTQ Algorithm**: Core algorithm ƒë·ªÉ learn Q-values v√† cost estimates
- **Network Update**: Update policy network v·ªõi learned parameters
- **Batch Learning**: Update sau khi collect ƒë·ªß experience

### D√≤ng 115-134: Agent Reset
```python
def reset(self):
    if not self.np_random:
        self.seed()
    
    # Initialize neural network
    network = BudgetedMLP(size_state=np.prod(self.env.observation_space.shape),
                          n_actions=self.env.action_space.n,
                          **self.config["network"])
    
    # Initialize BFTQ algorithm
    self.bftq = BudgetedFittedQ(value_network=network, 
                                config=self.config, 
                                writer=self.writer)
    
    # Initialize exploration policy
    self.exploration_policy = EpsilonGreedyBudgetedPolicy(
        pi_greedy=PytorchBudgetedFittedPolicy(...),
        pi_random=RandomBudgetedPolicy(...),
        config=self.config["exploration"],
        np_random=self.np_random
    )
```

**Components:**
- **BudgetedMLP**: Neural network architecture cho Q-value v√† cost prediction
- **BudgetedFittedQ**: Core algorithm implementation
- **EpsilonGreedyBudgetedPolicy**: Exploration strategy v·ªõi budget awareness

---

## üß† Budgeted FTQ Algorithm Deep Dive

### Mathematical Foundation:

**Budgeted Q-Function:**
```
Q^œÄ(s, Œ≤, a) = E[Œ£ Œ≥^t r_t | s_0=s, Œ≤_0=Œ≤, a_0=a, œÄ]
C^œÄ(s, Œ≤, a) = E[Œ£ Œ≥_c^t c_t | s_0=s, Œ≤_0=Œ≤, a_0=a, œÄ]

Subject to: C^œÄ(s, Œ≤, a) ‚â§ Œ≤
```

**Bellman Equations:**
```
Q*(s, Œ≤, a) = r + Œ≥ max_{a'} Q*(s', Œ≤', a')
C*(s, Œ≤, a) = c + Œ≥_c max_{a'} C*(s', Œ≤', a')

Where: Œ≤' = Œ≤ - c (remaining budget)
```

### Algorithm Steps:

1. **Experience Collection**: Collect (s, a, r, c, s') tuples
2. **Budget Discretization**: Discretize budget space
3. **Q-Value Fitting**: Fit Q(s, Œ≤, a) v√† C(s, Œ≤, a) networks
4. **Policy Extraction**: Extract optimal policy t·ª´ Q-values
5. **Convex Hull**: Compute Pareto frontier cho reward-cost trade-offs

---

## üîÑ Training Workflow

### Complete Training Process:

```
1. Environment Interaction
   State s, Budget Œ≤ ‚Üí Policy ‚Üí Action a
                    ‚Üì
   Environment ‚Üí Reward r, Cost c, Next State s'
                    ‚Üì
2. Experience Storage
   Store (s, Œ≤, a, r, c, s') in replay buffer
                    ‚Üì
3. Batch Learning (when buffer full)
   Sample batch ‚Üí BFTQ Algorithm ‚Üí Update Q-networks
                    ‚Üì
4. Policy Update
   New Q-networks ‚Üí Extract Policy ‚Üí Update Exploration Policy
                    ‚Üì
5. Repeat until convergence
```

---

## üéØ Key Features c·ªßa Budgeted FTQ

### 1. **Budget-Aware Learning**
```python
# Policy considers both state v√† budget
action = policy(state, budget)

# Q-function depends on available budget
q_value = Q(state, budget, action)
cost_estimate = C(state, budget, action)
```

### 2. **Pareto Optimality**
```python
# Find Pareto frontier c·ªßa reward-cost trade-offs
pareto_policies = compute_convex_hull(q_values, cost_estimates)

# Select policy based on budget constraint
optimal_policy = select_policy(pareto_policies, budget_constraint)
```

### 3. **Dual Discount Factors**
```python
# Separate discounting cho rewards v√† costs
discounted_reward = gamma ** t * reward
discounted_cost = gamma_c ** t * cost

# Allows different time preferences
```

---

## üí° ∆Øu ƒêi·ªÉm v√† Nh∆∞·ª£c ƒêi·ªÉm

### ‚úÖ **∆Øu ƒêi·ªÉm:**
- **Resource Awareness**: Explicitly handles budget constraints
- **Pareto Optimality**: Finds optimal reward-cost trade-offs
- **Flexible Budgeting**: Supports dynamic budget allocation
- **Theoretical Foundation**: Well-grounded mathematical framework

### ‚ùå **Nh∆∞·ª£c ƒêi·ªÉm:**
- **Computational Complexity**: Higher than standard Q-learning
- **Budget Discretization**: May lose precision v·ªõi continuous budgets
- **Environment Requirements**: Needs cost information from environment
- **Hyperparameter Sensitivity**: Many parameters to tune

---

## üöÄ Use Cases

### **Ideal Applications:**
1. **Autonomous Systems**: Energy-constrained robots
2. **Financial Trading**: Risk-constrained portfolio management
3. **Resource Management**: Cloud computing resource allocation
4. **Healthcare**: Treatment planning v·ªõi cost constraints
5. **Smart Grids**: Energy distribution v·ªõi budget limits

### **Environment Requirements:**
```python
# Environment must provide cost information
def step(self, action):
    # ... environment logic ...
    info = {"cost": computed_cost}
    return next_state, reward, done, info
```

---

## üîß Configuration Tips

### **Key Hyperparameters:**
```python
config = {
    "gamma": 0.9,           # Higher = more future-focused
    "gamma_c": 0.9,         # Can be different from gamma
    "beta": 0.5,            # Target budget level
    "betas_for_discretisation": "np.linspace(0, 1, 21)",  # Finer discretization
    "exploration": {
        "temperature": 1.0,  # Higher = more exploration
        "tau": 5000         # Slower decay = longer exploration
    },
    "network": {
        "layers": [128, 128, 64],  # Larger networks cho complex problems
    }
}
```

### **Performance Tuning:**
- **Budget Discretization**: More points = better precision, higher computation
- **Network Size**: Larger networks cho complex state spaces
- **Exploration**: Longer exploration cho complex environments
- **Memory Capacity**: Larger buffer cho better sample efficiency

**Next Algorithm**: Ch√∫ng ta s·∫Ω ti·∫øp t·ª•c v·ªõi **Common Components** - c√°c building blocks ƒë∆∞·ª£c share across multiple RL algorithms!
