# üìö Gi·∫£i Th√≠ch Chi Ti·∫øt File [2]_sources.md - Multi-Query & RAG Fusion

## üéØ M·ª•c ƒê√≠ch File N√†y
File `[2]_sources.md` ch·ª©a ngu·ªìn t√†i li·ªáu cho notebook `[2]_rag_multi_query_and_fusion.ipynb`. ƒê√¢y l√† level 2 trong RAG series, t·∫≠p trung v√†o **Multi-Query Retrieval** v√† **RAG Fusion** - c√°c techniques n√¢ng cao ƒë·ªÉ c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng retrieval.

---

## üìñ Ph√¢n T√≠ch T·ª´ng D√≤ng

### D√≤ng 1: Ti√™u ƒê·ªÅ Ch√≠nh
```markdown
### Here are all the sources used to write-up the `[2]_rag_multi_query_and_fusion.ipynb` file:
```

**Gi·∫£i th√≠ch:**
- File n√†y support notebook level 2 v·ªÅ advanced retrieval techniques
- Focus v√†o vi·ªác c·∫£i thi·ªán **recall** v√† **precision** c·ªßa retrieval process

**V·∫•n ƒë·ªÅ c·∫ßn gi·∫£i quy·∫øt:**
- Single query th∆∞·ªùng kh√¥ng capture ƒë∆∞·ª£c all relevant information
- User query c√≥ th·ªÉ kh√¥ng optimal cho vector search
- C·∫ßn multiple perspectives ƒë·ªÉ t√¨m comprehensive information

---

### D√≤ng 3: LangSmith Documentation
```markdown
1. https://docs.smith.langchain.com/ (LangSmith documentation)
```

**Vai tr√≤ trong Multi-Query RAG:**
- **Trace Multiple Queries**: Monitor performance c·ªßa t·ª´ng query variant
- **Compare Results**: So s√°nh quality c·ªßa different query strategies
- **Debug Fusion Process**: Hi·ªÉu c√°ch c√°c results ƒë∆∞·ª£c combine

**Metrics quan tr·ªçng:**
```python
# LangSmith s·∫Ω track:
{
    "original_query": "What is machine learning?",
    "generated_queries": [
        "Define machine learning",
        "How does ML work?", 
        "Machine learning algorithms explanation"
    ],
    "retrieval_results": {
        "query_1_docs": 5,
        "query_2_docs": 4,
        "query_3_docs": 6,
        "unique_docs_after_fusion": 8
    },
    "response_quality": 0.85
}
```

---

### D√≤ng 4: Multi-Query Retriever
```markdown
2. https://python.langchain.com/docs/how_to/MultiQueryRetriever/ (Multi-query retriever)
```

**Gi·∫£i th√≠ch:**
- **MultiQueryRetriever** l√† core component ƒë·ªÉ generate multiple query variants
- S·ª≠ d·ª•ng LLM ƒë·ªÉ t·∫°o ra different perspectives c·ªßa same question

**C√°ch ho·∫°t ƒë·ªông:**
```
Original Query: "What is Python?"
        ‚Üì
    LLM generates variants:
        ‚Üì
Query 1: "Define Python programming language"
Query 2: "Python language characteristics"  
Query 3: "What are Python features?"
        ‚Üì
    Retrieve docs for each query
        ‚Üì
    Combine & deduplicate results
        ‚Üì
    Return comprehensive document set
```

**Implementation example:**
```python
from langchain.retrievers.multi_query import MultiQueryRetriever

# Setup multi-query retriever
multi_query_retriever = MultiQueryRetriever.from_llm(
    retriever=base_retriever,
    llm=llm,
    prompt=QUERY_PROMPT  # Custom prompt for query generation
)

# S·ª≠ d·ª•ng
docs = multi_query_retriever.get_relevant_documents(
    "What is machine learning?"
)
# S·∫Ω generate 3-5 query variants v√† retrieve docs cho t·∫•t c·∫£
```

**Advantages:**
- **Better Coverage**: Capture more relevant documents
- **Reduced Bias**: Multiple perspectives reduce query bias
- **Improved Recall**: Find documents that single query might miss

---

### D√≤ng 5: RAG Fusion Blog Post
```markdown
3. https://blog.langchain.dev/rag-fusion-a-new-take-on-retrieval-augmented-generation/ (RAG fusion blog post)
```

**Gi·∫£i th√≠ch:**
- **RAG Fusion** l√† advanced technique k·∫øt h·ª£p multiple retrieval strategies
- S·ª≠ d·ª•ng **Reciprocal Rank Fusion (RRF)** ƒë·ªÉ combine results

**Core Concept:**
```
Traditional RAG: Single Query ‚Üí Single Retrieval ‚Üí Generate
RAG Fusion: Multiple Queries ‚Üí Multiple Retrievals ‚Üí Fusion ‚Üí Generate
```

**Reciprocal Rank Fusion Algorithm:**
```python
def reciprocal_rank_fusion(results_list, k=60):
    """
    Combine multiple ranked lists using RRF
    """
    fused_scores = {}
    
    for results in results_list:
        for rank, doc in enumerate(results):
            doc_id = doc.metadata.get('id', str(doc))
            if doc_id not in fused_scores:
                fused_scores[doc_id] = 0
            # RRF formula: 1 / (k + rank)
            fused_scores[doc_id] += 1 / (k + rank + 1)
    
    # Sort by fused score
    return sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)
```

**T·∫°i sao RRF hi·ªáu qu·∫£:**
- **Rank-based**: Focus v√†o relative ranking thay v√¨ absolute scores
- **Robust**: Kh√¥ng b·ªã ·∫£nh h∆∞·ªüng b·ªüi score scale differences
- **Simple**: Easy to implement v√† understand

---

### D√≤ng 6: Reciprocal Rank Fusion Paper
```markdown
4. https://plg.uwaterloo.ca/~gvcormac/cormacksigir09.pdf (Reciprocal rank fusion paper)
```

**Gi·∫£i th√≠ch:**
- Paper g·ªëc v·ªÅ **Reciprocal Rank Fusion** algorithm
- Nghi√™n c·ª©u academic v·ªÅ c√°ch combine multiple ranked lists

**Key Insights t·ª´ paper:**
1. **Parameter k**: Th∆∞·ªùng set k=60 cho optimal performance
2. **Rank Position**: Early ranks c√≥ weight cao h∆°n exponentially
3. **Fusion Effectiveness**: RRF outperform simple score averaging

**Mathematical Foundation:**
```
RRF Score = Œ£(1 / (k + rank_i))

Where:
- k = constant (usually 60)
- rank_i = position of document in list i
- Œ£ = sum across all retrieval lists
```

**Practical Benefits:**
- **Scale Invariant**: Kh√¥ng c·∫ßn normalize scores
- **Robust to Outliers**: Extreme scores kh√¥ng dominate
- **Proven Performance**: Academic validation

---

### D√≤ng 7: Multi-Query Retrieval Video
```markdown
5. https://www.youtube.com/watch?v=JChiEaI4fDo (Multi-query retrieval video)
```

**Gi·∫£i th√≠ch:**
- Video tutorial v·ªÅ Multi-Query Retrieval implementation
- Visual explanation c·ªßa concepts v√† code examples

**N·ªôi dung ch√≠nh:**
1. **Problem Statement**: T·∫°i sao single query kh√¥ng ƒë·ªß
2. **Solution Overview**: Multi-query approach
3. **Implementation**: Step-by-step coding
4. **Evaluation**: So s√°nh performance

**Key Takeaways:**
- **Query Diversity**: Generate diverse query variants
- **Retrieval Strategy**: Different queries find different docs
- **Fusion Importance**: Proper combination is crucial

---

### D√≤ng 8: Trace Example
```markdown
6. https://smith.langchain.com/public/c2ca61b4-3810-45d0-a156-3d6a73e9ee2a/r (Trace example)
```

**Gi·∫£i th√≠ch:**
- Real trace example c·ªßa Multi-Query RAG system
- Shows actual execution flow v√† performance metrics

**Trace Information:**
```json
{
  "run_id": "c2ca61b4-3810-45d0-a156-3d6a73e9ee2a",
  "inputs": {
    "question": "What is the capital of France?"
  },
  "outputs": {
    "generated_queries": [
      "What city is the capital of France?",
      "France capital city name",
      "Which city serves as France's capital?"
    ],
    "retrieved_documents": 12,
    "final_answer": "Paris is the capital of France..."
  },
  "execution_time": "2.3s",
  "token_usage": {
    "prompt_tokens": 1250,
    "completion_tokens": 180
  }
}
```

**Learning Points:**
- **Query Generation Quality**: Xem LLM generate queries nh∆∞ th·∫ø n√†o
- **Retrieval Effectiveness**: S·ªë l∆∞·ª£ng v√† quality c·ªßa retrieved docs
- **Performance Metrics**: Time v√† token usage

---

## üîÑ Multi-Query RAG Workflow

### Step-by-Step Process:

```
1. Original Query
   ‚Üì
2. LLM Query Generation
   "What is Python?" ‚Üí
   - "Define Python programming"
   - "Python language features"  
   - "What makes Python popular?"
   ‚Üì
3. Parallel Retrieval
   Query 1 ‚Üí [Doc1, Doc3, Doc5]
   Query 2 ‚Üí [Doc2, Doc3, Doc7]
   Query 3 ‚Üí [Doc1, Doc4, Doc8]
   ‚Üì
4. Reciprocal Rank Fusion
   Doc1: 1/61 + 0 + 1/61 = 0.0328
   Doc3: 1/62 + 1/62 + 0 = 0.0323
   Doc2: 0 + 1/61 + 0 = 0.0164
   ‚Üì
5. Ranked Final Results
   [Doc1, Doc3, Doc2, Doc4, Doc5, Doc7, Doc8]
   ‚Üì
6. Generate Answer
   Using top-k documents as context
```

---

## üéØ T·ªïng K·∫øt File [2]_sources.md

### Techniques Covered:
1. **Multi-Query Retrieval**: Generate multiple query variants
2. **RAG Fusion**: Combine results using RRF algorithm
3. **Performance Monitoring**: Track v·ªõi LangSmith
4. **Academic Foundation**: Research-backed approaches

### Key Benefits:
- **Improved Recall**: Find more relevant documents
- **Reduced Query Bias**: Multiple perspectives
- **Better Ranking**: RRF provides superior document ordering
- **Robust Performance**: Academic validation

### Implementation Complexity:
- **Medium**: Requires LLM for query generation
- **Cost**: More API calls (3-5x queries)
- **Latency**: Parallel retrieval helps but still slower
- **Quality**: Significant improvement in answer quality

### When to Use:
- **Complex Questions**: Multi-faceted queries
- **Critical Applications**: Where accuracy is paramount
- **Large Knowledge Base**: When single query might miss relevant info
- **User Queries**: Natural language questions that need interpretation

**Next Level**: File [3] s·∫Ω cover **Query Routing** v√† **Query Construction** - techniques ƒë·ªÉ intelligently route queries v√† construct structured queries cho specialized retrievers!
