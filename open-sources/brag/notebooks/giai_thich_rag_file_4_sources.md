# üìö Gi·∫£i Th√≠ch Chi Ti·∫øt File [4]_sources.md - Advanced Indexing & Retrieval

## üéØ M·ª•c ƒê√≠ch File N√†y
File `[4]_sources.md` ch·ª©a ngu·ªìn t√†i li·ªáu cho notebook `[4]_rag_indexing_and_retrieval.ipynb`. ƒê√¢y l√† level 4 trong RAG series, t·∫≠p trung v√†o **Advanced Indexing** v√† **Sophisticated Retrieval Techniques** ƒë·ªÉ optimize document storage v√† retrieval quality.

---

## üìñ Ph√¢n T√≠ch T·ª´ng D√≤ng

### D√≤ng 1: Ti√™u ƒê·ªÅ Ch√≠nh
```markdown
### Here are all the sources used to write-up the `[4]_rag_indexing_and_retrieval.ipynb` file:
```

**Gi·∫£i th√≠ch:**
- File n√†y support notebook level 4 v·ªÅ advanced indexing strategies
- Focus v√†o **sophisticated retrieval methods** v√† **index optimization**

**V·∫•n ƒë·ªÅ c·∫ßn gi·∫£i quy·∫øt:**
- Simple vector search kh√¥ng ƒë·ªß cho complex queries
- Large document collections c·∫ßn efficient indexing
- Different document types require specialized indexing strategies

---

### D√≤ng 3: LangSmith Documentation
```markdown
1. https://docs.smith.langchain.com/ (LangSmith documentation)
```

**Vai tr√≤ trong Advanced Indexing:**
- **Index Performance Monitoring**: Track indexing speed v√† quality
- **Retrieval Analytics**: Analyze retrieval patterns v√† effectiveness
- **A/B Testing**: Compare different indexing strategies

**Advanced Metrics:**
```python
# LangSmith tracking for indexing
{
    "indexing_strategy": "hierarchical_chunking",
    "documents_processed": 10000,
    "indexing_time": "45 minutes",
    "index_size": "2.3 GB",
    "retrieval_performance": {
        "avg_query_time": "0.15s",
        "recall@10": 0.85,
        "precision@10": 0.78
    },
    "chunk_statistics": {
        "avg_chunk_size": 512,
        "overlap_ratio": 0.1,
        "total_chunks": 45000
    }
}
```

---

### D√≤ng 4: Parent Document Retriever
```markdown
2. https://python.langchain.com/docs/how_to/parent_document_retriever/ (Parent document retriever)
```

**Gi·∫£i th√≠ch:**
- **Parent Document Retriever** l√† advanced technique ƒë·ªÉ maintain context hierarchy
- Retrieve small chunks nh∆∞ng return larger parent documents

**Problem v·ªõi Standard Chunking:**
```
Large Document ‚Üí Small Chunks ‚Üí Vector Search ‚Üí Small Chunks
                                                      ‚Üì
                                            Limited Context!
```

**Solution v·ªõi Parent Document Retriever:**
```
Large Document ‚Üí Small Chunks ‚Üí Vector Search ‚Üí Small Chunks
                                                      ‚Üì
                                            Retrieve Parent Document
                                                      ‚Üì
                                            Full Context Available!
```

**Implementation:**
```python
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore

# Setup storage for parent documents
store = InMemoryStore()

# Create parent document retriever
parent_retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=store,
    child_splitter=child_splitter,  # Small chunks for search
    parent_splitter=parent_splitter,  # Larger chunks for context
    k=4
)

# Index documents
parent_retriever.add_documents(documents)

# Retrieve - returns parent documents based on child chunk matches
docs = parent_retriever.get_relevant_documents("query")
```

**Benefits:**
- **Better Context**: Full document context available
- **Precise Search**: Small chunks for accurate matching
- **Flexible Granularity**: Different chunk sizes for different purposes

---

### D√≤ng 5: Self-Query Retriever
```markdown
3. https://python.langchain.com/docs/how_to/self_query/ (Self-query retriever)
```

**Gi·∫£i th√≠ch:**
- **Self-Query Retriever** automatically constructs structured queries from natural language
- Combines semantic search v·ªõi metadata filtering

**Traditional vs Self-Query:**
```
Traditional:
"Find documents about Python from 2023" ‚Üí Vector Search Only
                                        ‚Üì
                                   All Python docs (mixed years)

Self-Query:
"Find documents about Python from 2023" ‚Üí Parse Query
                                        ‚Üì
                        Semantic: "Python programming"
                        Filter: year = 2023
                                        ‚Üì
                        Vector Search + Metadata Filter
                                        ‚Üì
                        Only Python docs from 2023
```

**Implementation:**
```python
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain.chains.query_constructor.base import AttributeInfo

# Define metadata schema
metadata_field_info = [
    AttributeInfo(
        name="year",
        description="The year the document was published", 
        type="integer"
    ),
    AttributeInfo(
        name="author",
        description="The author of the document",
        type="string"
    ),
    AttributeInfo(
        name="topic",
        description="The main topic category",
        type="string"
    )
]

# Create self-query retriever
self_query_retriever = SelfQueryRetriever.from_llm(
    llm=llm,
    vectorstore=vectorstore,
    document_content_description="Technical documentation",
    metadata_field_info=metadata_field_info,
    verbose=True
)

# Use natural language with automatic query construction
docs = self_query_retriever.get_relevant_documents(
    "Show me Python tutorials written by John Doe after 2022"
)
# Automatically creates: semantic_query="Python tutorials" + 
#                       filters={"author": "John Doe", "year": {"$gt": 2022}}
```

---

### D√≤ng 6: Time-Weighted Retriever
```markdown
4. https://python.langchain.com/docs/how_to/time_weighted_retriever/ (Time-weighted retriever)
```

**Gi·∫£i th√≠ch:**
- **Time-Weighted Retriever** considers both relevance v√† recency
- Particularly useful cho dynamic content v√† news-like applications

**Scoring Formula:**
```
Final Score = Semantic Similarity + Time Decay Factor

Time Decay = e^(-Œª * time_since_last_access)

Where:
- Œª = decay constant (higher = faster decay)
- time_since_last_access = hours/days since document was accessed
```

**Implementation:**
```python
from langchain.retrievers import TimeWeightedVectorStoreRetriever

# Create time-weighted retriever
time_retriever = TimeWeightedVectorStoreRetriever(
    vectorstore=vectorstore,
    decay_rate=-0.01,  # Decay rate per hour
    k=4
)

# Add documents with timestamps
time_retriever.add_documents([
    Document(page_content="Recent news about AI", metadata={"timestamp": "2024-01-15"}),
    Document(page_content="Old news about AI", metadata={"timestamp": "2023-01-15"})
])

# Recent documents get higher scores
docs = time_retriever.get_relevant_documents("AI news")
```

**Use Cases:**
- **News Retrieval**: Recent articles more relevant
- **Documentation**: Latest versions preferred
- **Social Media**: Recent posts prioritized
- **Research**: Current papers over old ones

---

### D√≤ng 7: Multi-Vector Retriever
```markdown
5. https://python.langchain.com/docs/how_to/multi_vector/ (Multi-vector retriever)
```

**Gi·∫£i th√≠ch:**
- **Multi-Vector Retriever** creates multiple vector representations per document
- Enables more sophisticated matching strategies

**Multiple Vector Strategies:**

1. **Different Chunk Sizes**:
```python
# Create vectors for different granularities
small_chunks = split_document(doc, chunk_size=200)
medium_chunks = split_document(doc, chunk_size=500) 
large_chunks = split_document(doc, chunk_size=1000)

# Index all chunk sizes
for chunks in [small_chunks, medium_chunks, large_chunks]:
    vectorstore.add_documents(chunks)
```

2. **Different Representations**:
```python
# Create multiple representations of same content
representations = [
    doc.page_content,  # Original text
    summarize(doc.page_content),  # Summary
    extract_keywords(doc.page_content),  # Keywords
    generate_questions(doc.page_content)  # Potential questions
]

# Index all representations
for rep in representations:
    vectorstore.add_texts([rep], metadatas=[{"doc_id": doc.id, "type": rep_type}])
```

3. **Different Embedding Models**:
```python
# Use multiple embedding models
embeddings_1 = OpenAIEmbeddings()
embeddings_2 = HuggingFaceEmbeddings()

# Create separate vector stores
vs1 = FAISS.from_documents(docs, embeddings_1)
vs2 = FAISS.from_documents(docs, embeddings_2)

# Combine results from both
results_1 = vs1.similarity_search(query)
results_2 = vs2.similarity_search(query)
combined_results = merge_and_rank(results_1, results_2)
```

---

### D√≤ng 8: Ensemble Retriever
```markdown
6. https://python.langchain.com/docs/how_to/ensemble/ (Ensemble retriever)
```

**Gi·∫£i th√≠ch:**
- **Ensemble Retriever** combines multiple retrieval methods
- Uses **Reciprocal Rank Fusion** ƒë·ªÉ merge results

**Ensemble Strategies:**

1. **Different Retrieval Methods**:
```python
from langchain.retrievers import EnsembleRetriever

# Combine different retrieval approaches
bm25_retriever = BM25Retriever.from_documents(documents)
vector_retriever = vectorstore.as_retriever()

# Create ensemble
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[0.4, 0.6]  # Weight vector search higher
)

# Get combined results
docs = ensemble_retriever.get_relevant_documents("query")
```

2. **Different Vector Stores**:
```python
# Combine different vector databases
faiss_retriever = faiss_vectorstore.as_retriever()
pinecone_retriever = pinecone_vectorstore.as_retriever()
chroma_retriever = chroma_vectorstore.as_retriever()

ensemble = EnsembleRetriever(
    retrievers=[faiss_retriever, pinecone_retriever, chroma_retriever],
    weights=[0.33, 0.33, 0.34]
)
```

**Benefits:**
- **Robustness**: Multiple methods reduce single-point failures
- **Complementary Strengths**: BM25 + Vector search cover different aspects
- **Improved Recall**: More comprehensive document retrieval

---

### D√≤ng 9: Contextual Compression
```markdown
7. https://python.langchain.com/docs/how_to/contextual_compression/ (Contextual compression)
```

**Gi·∫£i th√≠ch:**
- **Contextual Compression** filters v√† compresses retrieved documents
- Removes irrelevant parts v√† keeps only query-relevant content

**Compression Pipeline:**
```
Retrieved Documents ‚Üí Relevance Filter ‚Üí Content Compression ‚Üí Compressed Results
        ‚Üì                    ‚Üì                    ‚Üì                    ‚Üì
   [Doc1, Doc2, Doc3] ‚Üí [Doc1, Doc3] ‚Üí [Relevant_Parts] ‚Üí Final_Context
```

**Implementation:**
```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

# Create compressor
compressor = LLMChainExtractor.from_llm(llm)

# Wrap base retriever with compression
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=base_retriever
)

# Get compressed, relevant content only
compressed_docs = compression_retriever.get_relevant_documents(
    "What are the benefits of machine learning?"
)
# Returns only ML-relevant parts from retrieved documents
```

**Compression Types:**

1. **Relevance Filtering**:
```python
from langchain.retrievers.document_compressors import LLMChainFilter

filter_compressor = LLMChainFilter.from_llm(llm)
# Removes completely irrelevant documents
```

2. **Content Extraction**:
```python
from langchain.retrievers.document_compressors import LLMChainExtractor

extractor = LLMChainExtractor.from_llm(llm)
# Extracts only relevant sentences/paragraphs
```

3. **Embedding-based Filtering**:
```python
from langchain.retrievers.document_compressors import EmbeddingsFilter

embeddings_filter = EmbeddingsFilter(
    embeddings=embeddings,
    similarity_threshold=0.76
)
# Filters based on embedding similarity
```

---

## üîÑ Advanced Indexing & Retrieval Workflow

### Complete Advanced Pipeline:

```
1. Document Ingestion
   Raw Documents
   ‚Üì
2. Multi-Level Chunking
   - Small chunks (search)
   - Large chunks (context)
   - Summaries (overview)
   ‚Üì
3. Multi-Vector Indexing
   - Different embeddings
   - Different representations
   - Metadata extraction
   ‚Üì
4. Intelligent Retrieval
   - Self-query parsing
   - Time-weighted scoring
   - Ensemble methods
   ‚Üì
5. Contextual Compression
   - Relevance filtering
   - Content extraction
   - Final optimization
   ‚Üì
6. Optimized Results
   High-quality, relevant context
```

---

## üéØ T·ªïng K·∫øt File [4]_sources.md

### Advanced Techniques:
1. **Parent Document Retriever**: Hierarchical context management
2. **Self-Query Retriever**: Automatic query construction
3. **Time-Weighted Retriever**: Recency-aware retrieval
4. **Multi-Vector Retriever**: Multiple representations
5. **Ensemble Retriever**: Combined retrieval methods
6. **Contextual Compression**: Intelligent content filtering

### Key Benefits:
- **Better Context**: Hierarchical document structure
- **Precise Filtering**: Automatic metadata queries
- **Temporal Relevance**: Time-aware scoring
- **Comprehensive Coverage**: Multiple retrieval strategies
- **Optimized Content**: Compressed, relevant results

### Implementation Complexity:
- **High**: Multiple sophisticated components
- **Resource Intensive**: More storage v√† compute
- **Configuration Heavy**: Many parameters to tune
- **Monitoring Critical**: Complex pipeline needs tracking

### When to Use:
- **Large Scale Systems**: Millions of documents
- **Complex Queries**: Multi-faceted information needs
- **Dynamic Content**: Time-sensitive information
- **High Accuracy Requirements**: Critical applications
- **Diverse Document Types**: Mixed content formats

**Final Level**: File [5] s·∫Ω cover **Retrieval & Reranking** - the ultimate techniques ƒë·ªÉ achieve maximum retrieval precision through advanced reranking methods!
