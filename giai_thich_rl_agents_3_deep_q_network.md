# üß† Gi·∫£i Th√≠ch Chi Ti·∫øt: Deep Q-Network (DQN) - Deep Reinforcement Learning Breakthrough

## üìö T·ªïng Quan Deep Q-Network

**Deep Q-Network (DQN)** l√† thu·∫≠t to√°n breakthrough ƒë·∫ßu ti√™n th√†nh c√¥ng trong vi·ªác combine **Deep Learning** v·ªõi **Q-Learning**. ƒê∆∞·ª£c ph√°t tri·ªÉn b·ªüi DeepMind nƒÉm 2015, DQN ƒë√£ ch·ª©ng minh kh·∫£ nƒÉng h·ªçc ch∆°i Atari games ·ªü m·ª©c superhuman.

---

## üéØ V·∫•n ƒê·ªÅ C·∫ßn Gi·∫£i Quy·∫øt

### Traditional Q-Learning Limitations:

**Tabular Q-Learning:**
```
State Space: Discrete v√† small
Q-Table: Q(s,a) for every (state, action) pair
Problem: Exponential growth v·ªõi large state spaces
```

**Function Approximation Challenge:**
```
Large State Space ‚Üí Need function approximation
Neural Networks ‚Üí Unstable training
Issues: 
- Non-stationary targets
- Correlated samples
- Catastrophic forgetting
```

### DQN Solutions:

1. **Experience Replay**: Break correlation between consecutive samples
2. **Target Network**: Stabilize learning targets
3. **Deep Neural Networks**: Handle high-dimensional state spaces

---

## üèóÔ∏è Ki·∫øn Tr√∫c DQN

### Core Components:

```python
class DQNAgent(AbstractDQNAgent):
    def __init__(self, env, config=None):
        # 1. Main Q-Network (Policy Network)
        self.value_net = model_factory(self.config["model"])
        
        # 2. Target Q-Network (Stable targets)
        self.target_net = model_factory(self.config["model"])
        self.target_net.load_state_dict(self.value_net.state_dict())
        
        # 3. Experience Replay Buffer
        self.memory = ReplayBuffer(capacity=10000)
        
        # 4. Optimizer v√† Loss Function
        self.optimizer = optimizer_factory(...)
        self.loss_function = loss_function_factory(...)
```

---

## üìñ Ph√¢n T√≠ch T·ª´ng D√≤ng Code

### D√≤ng 14-30: Initialization
```python
class DQNAgent(AbstractDQNAgent):
    def __init__(self, env, config=None):
        super(DQNAgent, self).__init__(env, config)
        
        # Configure model size based on environment
        size_model_config(self.env, self.config["model"])
        
        # Create main Q-network
        self.value_net = model_factory(self.config["model"])
        
        # Create target network (copy of main network)
        self.target_net = model_factory(self.config["model"])
        self.target_net.load_state_dict(self.value_net.state_dict())
        self.target_net.eval()  # Set to evaluation mode
        
        # Setup device (CPU/GPU)
        self.device = choose_device(self.config["device"])
        self.value_net.to(self.device)
        self.target_net.to(self.device)
        
        # Setup loss function v√† optimizer
        self.loss_function = loss_function_factory(self.config["loss_function"])
        self.optimizer = optimizer_factory(self.config["optimizer"]["type"],
                                           self.value_net.parameters(),
                                           **self.config["optimizer"])
```

**Key Points:**
- **Dual Networks**: Main network cho learning, target network cho stable targets
- **Device Management**: Automatic GPU/CPU selection
- **Factory Pattern**: Flexible model v√† optimizer creation

### D√≤ng 32-38: Gradient Optimization
```python
def step_optimizer(self, loss):
    # Clear previous gradients
    self.optimizer.zero_grad()
    
    # Compute gradients
    loss.backward()
    
    # Gradient clipping (prevent exploding gradients)
    for param in self.value_net.parameters():
        param.grad.data.clamp_(-1, 1)
    
    # Update parameters
    self.optimizer.step()
```

**Gi·∫£i th√≠ch:**
- **Gradient Clipping**: Prevent exploding gradients (clamp between -1 v√† 1)
- **Standard Optimization**: Zero gradients ‚Üí Backward pass ‚Üí Parameter update
- **Stability**: Gradient clipping crucial cho stable DQN training

### D√≤ng 40-73: Bellman Residual Computation (Core DQN Logic)
```python
def compute_bellman_residual(self, batch, target_state_action_value=None):
    # Convert batch to tensors if needed
    if not isinstance(batch.state, torch.Tensor):
        state = torch.cat(tuple(torch.tensor([batch.state], dtype=torch.float))).to(self.device)
        action = torch.tensor(batch.action, dtype=torch.long).to(self.device)
        reward = torch.tensor(batch.reward, dtype=torch.float).to(self.device)
        next_state = torch.cat(tuple(torch.tensor([batch.next_state], dtype=torch.float))).to(self.device)
        terminal = torch.tensor(batch.terminal, dtype=torch.bool).to(self.device)
        batch = Transition(state, action, reward, next_state, terminal, batch.info)

    # Compute Q(s_t, a_t) - current state-action values
    state_action_values = self.value_net(batch.state)
    state_action_values = state_action_values.gather(1, batch.action.unsqueeze(1)).squeeze(1)

    if target_state_action_value is None:
        with torch.no_grad():  # No gradients for target computation
            # Compute target values
            next_state_values = torch.zeros(batch.reward.shape).to(self.device)
            
            if self.config["double"]:
                # Double DQN: Use main network to select actions
                _, best_actions = self.value_net(batch.next_state).max(1)
                # Use target network to evaluate selected actions
                best_values = self.target_net(batch.next_state).gather(1, best_actions.unsqueeze(1)).squeeze(1)
            else:
                # Standard DQN: Use target network for both selection v√† evaluation
                best_values, _ = self.target_net(batch.next_state).max(1)
            
            # Only update non-terminal states
            next_state_values[~batch.terminal] = best_values[~batch.terminal]
            
            # Bellman equation: Q_target = r + Œ≥ * max_a Q_target(s', a')
            target_state_action_value = batch.reward + self.config["gamma"] * next_state_values

    # Compute loss between current Q-values v√† targets
    loss = self.loss_function(state_action_values, target_state_action_value)
    return loss, target_state_action_value, batch
```

**Core DQN Algorithm:**

1. **Current Q-Values**: `Q(s_t, a_t)` t·ª´ main network
2. **Target Q-Values**: `r + Œ≥ * max_a Q_target(s_{t+1}, a)` t·ª´ target network
3. **Loss Computation**: MSE between current v√† target Q-values
4. **Double DQN**: Optional improvement ƒë·ªÉ reduce overestimation bias

### D√≤ng 60-67: Double DQN Implementation
```python
if self.config["double"]:
    # Double Q-learning: pick best actions from policy network
    _, best_actions = self.value_net(batch.next_state).max(1)
    # Double Q-learning: estimate action values from target network
    best_values = self.target_net(batch.next_state).gather(1, best_actions.unsqueeze(1)).squeeze(1)
else:
    # Standard DQN
    best_values, _ = self.target_net(batch.next_state).max(1)
```

**Double DQN vs Standard DQN:**

**Standard DQN:**
```
Target = r + Œ≥ * max_a Q_target(s', a)
Problem: Overestimation bias (same network selects v√† evaluates)
```

**Double DQN:**
```
Action Selection: a* = argmax_a Q_main(s', a)
Action Evaluation: Target = r + Œ≥ * Q_target(s', a*)
Benefit: Reduces overestimation bias
```

### D√≤ng 75-81: Batch Processing
```python
def get_batch_state_values(self, states):
    values, actions = self.value_net(torch.tensor(states, dtype=torch.float).to(self.device)).max(1)
    return values.data.cpu().numpy(), actions.data.cpu().numpy()

def get_batch_state_action_values(self, states):
    return self.value_net(torch.tensor(states, dtype=torch.float).to(self.device)).data.cpu().numpy()
```

**Gi·∫£i th√≠ch:**
- **Batch Inference**: Process multiple states simultaneously
- **Device Management**: Move tensors to appropriate device
- **Return Format**: Convert back to numpy arrays cho compatibility

### D√≤ng 82-93: Model Persistence
```python
def save(self, filename):
    state = {'state_dict': self.value_net.state_dict(),
             'optimizer': self.optimizer.state_dict()}
    torch.save(state, filename)
    return filename

def load(self, filename):
    checkpoint = torch.load(filename, map_location=self.device)
    self.value_net.load_state_dict(checkpoint['state_dict'])
    self.target_net.load_state_dict(checkpoint['state_dict'])  # Sync target network
    self.optimizer.load_state_dict(checkpoint['optimizer'])
    return filename
```

**Gi·∫£i th√≠ch:**
- **Complete State**: Save both network parameters v√† optimizer state
- **Target Network Sync**: Load same weights into target network
- **Device Mapping**: Handle device differences when loading

---

## üîÑ DQN Training Workflow

### Complete Training Process:

```
1. Environment Interaction
   State s ‚Üí Œµ-greedy Policy ‚Üí Action a
                    ‚Üì
   Environment ‚Üí Reward r, Next State s'
                    ‚Üì
2. Experience Storage
   Store (s, a, r, s', done) in Replay Buffer
                    ‚Üì
3. Batch Learning (every N steps)
   Sample batch from Replay Buffer
                    ‚Üì
4. Q-Value Computation
   Current Q: Q_main(s, a)
   Target Q: r + Œ≥ * max_a Q_target(s', a)
                    ‚Üì
5. Loss Computation v√† Backpropagation
   Loss = MSE(Current Q, Target Q)
   Update main network parameters
                    ‚Üì
6. Target Network Update (every M steps)
   Q_target ‚Üê Q_main (hard update)
   or Q_target ‚Üê œÑ*Q_main + (1-œÑ)*Q_target (soft update)
                    ‚Üì
7. Repeat until convergence
```

---

## üéØ Key Innovations c·ªßa DQN

### 1. **Experience Replay**
```python
# Break correlation between consecutive samples
class ReplayBuffer:
    def sample(self, batch_size):
        # Random sampling breaks temporal correlation
        batch = random.sample(self.buffer, batch_size)
        return batch

# Benefits:
# - Data efficiency: Reuse past experiences
# - Stability: Break correlation between samples
# - Sample efficiency: Learn from diverse experiences
```

### 2. **Target Network**
```python
# Stable learning targets
def update_target_network(self):
    # Hard update (every C steps)
    self.target_net.load_state_dict(self.value_net.state_dict())
    
    # Or soft update (every step)
    # œÑ = 0.001
    # for target_param, main_param in zip(self.target_net.parameters(), self.value_net.parameters()):
    #     target_param.data.copy_(œÑ * main_param.data + (1 - œÑ) * target_param.data)
```

### 3. **Deep Neural Networks**
```python
# Handle high-dimensional state spaces
class DQNNetwork(nn.Module):
    def __init__(self, state_size, action_size, hidden_sizes=[512, 512]):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(state_size, hidden_sizes[0]),
            nn.ReLU(),
            nn.Linear(hidden_sizes[0], hidden_sizes[1]),
            nn.ReLU(),
            nn.Linear(hidden_sizes[1], action_size)
        )
    
    def forward(self, state):
        return self.network(state)
```

---

## üìä DQN Variants v√† Improvements

### **Double DQN (DDQN)**
- **Problem**: Overestimation bias trong standard DQN
- **Solution**: Decouple action selection t·ª´ action evaluation
- **Implementation**: Use main network cho selection, target network cho evaluation

### **Dueling DQN**
- **Innovation**: Separate value v√† advantage streams
- **Architecture**: V(s) + A(s,a) - mean(A(s,:))
- **Benefit**: Better value estimation, especially cho states where action choice doesn't matter

### **Prioritized Experience Replay**
- **Innovation**: Sample important transitions more frequently
- **Priority**: Based on TD-error magnitude
- **Benefit**: Faster learning from important experiences

### **Rainbow DQN**
- **Combination**: Multiple improvements together
- **Components**: Double DQN + Dueling + Prioritized Replay + Multi-step + Distributional + Noisy Networks

---

## üí° ∆Øu ƒêi·ªÉm v√† Nh∆∞·ª£c ƒêi·ªÉm

### ‚úÖ **∆Øu ƒêi·ªÉm:**
- **High-Dimensional States**: Handle complex observations (images, etc.)
- **Model-Free**: No need for environment model
- **Sample Efficiency**: Experience replay improves data usage
- **Stability**: Target network stabilizes training
- **Proven Performance**: Superhuman performance on Atari games

### ‚ùå **Nh∆∞·ª£c ƒêi·ªÉm:**
- **Sample Complexity**: Still requires many environment interactions
- **Hyperparameter Sensitivity**: Many parameters to tune
- **Discrete Actions Only**: Cannot handle continuous action spaces directly
- **Overestimation Bias**: Tendency to overestimate Q-values
- **Exploration Challenge**: Œµ-greedy exploration can be inefficient

---

## üöÄ Use Cases v√† Applications

### **Ideal Applications:**
1. **Game Playing**: Atari games, board games
2. **Robotics**: Discrete control tasks
3. **Resource Management**: Scheduling, allocation problems
4. **Trading**: Discrete trading decisions
5. **Navigation**: Grid-world navigation

### **Environment Requirements:**
```python
# DQN requires:
# 1. Discrete action space
assert isinstance(env.action_space, gym.spaces.Discrete)

# 2. Bounded observation space
assert isinstance(env.observation_space, gym.spaces.Box)

# 3. Reward signal
# 4. Episode termination conditions
```

---

## üîß Configuration Best Practices

### **Key Hyperparameters:**
```python
dqn_config = {
    "model": {
        "type": "MultiLayerPerceptron",
        "layers": [512, 512],  # Hidden layer sizes
        "activation": "RELU"
    },
    "optimizer": {
        "type": "ADAM",
        "learning_rate": 1e-4,  # Lower LR cho stability
        "weight_decay": 1e-5
    },
    "gamma": 0.99,              # Discount factor
    "batch_size": 32,           # Batch size cho training
    "memory_capacity": 100000,   # Replay buffer size
    "target_update": 1000,      # Target network update frequency
    "double": True,             # Enable Double DQN
    "exploration": {
        "method": "EpsilonGreedy",
        "epsilon": 1.0,         # Initial exploration
        "final_epsilon": 0.01,  # Final exploration
        "decay_steps": 100000   # Exploration decay
    }
}
```

### **Training Tips:**
- **Learning Rate**: Start v·ªõi 1e-4, adjust based on loss stability
- **Batch Size**: 32-128 works well cho most problems
- **Target Update**: Every 1000-10000 steps
- **Exploration**: Long decay period cho complex environments
- **Memory Size**: Larger buffer cho better sample diversity

**Next Algorithm**: Ch√∫ng ta s·∫Ω ti·∫øp t·ª•c v·ªõi **Dynamic Programming** - classical planning methods trong RL!
