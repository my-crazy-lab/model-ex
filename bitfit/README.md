# âš¡ BitFit: Bias-only Fine-tuning Implementation

This project implements BitFit (Bias-only Fine-tuning) based on the checklist in `fine-tuning/BitFit.md`.

## ğŸ“‹ What is BitFit?

BitFit is an extremely parameter-efficient fine-tuning method that:
- **Only trains bias parameters** while freezing all other weights
- Achieves competitive performance with minimal parameter updates
- Requires only 0.08-0.1% of total parameters to be trainable
- Provides fast training and inference with minimal memory overhead

## ğŸ—ï¸ Architecture

```
Pre-trained Model (110M parameters)
    â†“
Freeze All Weights (109.9M parameters frozen)
    â†“
Train Only Bias Terms (0.1M parameters trainable)
    â†“
Fine-tuned Model (99.9% parameter reduction!)
```

### BitFit vs Other Methods

| Method | Trainable Parameters | Performance | Memory | Speed |
|--------|---------------------|-------------|---------|-------|
| Full Fine-tuning | 100% | 100% | High | Slow |
| Adapter | ~1% | 95-98% | Medium | Medium |
| LoRA | ~0.5% | 96-99% | Medium | Medium |
| **BitFit** | **~0.1%** | **90-95%** | **Low** | **Fast** |

## ğŸ“ Project Structure

```
bitfit/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ setup.py                    # Package setup
â”œâ”€â”€ config/                     # Configuration files
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_config.py         # Model configurations
â”‚   â”œâ”€â”€ bitfit_config.py        # BitFit-specific configs
â”‚   â””â”€â”€ training_config.py      # Training configurations
â”œâ”€â”€ bitfit/                     # Core BitFit implementation
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ bitfit_model.py         # BitFit model wrapper
â”‚   â”œâ”€â”€ parameter_utils.py      # Parameter freezing utilities
â”‚   â””â”€â”€ bias_optimizer.py       # Bias-only optimizer
â”œâ”€â”€ data/                       # Data processing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py          # Data loading utilities
â”‚   â””â”€â”€ preprocessing.py        # Data preprocessing
â”œâ”€â”€ training/                   # Training scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ bitfit_trainer.py       # BitFit trainer
â”‚   â”œâ”€â”€ evaluation.py           # Evaluation utilities
â”‚   â””â”€â”€ callbacks.py            # Training callbacks
â”œâ”€â”€ inference/                  # Inference scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ bitfit_pipeline.py      # Inference pipeline
â”œâ”€â”€ experiments/                # Experiment tracking
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ experiment_manager.py   # Experiment management
â”‚   â””â”€â”€ comparison.py           # Method comparison
â”œâ”€â”€ examples/                   # Example scripts
â”‚   â”œâ”€â”€ text_classification.py
â”‚   â”œâ”€â”€ sentiment_analysis.py
â”‚   â””â”€â”€ glue_benchmark.py
â””â”€â”€ notebooks/                  # Jupyter notebooks
    â”œâ”€â”€ 01_bitfit_basics.ipynb
    â”œâ”€â”€ 02_parameter_analysis.ipynb
    â””â”€â”€ 03_performance_comparison.ipynb
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Install the package
pip install -e .
```

### 2. Basic BitFit Fine-tuning

```python
from bitfit import BitFitModel, BitFitTrainer
from config import ModelConfig, BitFitConfig, TrainingConfig

# Setup configurations
model_config = ModelConfig(
    model_name_or_path="bert-base-uncased",
    num_labels=2,
    task_type="classification"
)

bitfit_config = BitFitConfig(
    freeze_all_weights=True,
    train_bias_only=True,
    bias_learning_rate=1e-3
)

training_config = TrainingConfig(
    output_dir="./bitfit_results",
    num_train_epochs=3,
    per_device_train_batch_size=32,
    learning_rate=1e-3
)

# Create BitFit model
model = BitFitModel(model_config, bitfit_config)

# Train model
trainer = BitFitTrainer(model, training_config)
trainer.train(train_dataset, eval_dataset)
```

### 3. Inference

```python
from inference import BitFitPipeline

# Load trained model
pipeline = BitFitPipeline.from_pretrained("./bitfit_results")

# Make predictions
result = pipeline.predict("This movie is great!")
print(f"Prediction: {result}")
```

## ğŸ”§ Key Features

### âœ… Extreme Parameter Efficiency
- **Only bias parameters are trainable** (0.08-0.1% of total)
- Automatic parameter freezing and bias identification
- Memory-efficient training and inference

### âœ… Fast Training
- **Minimal gradient computation** for bias terms only
- Reduced memory footprint during training
- Quick convergence due to focused parameter updates

### âœ… Easy Integration
- **Drop-in replacement** for standard fine-tuning
- Compatible with any transformer architecture
- Seamless integration with Hugging Face models

### âœ… Comprehensive Analysis
- **Parameter counting** and efficiency metrics
- Performance comparison with other methods
- Detailed experiment tracking and logging

## ğŸ“Š Supported Tasks

### Text Classification
- Sentiment Analysis (SST-2, IMDB)
- Topic Classification (AG News)
- Intent Detection

### Natural Language Inference
- MNLI, SNLI, RTE
- Textual Entailment

### Token Classification
- Named Entity Recognition (CoNLL-2003)
- Part-of-Speech Tagging

### Question Answering
- SQuAD 1.1/2.0
- Reading Comprehension

## ğŸ§  BitFit Principles

### 1. Bias-only Training
```python
# Only bias parameters are trainable
for name, param in model.named_parameters():
    if 'bias' in name:
        param.requires_grad = True
    else:
        param.requires_grad = False
```

### 2. Minimal Parameter Updates
```python
# Count trainable parameters
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
efficiency = trainable_params / total_params * 100
print(f"Parameter efficiency: {efficiency:.2f}%")
```

### 3. Task-specific Adaptation
```python
# Bias terms adapt to task-specific patterns
# while preserving pre-trained representations
```

## ğŸ“ˆ Performance Benefits

### Parameter Efficiency
```
BERT-base: 110M parameters
BitFit trainable: 0.1M parameters (0.09% of total)
Storage: 0.4MB vs 440MB (1100x reduction)
```

### Training Speed
```
Full Fine-tuning: 100% computation
BitFit: ~10% computation (bias gradients only)
Training time: 5-10x faster
```

### Memory Usage
```
Full Fine-tuning: High memory for all gradients
BitFit: Low memory for bias gradients only
Memory reduction: 80-90%
```

## ğŸ”¬ Advanced Features

### Selective Bias Training
```python
# Train only specific bias types
bitfit_config = BitFitConfig(
    train_attention_bias=True,
    train_feedforward_bias=True,
    train_layer_norm_bias=False,
    train_classifier_bias=True
)
```

### Learning Rate Scheduling
```python
# Different learning rates for different bias types
bias_optimizer = BiasOptimizer(
    attention_lr=1e-3,
    feedforward_lr=5e-4,
    classifier_lr=2e-3
)
```

### Gradient Clipping
```python
# Specialized gradient clipping for bias terms
training_config = TrainingConfig(
    bias_gradient_clipping=True,
    max_bias_grad_norm=1.0
)
```

## ğŸ“– Documentation

See the `notebooks/` directory for detailed tutorials:
1. **BitFit Basics**: Understanding bias-only fine-tuning
2. **Parameter Analysis**: Analyzing parameter efficiency
3. **Performance Comparison**: Comparing with other methods

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Implement your changes
4. Add tests and documentation
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- [BitFit Paper](https://arxiv.org/abs/2106.10199) for the original method
- [Hugging Face](https://huggingface.co/) for transformer implementations
- [Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2106.04647) survey
