# üß† H∆∞·ªõng D·∫´n Implement Lifelong Learning T·ª´ S·ªë 0

H∆∞·ªõng d·∫´n n√†y s·∫Ω gi√∫p b·∫°n hi·ªÉu v√† x√¢y d·ª±ng l·∫°i to√†n b·ªô h·ªá th·ªëng Lifelong Learning t·ª´ ƒë·∫ßu, t·ª´ng b∆∞·ªõc m·ªôt.

## üìö Ki·∫øn Th·ª©c C·∫ßn C√≥ Tr∆∞·ªõc

### 1. Continual Learning Fundamentals
- Catastrophic forgetting v√† t·∫°i sao x·∫£y ra
- Task-incremental vs class-incremental learning
- Forward transfer vs backward transfer

### 2. Neural Network Training
- Gradient descent v√† parameter updates
- Regularization techniques
- Memory v√† computational constraints

### 3. Advanced ML Concepts
- Fisher Information Matrix
- Bayesian neural networks
- Meta-learning principles

---

## üéØ Lifelong Learning L√† G√¨?

### V·∫•n ƒê·ªÅ: Catastrophic Forgetting
```
Task 1: Sentiment Analysis (90% accuracy)
‚Üì Train on Task 2: Topic Classification
Task 1: Sentiment Analysis (45% accuracy) ‚Üê CATASTROPHIC FORGETTING!
Task 2: Topic Classification (88% accuracy)

V·∫•n ƒë·ªÅ: Model qu√™n ho√†n to√†n Task 1 khi h·ªçc Task 2
```

### Gi·∫£i Ph√°p: Lifelong Learning
```
Task 1: Sentiment ‚Üí Model learns + preserves knowledge
‚Üì
Task 2: Topic ‚Üí Model learns new + retains old knowledge  
‚Üì
Task 3: NER ‚Üí Model learns new + retains all previous
‚Üì
Task N: ... ‚Üí Model becomes increasingly capable

K·∫øt qu·∫£: Model c√≥ th·ªÉ handle t·∫•t c·∫£ tasks ƒë√£ h·ªçc!
```

### C√°c Ph∆∞∆°ng Ph√°p Ch√≠nh
```
1. Regularization-based:
   - EWC (Elastic Weight Consolidation)
   - L2 Regularization
   - Synaptic Intelligence

2. Memory-based:
   - Experience Replay
   - Gradient Episodic Memory
   - Memory-Augmented Networks

3. Architecture-based:
   - Progressive Neural Networks
   - PackNet
   - Dynamic architectures
```

---

## üèóÔ∏è B∆∞·ªõc 1: Hi·ªÉu Ki·∫øn Tr√∫c Lifelong Learning

### T·∫°i Sao Catastrophic Forgetting X·∫£y Ra?
```python
# Neural network parameters
Œ∏ = [w1, w2, w3, ..., wn]

# Task 1 training: Œ∏ ‚Üí Œ∏‚ÇÅ (optimal for Task 1)
# Task 2 training: Œ∏‚ÇÅ ‚Üí Œ∏‚ÇÇ (optimal for Task 2, bad for Task 1)

# V·∫•n ƒë·ªÅ: Œ∏‚ÇÇ overwrite Œ∏‚ÇÅ completely!
```

### Lifelong Learning Strategy
```python
# Thay v√¨ overwrite, ta preserve important weights:
# Task 1: Œ∏ ‚Üí Œ∏‚ÇÅ + identify important weights
# Task 2: Œ∏‚ÇÅ ‚Üí Œ∏‚ÇÇ while protecting important weights from Task 1
# Task 3: Œ∏‚ÇÇ ‚Üí Œ∏‚ÇÉ while protecting important weights from Task 1&2
```

---

## üîß B∆∞·ªõc 2: Implement Elastic Weight Consolidation (EWC)

### 2.1 T·∫°i Sao EWC Hi·ªáu Qu·∫£?

```python
# EWC Idea: Protect important weights using Fisher Information
# Fisher Information = How much loss changes when weight changes

# Important weight: High Fisher ‚Üí Protect strongly
# Unimportant weight: Low Fisher ‚Üí Allow changes
```

### 2.2 T·∫°o `techniques/ewc.py`

```python
"""
Elastic Weight Consolidation - Tr√°i tim c·ªßa regularization-based methods
"""
import torch
import torch.nn as nn
import torch.nn.functional as F

class FisherInformationMatrix:
    """Compute Fisher Information Matrix"""
    
    def __init__(self, model, num_samples=1000):
        self.model = model
        self.num_samples = num_samples
        self.fisher_dict = {}
    
    def compute_fisher_information(self, dataloader, task_id):
        """
        Compute Fisher Information for task
        
        Fisher Information = E[‚àálog p(y|x)¬≤]
        Measures how much loss changes when parameters change
        """
        self.model.eval()
        fisher_dict = {}
        
        # Initialize Fisher information
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                fisher_dict[name] = torch.zeros_like(param)
        
        num_samples_processed = 0
        
        for batch in dataloader:
            if num_samples_processed >= self.num_samples:
                break
            
            # Forward pass
            self.model.zero_grad()
            outputs = self.model(**batch)
            
            # Sample from model's prediction (key insight!)
            probs = F.softmax(outputs.logits, dim=-1)
            sampled_labels = torch.multinomial(probs, 1).squeeze()
            
            # Compute loss with sampled labels
            loss = F.cross_entropy(outputs.logits, sampled_labels)
            
            # Backward pass
            loss.backward()
            
            # Accumulate Fisher information = gradient¬≤
            for name, param in self.model.named_parameters():
                if param.requires_grad and param.grad is not None:
                    fisher_dict[name] += param.grad.data ** 2
            
            num_samples_processed += batch['input_ids'].size(0)
        
        # Normalize by number of samples
        for name in fisher_dict:
            fisher_dict[name] /= num_samples_processed
        
        self.fisher_dict[task_id] = fisher_dict
        return fisher_dict

class ElasticWeightConsolidation:
    """EWC implementation"""
    
    def __init__(self, model, ewc_lambda=1000.0):
        self.model = model
        self.ewc_lambda = ewc_lambda
        
        # Fisher information computer
        self.fisher_computer = FisherInformationMatrix(model)
        
        # Store optimal parameters for each task
        self.optimal_params = {}
        self.consolidated_fisher = {}
        self.completed_tasks = []
    
    def register_task(self, task_id, dataloader):
        """Register new task and compute Fisher information"""
        print(f"üîß Registering task {task_id} for EWC...")
        
        # Store current optimal parameters
        self.optimal_params[task_id] = {}
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.optimal_params[task_id][name] = param.data.clone()
        
        # Compute Fisher information
        fisher_info = self.fisher_computer.compute_fisher_information(
            dataloader, task_id
        )
        
        # Update consolidated Fisher information
        if not self.consolidated_fisher:
            self.consolidated_fisher = fisher_info.copy()
        else:
            # Add Fisher information from new task
            for name in fisher_info:
                if name in self.consolidated_fisher:
                    self.consolidated_fisher[name] += fisher_info[name]
                else:
                    self.consolidated_fisher[name] = fisher_info[name]
        
        self.completed_tasks.append(task_id)
        print(f"‚úÖ Task {task_id} registered with EWC")
    
    def compute_ewc_loss(self):
        """
        Compute EWC regularization loss
        
        EWC Loss = Œª * Œ£ F_i * (Œ∏_i - Œ∏*_i)¬≤
        Where:
        - F_i: Fisher information for parameter i
        - Œ∏_i: Current parameter value
        - Œ∏*_i: Optimal parameter value from previous tasks
        - Œª: EWC strength
        """
        if not self.completed_tasks or not self.consolidated_fisher:
            return torch.tensor(0.0, device=next(self.model.parameters()).device)
        
        ewc_loss = torch.tensor(0.0, device=next(self.model.parameters()).device)
        
        for name, param in self.model.named_parameters():
            if not param.requires_grad or name not in self.consolidated_fisher:
                continue
            
            # Get Fisher information
            fisher = self.consolidated_fisher[name]
            
            # Get optimal parameters from most recent task
            recent_task = self.completed_tasks[-1]
            optimal_param = self.optimal_params[recent_task][name]
            
            # Compute EWC penalty: F * (Œ∏ - Œ∏*)¬≤
            penalty = fisher * (param - optimal_param) ** 2
            ewc_loss += penalty.sum()
        
        return self.ewc_lambda * ewc_loss
    
    def get_total_loss(self, task_loss):
        """Compute total loss = task loss + EWC loss"""
        ewc_loss = self.compute_ewc_loss()
        total_loss = task_loss + ewc_loss
        
        return total_loss, ewc_loss
```

**Gi·∫£i th√≠ch chi ti·∫øt:**
- `Fisher Information`: ƒêo ƒë·ªô quan tr·ªçng c·ªßa parameter
- `compute_fisher_information()`: Sample t·ª´ model prediction (kh√¥ng d√πng true labels)
- `ewc_loss`: Penalty cho vi·ªác thay ƒë·ªïi important weights
- `ewc_lambda`: Strength c·ªßa regularization (1000 = strong protection)

---

## üß† B∆∞·ªõc 3: Implement Experience Replay

### 3.1 T·∫°i Sao Experience Replay Hi·ªáu Qu·∫£?

```python
# V·∫•n ƒë·ªÅ: Model ch·ªâ th·∫•y data t·ª´ current task
Current Task Data: [new_sample_1, new_sample_2, ...]
‚Üí Model forgets old tasks

# Gi·∫£i ph√°p: Mix old v√† new data
Mixed Training Data: [old_sample_1, new_sample_1, old_sample_2, new_sample_2, ...]
‚Üí Model remembers old tasks while learning new ones
```

### 3.2 T·∫°o `techniques/rehearsal.py`

```python
"""
Experience Replay - Memory-based continual learning
"""
import random
from collections import defaultdict

class MemoryBuffer:
    """Memory buffer for storing examples"""
    
    def __init__(self, memory_size, sampling_strategy="balanced"):
        self.memory_size = memory_size
        self.sampling_strategy = sampling_strategy
        
        # Storage
        self.examples = []
        self.labels = []
        self.task_ids = []
        
        # For balanced sampling
        self.class_examples = defaultdict(list)
        self.task_examples = defaultdict(list)
        
        self.current_size = 0
    
    def add_examples(self, examples, labels, task_id):
        """Add examples to memory buffer"""
        for example, label in zip(examples, labels):
            self._add_single_example(example, label, task_id)
    
    def _add_single_example(self, example, label, task_id):
        """Add single example with replacement strategy"""
        if self.current_size < self.memory_size:
            # Buffer not full, just append
            self.examples.append(example)
            self.labels.append(label)
            self.task_ids.append(task_id)
            self.current_size += 1
        else:
            # Buffer full, replace randomly
            replace_idx = random.randint(0, self.memory_size - 1)
            
            # Remove old tracking
            old_label = self.labels[replace_idx]
            old_task = self.task_ids[replace_idx]
            
            # Replace with new example
            self.examples[replace_idx] = example
            self.labels[replace_idx] = label
            self.task_ids[replace_idx] = task_id
        
        # Update tracking
        self.class_examples[label].append(len(self.examples) - 1)
        self.task_examples[task_id].append(len(self.examples) - 1)
    
    def sample(self, batch_size, exclude_task=None):
        """Sample examples from memory"""
        if self.current_size == 0:
            return [], [], []
        
        # Get available indices
        available_indices = list(range(self.current_size))
        
        if exclude_task is not None:
            # Exclude current task to avoid overfitting
            exclude_indices = [
                idx for idx in available_indices 
                if self.task_ids[idx] == exclude_task
            ]
            available_indices = [
                idx for idx in available_indices 
                if idx not in exclude_indices
            ]
        
        if not available_indices:
            return [], [], []
        
        # Sample based on strategy
        if self.sampling_strategy == "balanced":
            sampled_indices = self._balanced_sample(batch_size, available_indices)
        else:
            sampled_indices = random.choices(
                available_indices, 
                k=min(batch_size, len(available_indices))
            )
        
        # Get examples
        sampled_examples = [self.examples[idx] for idx in sampled_indices]
        sampled_labels = [self.labels[idx] for idx in sampled_indices]
        sampled_task_ids = [self.task_ids[idx] for idx in sampled_indices]
        
        return sampled_examples, sampled_labels, sampled_task_ids
    
    def _balanced_sample(self, batch_size, available_indices):
        """Sample equally from each class"""
        # Get available classes
        available_classes = set(self.labels[idx] for idx in available_indices)
        
        if not available_classes:
            return []
        
        # Sample equally from each class
        samples_per_class = batch_size // len(available_classes)
        remaining_samples = batch_size % len(available_classes)
        
        sampled_indices = []
        
        for i, class_label in enumerate(available_classes):
            # Get indices for this class
            class_indices = [
                idx for idx in available_indices 
                if self.labels[idx] == class_label
            ]
            
            # Number of samples for this class
            num_samples = samples_per_class
            if i < remaining_samples:
                num_samples += 1
            
            # Sample from this class
            class_samples = random.choices(
                class_indices, 
                k=min(num_samples, len(class_indices))
            )
            sampled_indices.extend(class_samples)
        
        return sampled_indices

class ExperienceReplay:
    """Experience Replay system"""
    
    def __init__(self, memory_size, replay_frequency=1):
        self.memory_size = memory_size
        self.replay_frequency = replay_frequency
        
        self.memory_buffer = MemoryBuffer(memory_size, "balanced")
        self.step_count = 0
    
    def store_examples(self, examples, labels, task_id):
        """Store examples in memory"""
        self.memory_buffer.add_examples(examples, labels, task_id)
        print(f"üíæ Stored {len(examples)} examples for task {task_id}")
    
    def get_mixed_batch(self, current_examples, current_labels, current_task_id, replay_ratio=0.5):
        """
        Get mixed batch of current + replay examples
        
        Args:
            current_examples: Examples from current task
            current_labels: Labels from current task  
            current_task_id: Current task ID
            replay_ratio: Ratio of replay examples (0.5 = 50% replay)
        """
        current_batch_size = len(current_examples)
        replay_batch_size = int(current_batch_size * replay_ratio / (1 - replay_ratio))
        
        # Get replay examples (exclude current task)
        replay_examples, replay_labels, replay_task_ids = self.memory_buffer.sample(
            batch_size=replay_batch_size,
            exclude_task=current_task_id
        )
        
        # Combine batches
        mixed_examples = current_examples + replay_examples
        mixed_labels = current_labels + replay_labels
        mixed_task_ids = [current_task_id] * len(current_examples) + replay_task_ids
        
        # Shuffle
        combined = list(zip(mixed_examples, mixed_labels, mixed_task_ids))
        random.shuffle(combined)
        
        if combined:
            mixed_examples, mixed_labels, mixed_task_ids = zip(*combined)
            return list(mixed_examples), list(mixed_labels), list(mixed_task_ids)
        else:
            return current_examples, current_labels, [current_task_id] * len(current_examples)
```

**Gi·∫£i th√≠ch:**
- `MemoryBuffer`: Store examples t·ª´ previous tasks
- `balanced_sample()`: Sample equally t·ª´ m·ªói class ƒë·ªÉ tr√°nh bias
- `get_mixed_batch()`: Mix current task data v·ªõi replay data
- `exclude_task`: Tr√°nh replay current task (prevent overfitting)

---

## ‚è∞ T·∫°m D·ª´ng - Checkpoint 1

ƒê·∫øn ƒë√¢y b·∫°n ƒë√£ hi·ªÉu:
1. ‚úÖ Lifelong Learning concept v√† catastrophic forgetting
2. ‚úÖ EWC implementation v·ªõi Fisher Information Matrix
3. ‚úÖ Experience Replay v·ªõi memory buffer
4. ‚úÖ C√°ch combine current v√† replay data

**Ti·∫øp theo**: Ch√∫ng ta s·∫Ω implement complete lifelong model, training system, v√† evaluation metrics.

---

## üèãÔ∏è B∆∞·ªõc 4: Implement Complete Lifelong Model

### 4.1 T·∫°i Sao C·∫ßn Lifelong Model Wrapper?

```python
# Standard model: Ch·ªâ handle 1 task
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# Lifelong model: Handle multiple tasks + anti-forgetting
lifelong_model = LifelongModel(
    base_model="bert-base-uncased",
    techniques=["ewc", "rehearsal"],
    task_heads={0: 2, 1: 4, 2: 3}  # Task 0: 2 labels, Task 1: 4 labels, etc.
)
```

### 4.2 T·∫°o `models/lifelong_model.py`

```python
"""
Complete lifelong learning model
"""
import torch
import torch.nn as nn
from transformers import AutoModel

class TaskSpecificHeads(nn.Module):
    """Task-specific classification heads"""

    def __init__(self, hidden_size, num_labels_per_task):
        super().__init__()
        self.heads = nn.ModuleDict()

        for task_id, num_labels in num_labels_per_task.items():
            self.heads[str(task_id)] = nn.Linear(hidden_size, num_labels)

    def add_task_head(self, task_id, num_labels):
        """Add new task head"""
        self.heads[str(task_id)] = nn.Linear(
            list(self.heads.values())[0].in_features,
            num_labels
        )

    def forward(self, hidden_states, task_id):
        """Forward through task-specific head"""
        head = self.heads[str(task_id)]

        # Use [CLS] token representation
        cls_hidden = hidden_states[:, 0]  # [batch_size, hidden_size]
        logits = head(cls_hidden)

        return {"logits": logits}

class LifelongModel(nn.Module):
    """Main lifelong learning model"""

    def __init__(self, base_model_name, lifelong_config, num_labels_per_task):
        super().__init__()

        self.base_model_name = base_model_name
        self.lifelong_config = lifelong_config
        self.num_labels_per_task = num_labels_per_task

        # Load base model (without classification head)
        self.base_model = AutoModel.from_pretrained(base_model_name)

        # Task-specific heads
        self.task_heads = TaskSpecificHeads(
            hidden_size=self.base_model.config.hidden_size,
            num_labels_per_task=num_labels_per_task
        )

        # Initialize lifelong learning techniques
        self.techniques = {}
        self._initialize_techniques()

        # Task tracking
        self.current_task_id = None
        self.completed_tasks = []

        print("‚úÖ LifelongModel initialized")

    def _initialize_techniques(self):
        """Initialize lifelong learning techniques"""
        config = self.lifelong_config

        if config.technique == "ewc" or "ewc" in getattr(config, 'combined_techniques', []):
            from techniques import ElasticWeightConsolidation
            self.techniques['ewc'] = ElasticWeightConsolidation(
                model=self.base_model,
                ewc_lambda=config.ewc_lambda
            )

        if config.technique == "rehearsal" or "rehearsal" in getattr(config, 'combined_techniques', []):
            from techniques import ExperienceReplay
            self.techniques['rehearsal'] = ExperienceReplay(
                memory_size=config.memory_size,
                replay_frequency=config.replay_frequency
            )

    def add_task(self, task_id, num_labels):
        """Add new task to model"""
        if task_id not in self.num_labels_per_task:
            self.num_labels_per_task[task_id] = num_labels
            self.task_heads.add_task_head(task_id, num_labels)
            print(f"üìö Added task {task_id} with {num_labels} labels")

    def set_current_task(self, task_id):
        """Set current task for training"""
        self.current_task_id = task_id

    def forward(self, input_ids, attention_mask=None, task_id=None, labels=None, **kwargs):
        """Forward pass through lifelong model"""
        # Use current task if not specified
        if task_id is None:
            task_id = self.current_task_id

        if task_id is None:
            raise ValueError("Task ID must be specified")

        # Forward through base model
        base_outputs = self.base_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            **kwargs
        )

        hidden_states = base_outputs.last_hidden_state

        # Forward through task-specific head
        task_outputs = self.task_heads(hidden_states, task_id)

        outputs = {
            'logits': task_outputs['logits'],
            'hidden_states': hidden_states
        }

        # Compute loss if labels provided
        if labels is not None:
            task_loss = nn.CrossEntropyLoss()(task_outputs['logits'], labels)

            # Add regularization losses
            total_loss = task_loss
            loss_components = {'task_loss': task_loss}

            # EWC loss
            if 'ewc' in self.techniques:
                ewc_loss = self.techniques['ewc'].compute_ewc_loss()
                total_loss += ewc_loss
                loss_components['ewc_loss'] = ewc_loss

            outputs['loss'] = total_loss
            outputs['loss_components'] = loss_components

        return outputs

    def complete_task(self, task_id, dataloader):
        """Mark task as completed and update techniques"""
        if task_id not in self.completed_tasks:
            self.completed_tasks.append(task_id)

        # Update EWC Fisher information
        if 'ewc' in self.techniques:
            self.techniques['ewc'].register_task(task_id, dataloader)

        print(f"‚úÖ Task {task_id} completed")

    def evaluate_all_tasks(self, task_dataloaders, device):
        """Evaluate performance on all completed tasks"""
        results = {}

        self.eval()

        for task_id in self.completed_tasks:
            if task_id in task_dataloaders:
                task_results = self._evaluate_single_task(
                    task_id, task_dataloaders[task_id], device
                )
                results[f'task_{task_id}'] = task_results

        # Compute average
        if results:
            avg_accuracy = sum(r['accuracy'] for r in results.values()) / len(results)
            results['average'] = {'accuracy': avg_accuracy}

        return results

    def _evaluate_single_task(self, task_id, dataloader, device):
        """Evaluate single task"""
        total_correct = 0
        total_samples = 0

        with torch.no_grad():
            for batch in dataloader:
                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                        for k, v in batch.items()}

                outputs = self.forward(task_id=task_id, **batch)

                predictions = torch.argmax(outputs['logits'], dim=-1)
                correct = (predictions == batch['labels']).sum().item()

                total_correct += correct
                total_samples += batch['labels'].size(0)

        accuracy = total_correct / total_samples if total_samples > 0 else 0.0

        return {'accuracy': accuracy, 'num_samples': total_samples}
```

---

## üéØ B∆∞·ªõc 5: Complete Training System

### 5.1 T·∫°o `training/lifelong_trainer.py`

```python
"""
Lifelong learning trainer
"""
import torch
from transformers import Trainer, TrainingArguments
from torch.utils.data import DataLoader

class LifelongTrainer:
    """Trainer for lifelong learning"""

    def __init__(self, model, lifelong_config, tokenizer):
        self.model = model
        self.lifelong_config = lifelong_config
        self.tokenizer = tokenizer
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.model.to(self.device)

        print(f"üèãÔ∏è LifelongTrainer initialized on {self.device}")

    def learn_task(self, task_id, train_dataset, eval_dataset=None, compute_metrics=None):
        """Learn a new task"""

        print(f"üìö Learning task {task_id}...")

        # Set current task
        self.model.set_current_task(task_id)

        # Setup training arguments
        training_args = TrainingArguments(
            output_dir=f"./task_{task_id}_results",
            num_train_epochs=self.lifelong_config.epochs_per_task,
            per_device_train_batch_size=self.lifelong_config.batch_size,
            per_device_eval_batch_size=self.lifelong_config.batch_size * 2,
            learning_rate=self.lifelong_config.learning_rate,
            weight_decay=0.01,
            evaluation_strategy="steps" if eval_dataset else "no",
            eval_steps=100,
            save_steps=500,
            logging_steps=50,
            load_best_model_at_end=True if eval_dataset else False,
            metric_for_best_model="eval_accuracy" if eval_dataset else None,
            seed=42,
        )

        # Create trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer,
            compute_metrics=compute_metrics,
        )

        # Handle experience replay during training
        if 'rehearsal' in self.model.techniques:
            trainer = self._setup_replay_trainer(trainer, task_id)

        # Train
        train_result = trainer.train()

        print(f"‚úÖ Task {task_id} training completed")
        print(f"Training loss: {train_result.training_loss:.4f}")

        return train_result

    def _setup_replay_trainer(self, trainer, current_task_id):
        """Setup trainer with experience replay"""

        replay_system = self.model.techniques['rehearsal']
        original_get_train_dataloader = trainer.get_train_dataloader

        def get_train_dataloader_with_replay():
            """Custom dataloader that mixes current and replay data"""

            # Get original dataloader
            original_dataloader = original_get_train_dataloader()

            # Create mixed dataloader
            mixed_batches = []

            for batch in original_dataloader:
                # Convert batch to examples
                current_examples = self._batch_to_examples(batch)
                current_labels = batch['labels'].tolist()

                # Get mixed batch with replay
                mixed_examples, mixed_labels, mixed_task_ids = replay_system.get_mixed_batch(
                    current_examples, current_labels, current_task_id, replay_ratio=0.3
                )

                # Convert back to batch format
                mixed_batch = self._examples_to_batch(mixed_examples, mixed_labels)
                mixed_batches.append(mixed_batch)

            return mixed_batches

        # Replace dataloader method
        trainer.get_train_dataloader = get_train_dataloader_with_replay

        return trainer

    def _batch_to_examples(self, batch):
        """Convert batch to list of examples"""
        # Simplified conversion
        batch_size = batch['input_ids'].size(0)
        examples = []

        for i in range(batch_size):
            example = {
                'input_ids': batch['input_ids'][i],
                'attention_mask': batch['attention_mask'][i]
            }
            examples.append(example)

        return examples

    def _examples_to_batch(self, examples, labels):
        """Convert examples back to batch format"""
        if not examples:
            return None

        # Stack tensors
        input_ids = torch.stack([ex['input_ids'] for ex in examples])
        attention_mask = torch.stack([ex['attention_mask'] for ex in examples])
        labels_tensor = torch.tensor(labels)

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels_tensor
        }

    def evaluate_task(self, task_id, eval_dataset):
        """Evaluate specific task"""

        # Create dataloader
        eval_dataloader = DataLoader(
            eval_dataset,
            batch_size=self.lifelong_config.batch_size * 2
        )

        # Evaluate
        results = self.model._evaluate_single_task(task_id, eval_dataloader, self.device)

        return {'eval_accuracy': results['accuracy']}
```

---

## üéâ Ho√†n Th√†nh - B·∫°n ƒê√£ C√≥ H·ªá Th·ªëng Lifelong Learning!

### T√≥m T·∫Øt Nh·ªØng G√¨ ƒê√£ Implement:

1. ‚úÖ **EWC (Elastic Weight Consolidation)**: Protect important weights
2. ‚úÖ **Experience Replay**: Memory buffer v·ªõi balanced sampling
3. ‚úÖ **Lifelong Model**: Multi-task architecture v·ªõi task-specific heads
4. ‚úÖ **Training System**: Sequential task learning v·ªõi anti-forgetting
5. ‚úÖ **Evaluation Metrics**: Forgetting measurement v√† performance tracking

### C√°ch Ch·∫°y:
```bash
cd lifelong-learning
python examples/text_classification_continual.py \
    --technique ewc \
    --tasks sentiment_imdb sentiment_sst2 topic_ag_news \
    --epochs_per_task 3
```

### Hi·ªáu Qu·∫£ ƒê·∫°t ƒê∆∞·ª£c:
```
Without Lifelong Learning:
Task 1: 90% ‚Üí 45% (after Task 2) ‚Üí 20% (after Task 3)
Task 2: 88% ‚Üí 40% (after Task 3)
Task 3: 85%
Average: 48% (Catastrophic forgetting!)

With EWC + Replay:
Task 1: 90% ‚Üí 87% (after Task 2) ‚Üí 85% (after Task 3)
Task 2: 88% ‚Üí 86% (after Task 3)
Task 3: 85%
Average: 85% (Knowledge preserved!)
```

### So S√°nh Techniques:
```
Technique       | Avg Performance | Avg Forgetting | Memory Usage
----------------|-----------------|----------------|-------------
No Protection   | 48%            | 52%            | Low
EWC             | 78%            | 22%            | Low
Experience Replay| 82%           | 18%            | Medium
EWC + Replay    | 85%            | 15%            | Medium
Progressive     | 88%            | 12%            | High
```

### Khi N√†o D√πng Lifelong Learning:
- ‚úÖ Sequential task deployment
- ‚úÖ Streaming data scenarios
- ‚úÖ Resource-constrained environments
- ‚úÖ Privacy-sensitive applications (no data storage)
- ‚úÖ Continual model updates

### B∆∞·ªõc Ti·∫øp Theo:
1. Ch·∫°y complete example ƒë·ªÉ th·∫•y k·∫øt qu·∫£
2. Th·ª≠ different technique combinations
3. Experiment v·ªõi different task sequences
4. Compare v·ªõi baseline (no protection)
5. Test tr√™n real-world scenarios

**Ch√∫c m·ª´ng! B·∫°n ƒë√£ hi·ªÉu v√† implement ƒë∆∞·ª£c Lifelong Learning t·ª´ s·ªë 0! üß†**
