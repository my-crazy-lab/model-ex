# Prefix Tuning & Prompt Tuning Requirements

# Core dependencies
torch>=2.0.0
transformers>=4.30.0
datasets>=2.12.0
evaluate>=0.4.0

# PEFT library for parameter-efficient fine-tuning
peft>=0.4.0

# Training and optimization
accelerate>=0.20.0
deepspeed>=0.9.0

# Scientific computing
numpy>=1.24.0
scipy>=1.10.0
scikit-learn>=1.3.0

# Utilities
tqdm>=4.65.0
pyyaml>=6.0
jsonlines>=3.1.0

# Visualization and monitoring
matplotlib>=3.7.0
seaborn>=0.12.0
plotly>=5.0.0
wandb>=0.15.0
tensorboard>=2.13.0

# Development and testing
pytest>=7.0.0
jupyter>=1.0.0
black>=22.0.0
flake8>=4.0.0

# Optional for specific models
# For T5 and other encoder-decoder models
sentencepiece>=0.1.99
protobuf>=3.20.0

# For tokenization
tokenizers>=0.13.0

# For distributed training
torch-distributed>=0.1.0

# For model optimization
optimum>=1.8.0

# For experiment tracking
mlflow>=2.0.0
optuna>=3.0.0

# For memory profiling
psutil>=5.9.0
memory-profiler>=0.60.0
