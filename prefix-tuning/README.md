# ğŸ¯ Prefix Tuning & Prompt Tuning Implementation

This project implements comprehensive Prefix Tuning and Prompt Tuning techniques based on the checklist in `fine-tuning/Prefix Tuning.md`.

## ğŸ“‹ What is Prefix Tuning?

Prefix Tuning is an extremely parameter-efficient fine-tuning method that:
- **Only trains prefix embeddings** while freezing all model parameters
- **Prepends learnable vectors** to the input sequence
- **Achieves competitive performance** with minimal trainable parameters
- **Enables task-specific adaptation** through prefix conditioning

## ğŸ—ï¸ Architecture

```
Input: "Classify: This movie is great!"
    â†“
Prefix: [P1] [P2] [P3] [P4] [P5] + Input tokens
    â†“
Frozen LLM: Process prefixed input
    â†“
Output: Task-specific predictions
```

### Prefix Tuning vs Other Methods

| Method | Trainable Parameters | Storage | Performance | Flexibility |
|--------|---------------------|---------|-------------|-------------|
| Full Fine-tuning | 100% | High | 100% | Low |
| LoRA | ~0.5% | Medium | 96-99% | Medium |
| Adapter | ~1% | Medium | 95-98% | Medium |
| **Prefix Tuning** | **~0.1%** | **Very Low** | **90-95%** | **High** |

## ğŸ“ Project Structure

```
prefix-tuning/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ setup.py                    # Package setup
â”œâ”€â”€ config/                     # Configuration files
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ prefix_config.py        # Prefix tuning configurations
â”‚   â”œâ”€â”€ prompt_config.py        # Prompt tuning configurations
â”‚   â””â”€â”€ model_config.py         # Model configurations
â”œâ”€â”€ prefix_tuning/              # Core implementation
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ prefix_model.py         # Prefix tuning model
â”‚   â”œâ”€â”€ prompt_model.py         # Prompt tuning model
â”‚   â”œâ”€â”€ prefix_embeddings.py    # Prefix embedding layers
â”‚   â””â”€â”€ prompt_embeddings.py    # Prompt embedding layers
â”œâ”€â”€ training/                   # Training systems
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ prefix_trainer.py       # Prefix tuning trainer
â”‚   â”œâ”€â”€ prompt_trainer.py       # Prompt tuning trainer
â”‚   â””â”€â”€ evaluation.py           # Evaluation utilities
â”œâ”€â”€ data/                       # Data processing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py          # Data loading utilities
â”‚   â””â”€â”€ preprocessing.py        # Data preprocessing
â”œâ”€â”€ experiments/                # Experiment management
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ experiment_runner.py    # Experiment orchestration
â”‚   â””â”€â”€ comparison.py           # Method comparison
â”œâ”€â”€ examples/                   # Example scripts
â”‚   â”œâ”€â”€ text_classification_prefix.py
â”‚   â”œâ”€â”€ text_generation_prefix.py
â”‚   â””â”€â”€ multi_task_prefix.py
â””â”€â”€ notebooks/                  # Jupyter notebooks
    â”œâ”€â”€ 01_prefix_tuning_basics.ipynb
    â”œâ”€â”€ 02_prompt_tuning_comparison.ipynb
    â””â”€â”€ 03_parameter_efficiency_analysis.ipynb
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Install the package
pip install -e .
```

### 2. Basic Prefix Tuning

```python
from prefix_tuning import PrefixTuningModel, PrefixTrainer
from config import PrefixConfig, ModelConfig

# Setup configurations
model_config = ModelConfig(
    model_name_or_path="gpt2-medium",
    task_type="text_generation"
)

prefix_config = PrefixConfig(
    prefix_length=10,
    prefix_hidden_size=512,
    prefix_dropout=0.1,
    reparameterization=True
)

# Create prefix tuning model
model = PrefixTuningModel(model_config, prefix_config)

# Train model
trainer = PrefixTrainer(model, training_config)
trainer.train(train_dataset, eval_dataset)
```

### 3. Prompt Tuning

```python
from prefix_tuning import PromptTuningModel, PromptTrainer
from config import PromptConfig

# Setup prompt tuning
prompt_config = PromptConfig(
    num_virtual_tokens=20,
    prompt_tuning_init="random",  # random, text, vocab_sample
    prompt_tuning_init_text="Classify the sentiment:"
)

# Create prompt tuning model
model = PromptTuningModel(model_config, prompt_config)

# Train model
trainer = PromptTrainer(model, training_config)
trainer.train(train_dataset, eval_dataset)
```

## ğŸ”§ Key Features

### âœ… Extreme Parameter Efficiency
- **Only prefix/prompt embeddings are trainable** (0.01-0.1% of total)
- Automatic parameter freezing for base model
- Memory-efficient training and inference

### âœ… Multiple Prefix Strategies
- **Prefix Tuning**: Learnable prefixes for each layer
- **Prompt Tuning**: Learnable soft prompts at input
- **P-Tuning v2**: Deep prompt tuning across layers

### âœ… Flexible Initialization
- **Random initialization**: Start from scratch
- **Text initialization**: Initialize from text prompts
- **Vocabulary sampling**: Sample from model vocabulary

### âœ… Task Adaptation
- **Classification tasks**: Sentiment, topic, NLI
- **Generation tasks**: Summarization, translation
- **Multi-task learning**: Shared prefixes across tasks

## ğŸ“Š Supported Tasks

### Text Classification
- Sentiment Analysis (SST-2, IMDB)
- Topic Classification (AG News)
- Natural Language Inference (MNLI, SNLI)

### Text Generation
- Summarization (CNN/DailyMail, XSum)
- Translation (WMT datasets)
- Dialogue Generation

### Question Answering
- Reading Comprehension (SQuAD)
- Commonsense QA (CommonsenseQA)

## ğŸ§  Prefix Tuning Principles

### 1. Prefix Conditioning
```python
# Standard input: [x1, x2, x3, ...]
# Prefixed input: [P1, P2, ..., Pk, x1, x2, x3, ...]
# Where P1, P2, ..., Pk are learnable prefix embeddings
```

### 2. Layer-wise Prefixes
```python
# Prefix tuning adds prefixes to each transformer layer
for layer in transformer_layers:
    # Add prefix to key and value projections
    prefixed_keys = concat([prefix_keys[layer], original_keys])
    prefixed_values = concat([prefix_values[layer], original_values])
```

### 3. Reparameterization
```python
# Use MLP to generate actual prefix embeddings
prefix_embeddings = MLP(trainable_prefix_params)
# This improves optimization stability
```

## ğŸ“ˆ Performance Benefits

### Parameter Efficiency
```
GPT-2 Medium: 345M parameters
Prefix Tuning: 0.1M trainable parameters (0.03% of total)
Storage: 0.4MB vs 1.4GB (3500x reduction)
```

### Training Speed
```
Full Fine-tuning: 100% computation
Prefix Tuning: ~5% computation (only prefix gradients)
Training time: 20x faster
```

### Memory Usage
```
Full Fine-tuning: High memory for all gradients
Prefix Tuning: Low memory for prefix gradients only
Memory reduction: 95%
```

## ğŸ”¬ Advanced Features

### Multi-Task Prefix Sharing
```python
# Share base model, use task-specific prefixes
shared_model = "gpt2-medium"
task_a_prefix = "sentiment_prefix.bin"  # 0.4MB
task_b_prefix = "topic_prefix.bin"      # 0.4MB
task_c_prefix = "qa_prefix.bin"         # 0.4MB

# Total: 1.4GB + 1.2MB vs 4.2GB for separate models
```

### Dynamic Prefix Length
```python
# Adjust prefix length based on task complexity
simple_task_config = PrefixConfig(prefix_length=5)
complex_task_config = PrefixConfig(prefix_length=20)
```

### Prefix Interpolation
```python
# Interpolate between task-specific prefixes
mixed_prefix = alpha * task_a_prefix + (1-alpha) * task_b_prefix
```

## ğŸ“– Documentation

See the `notebooks/` directory for detailed tutorials:
1. **Prefix Tuning Basics**: Understanding prefix conditioning
2. **Prompt Tuning Comparison**: Comparing different approaches
3. **Parameter Efficiency Analysis**: Analyzing efficiency gains

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Implement your changes
4. Add tests and documentation
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- [Prefix Tuning Paper](https://arxiv.org/abs/2101.00190)
- [Prompt Tuning Paper](https://arxiv.org/abs/2104.08691)
- [P-Tuning v2 Paper](https://arxiv.org/abs/2110.07602)
- [PEFT Library](https://github.com/huggingface/peft)
