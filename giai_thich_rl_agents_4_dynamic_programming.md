# üéØ Gi·∫£i Th√≠ch Chi Ti·∫øt: Dynamic Programming - Classical Planning trong RL

## üìö T·ªïng Quan Dynamic Programming

**Dynamic Programming (DP)** l√† foundation c·ªßa Reinforcement Learning, cung c·∫•p **exact solutions** cho Markov Decision Processes khi environment model ho√†n to√†n known. DP algorithms bao g·ªìm **Value Iteration** v√† **Policy Iteration**.

---

## üéØ V·∫•n ƒê·ªÅ C·∫ßn Gi·∫£i Quy·∫øt

### Model-Based vs Model-Free:

**Model-Based (Dynamic Programming):**
```
Known: Transition probabilities P(s'|s,a), Rewards R(s,a)
Goal: Find optimal policy œÄ* v√† value function V*
Method: Exact computation using Bellman equations
```

**Model-Free (Q-Learning, etc.):**
```
Unknown: Environment dynamics
Goal: Learn optimal policy through interaction
Method: Trial-and-error learning
```

### Finite MDP Requirements:
```
Finite State Space: S = {s‚ÇÅ, s‚ÇÇ, ..., s‚Çô}
Finite Action Space: A = {a‚ÇÅ, a‚ÇÇ, ..., a‚Çò}
Known Dynamics: P(s'|s,a) v√† R(s,a)
```

---

## üèóÔ∏è Ki·∫øn Tr√∫c Value Iteration

### Core Components:

```python
class ValueIterationAgent(AbstractAgent):
    def __init__(self, env, config=None):
        # 1. MDP Model (transition probabilities, rewards)
        self.mdp = env.mdp or env.unwrapped.to_finite_mdp()
        
        # 2. State-Action Value Function
        self.state_action_value = self.get_state_action_value()
        
        # 3. Configuration (gamma, iterations)
        self.config = config or self.default_config()
```

---

## üìñ Ph√¢n T√≠ch T·ª´ng D√≤ng Code

### D√≤ng 9-23: Initialization v√† MDP Setup
```python
class ValueIterationAgent(AbstractAgent):
    def __init__(self, env, config=None):
        super(ValueIterationAgent, self).__init__(config)
        
        # Check if environment is already a finite MDP
        self.finite_mdp = self.is_finite_mdp(env)
        
        if self.finite_mdp:
            # Direct access to MDP
            self.mdp = env.mdp
        elif not self.finite_mdp:
            try:
                # Convert environment to finite MDP
                self.mdp = env.unwrapped.to_finite_mdp()
            except AttributeError:
                raise TypeError("Environment must be of type finite_mdp.envs.finite_mdp.FiniteMDPEnv or handle a "
                                "conversion method called 'to_finite_mdp' to such a type.")
        
        self.env = env
        # Compute optimal Q-values using Value Iteration
        self.state_action_value = self.get_state_action_value()
```

**Gi·∫£i th√≠ch:**
- **MDP Validation**: Ensure environment c√≥ finite state/action spaces
- **Model Access**: Get transition probabilities v√† rewards
- **Conversion**: Convert gym environments to finite MDP format
- **Precomputation**: Compute optimal policy during initialization

### D√≤ng 24-27: Default Configuration
```python
@classmethod
def default_config(cls):
    return dict(gamma=1.0,        # Discount factor
                iterations=100)   # Maximum iterations
```

**Parameters:**
- **gamma=1.0**: No discounting (episodic tasks)
- **iterations=100**: Maximum iterations cho convergence

### D√≤ng 29-35: Action Selection
```python
def act(self, state):
    # Handle dynamic environments
    if not self.finite_mdp:
        # Recompute MDP if environment changed
        self.mdp = self.env.unwrapped.to_finite_mdp()
        state = self.mdp.state
        # Recompute optimal values
        self.state_action_value = self.get_state_action_value()
    
    # Select greedy action based on Q-values
    return np.argmax(self.state_action_value[state, :])
```

**Gi·∫£i th√≠ch:**
- **Dynamic Recomputation**: Handle changing environments
- **Greedy Policy**: Always select action v·ªõi highest Q-value
- **Optimal Policy**: No exploration needed (exact solution)

### D√≤ng 37-45: Value Function Computation
```python
def get_state_value(self):
    """Compute V*(s) using Value Iteration"""
    return self.fixed_point_iteration(
        lambda v: ValueIterationAgent.best_action_value(self.bellman_expectation(v)),
        np.zeros((self.mdp.transition.shape[0],))  # Initialize V(s) = 0
    )

def get_state_action_value(self):
    """Compute Q*(s,a) using Value Iteration"""
    return self.fixed_point_iteration(
        lambda q: self.bellman_expectation(ValueIterationAgent.best_action_value(q)),
        np.zeros((self.mdp.transition.shape[0:2]))  # Initialize Q(s,a) = 0
    )
```

**Value Iteration Algorithm:**

**State Value Version:**
```
V_{k+1}(s) = max_a Œ£_{s'} P(s'|s,a) [R(s,a,s') + Œ≥ V_k(s')]
```

**State-Action Value Version:**
```
Q_{k+1}(s,a) = Œ£_{s'} P(s'|s,a) [R(s,a,s') + Œ≥ max_{a'} Q_k(s',a')]
```

### D√≤ng 47-49: Best Action Selection
```python
@staticmethod
def best_action_value(action_values):
    """Extract best value for each state: max_a Q(s,a)"""
    return action_values.max(axis=-1)
```

**Gi·∫£i th√≠ch:**
- **Max Operation**: Core c·ªßa Bellman optimality equation
- **Axis=-1**: Max over actions (last dimension)
- **Return**: V(s) = max_a Q(s,a) for each state

### D√≤ng 51-63: Bellman Expectation Operator
```python
def bellman_expectation(self, value):
    """Apply Bellman expectation operator"""
    
    if self.mdp.mode == "deterministic":
        # Deterministic transitions: P(s'|s,a) ‚àà {0,1}
        next_v = value[self.mdp.transition]
        
    elif self.mdp.mode == "stochastic":
        # Stochastic transitions: full probability matrix
        next_v = (self.mdp.transition * value.reshape((1, 1, value.size))).sum(axis=-1)
        
    elif self.mdp.mode == "sparse":
        # Sparse representation: only non-zero transitions
        next_values = np.take(value, self.mdp.next)
        next_v = (self.mdp.transition * next_values).sum(axis=-1)
    else:
        raise ValueError("Unknown mode")
    
    # Terminal states have value 0
    next_v[self.mdp.terminal] = 0
    
    # Bellman equation: R(s,a) + Œ≥ Œ£_{s'} P(s'|s,a) V(s')
    return self.mdp.reward + self.config["gamma"] * next_v
```

**Bellman Expectation Operator:**
```
(T_œÄ V)(s) = Œ£_a œÄ(a|s) Œ£_{s'} P(s'|s,a) [R(s,a,s') + Œ≥ V(s')]
```

**Implementation Details:**
- **Deterministic**: Direct indexing cho single next state
- **Stochastic**: Matrix multiplication v·ªõi probability distributions
- **Sparse**: Efficient computation cho sparse transition matrices
- **Terminal Handling**: Ensure terminal states have value 0

### D√≤ng 65-73: Fixed Point Iteration
```python
def fixed_point_iteration(self, operator, initial):
    """Iterate until convergence"""
    value = initial
    
    for iteration in range(self.config["iterations"]):
        logger.debug("Value Iteration: {}/{}".format(iteration, self.config["iterations"]))
        
        # Apply Bellman operator
        next_value = operator(value)
        
        # Check convergence
        if np.allclose(value, next_value):
            break
            
        value = next_value
    
    return value
```

**Convergence Theory:**
- **Contraction Mapping**: Bellman operator is contraction v·ªõi factor Œ≥
- **Unique Fixed Point**: V* is unique fixed point
- **Convergence Rate**: Linear v·ªõi rate Œ≥
- **Stopping Criterion**: ||V_{k+1} - V_k|| < Œµ

### D√≤ng 84-96: Trajectory Planning
```python
def plan_trajectory(self, state, horizon=10):
    """Plan optimal trajectory from given state"""
    action_value = self.get_state_action_value()
    states, actions = [], []
    
    for _ in range(horizon):
        # Select optimal action
        action = np.argmax(action_value[state])
        states.append(state)
        actions.append(action)
        
        # Transition to next state
        state = self.mdp.next_state(state, action)
        
        # Check if terminal
        if self.mdp.terminal[state]:
            states.append(state)
            actions.append(None)
            break
    
    return states, actions
```

**Gi·∫£i th√≠ch:**
- **Multi-Step Planning**: Generate sequence of optimal actions
- **Deterministic Execution**: Follow optimal policy
- **Terminal Handling**: Stop at terminal states
- **Horizon Limit**: Prevent infinite loops

---

## üîÑ Value Iteration Algorithm Deep Dive

### Mathematical Foundation:

**Bellman Optimality Equation:**
```
V*(s) = max_a Œ£_{s'} P(s'|s,a) [R(s,a,s') + Œ≥ V*(s')]
Q*(s,a) = Œ£_{s'} P(s'|s,a) [R(s,a,s') + Œ≥ max_{a'} Q*(s',a')]
```

**Value Iteration Update:**
```
V_{k+1}(s) ‚Üê max_a Œ£_{s'} P(s'|s,a) [R(s,a,s') + Œ≥ V_k(s')]
```

### Algorithm Steps:

1. **Initialize**: V‚ÇÄ(s) = 0 for all s
2. **Iterate**: Apply Bellman optimality operator
3. **Converge**: Stop when ||V_{k+1} - V_k|| < Œµ
4. **Extract Policy**: œÄ*(s) = argmax_a Q*(s,a)

### Complexity Analysis:

**Time Complexity**: O(|S|¬≤|A| √ó iterations)
**Space Complexity**: O(|S||A|)
**Convergence**: Linear v·ªõi rate Œ≥

---

## üéØ Robust Value Iteration

### Uncertainty in Transition Probabilities:

```python
# File: robust_value_iteration.py
class RobustValueIterationAgent(ValueIterationAgent):
    """Handle uncertainty in MDP parameters"""
    
    def robust_bellman_expectation(self, value):
        """Worst-case Bellman operator"""
        # Consider uncertainty set around nominal probabilities
        worst_case_values = []
        
        for uncertainty_level in self.config["uncertainty_set"]:
            perturbed_transitions = self.perturb_transitions(uncertainty_level)
            perturbed_value = self.compute_value_with_transitions(perturbed_transitions, value)
            worst_case_values.append(perturbed_value)
        
        # Take worst case (minimum value)
        return np.min(worst_case_values, axis=0)
```

**Robust MDP Formulation:**
```
V*(s) = max_a min_{P ‚àà U} Œ£_{s'} P(s'|s,a) [R(s,a,s') + Œ≥ V*(s')]

Where U is uncertainty set around nominal probabilities
```

---

## üí° ∆Øu ƒêi·ªÉm v√† Nh∆∞·ª£c ƒêi·ªÉm

### ‚úÖ **∆Øu ƒêi·ªÉm:**
- **Exact Solution**: Guaranteed optimal policy
- **No Exploration**: No need for trial-and-error
- **Convergence Guarantee**: Theoretical convergence proof
- **Policy Quality**: Optimal performance from start
- **Planning Capability**: Can plan multi-step trajectories

### ‚ùå **Nh∆∞·ª£c ƒêi·ªÉm:**
- **Model Requirement**: Need complete environment model
- **Computational Complexity**: O(|S|¬≤|A|) per iteration
- **Scalability**: Intractable cho large state spaces
- **Model Accuracy**: Performance depends on model accuracy
- **Discrete Spaces**: Only works v·ªõi finite state/action spaces

---

## üöÄ Use Cases v√† Applications

### **Ideal Applications:**
1. **Grid Worlds**: Navigation problems v·ªõi known maps
2. **Inventory Management**: Known demand distributions
3. **Game Playing**: Perfect information games
4. **Resource Allocation**: Known resource dynamics
5. **Scheduling**: Known task requirements

### **Environment Requirements:**
```python
# DP requires:
# 1. Finite state space
assert len(env.observation_space) < float('inf')

# 2. Finite action space  
assert len(env.action_space) < float('inf')

# 3. Known transition probabilities
assert hasattr(env, 'transition_probabilities')

# 4. Known reward function
assert hasattr(env, 'reward_function')
```

---

## üîß Implementation Variants

### **Policy Iteration vs Value Iteration:**

**Policy Iteration:**
```python
def policy_iteration(self):
    # 1. Initialize random policy
    policy = np.random.randint(0, self.mdp.n_actions, self.mdp.n_states)
    
    while True:
        # 2. Policy Evaluation: solve V^œÄ
        value = self.policy_evaluation(policy)
        
        # 3. Policy Improvement: œÄ' = greedy(V^œÄ)
        new_policy = self.policy_improvement(value)
        
        # 4. Check convergence
        if np.array_equal(policy, new_policy):
            break
            
        policy = new_policy
    
    return policy, value
```

**Comparison:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Algorithm       ‚îÇ Convergence     ‚îÇ Computation     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Value Iteration ‚îÇ Slower          ‚îÇ Less per iter   ‚îÇ
‚îÇ Policy Iteration‚îÇ Faster          ‚îÇ More per iter   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **Asynchronous Value Iteration:**
```python
def asynchronous_value_iteration(self):
    """Update states in arbitrary order"""
    for iteration in range(self.config["iterations"]):
        # Random order updates
        states = np.random.permutation(self.mdp.n_states)
        
        for state in states:
            # Update single state
            old_value = self.value[state]
            self.value[state] = max(self.bellman_expectation_single_state(state))
            
            # Early stopping if converged
            if abs(self.value[state] - old_value) < self.config["tolerance"]:
                continue
```

---

## üîß Configuration Best Practices

### **Key Parameters:**
```python
dp_config = {
    "gamma": 0.99,              # Discount factor (< 1 for infinite horizon)
    "iterations": 1000,         # Maximum iterations
    "tolerance": 1e-6,          # Convergence tolerance
    "mode": "stochastic",       # Transition mode
    "robust": False,            # Enable robust DP
    "uncertainty_set": None     # Uncertainty parameters
}
```

### **Performance Tuning:**
- **Convergence Tolerance**: Balance accuracy vs computation time
- **Discount Factor**: Œ≥ < 1 ensures convergence
- **Sparse Representation**: Use sparse matrices cho large state spaces
- **Parallel Computation**: Parallelize Bellman updates
- **State Ordering**: Smart ordering can improve convergence

**Next Algorithm**: Ch√∫ng ta s·∫Ω ti·∫øp t·ª•c v·ªõi **Tree Search Methods** - advanced planning algorithms nh∆∞ MCTS!
